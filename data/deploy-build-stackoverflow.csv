input_text,output_text
"What IAM role does serviceaccount need for exporting dns record-sets on GCP?<p>I'm running a bash script from CloudBuild. The bash script is gonna export DNS record to a file.</p>
<pre><code>gcloud dns record-sets export zonedump-$i.tmp --zone-file-format --project $projectname --zone=$i
</code></pre>
<p>The serviceaccount of cloudbuild has roles/dns.reader</p>
<p>I get this error when I run cloudbuild:</p>
<pre><code>ERROR: (gcloud.dns.record-sets.export) HTTPError 403: Forbidden
Finished Step #3
ERROR
ERROR: build step 3 &quot;gcr.io/google.com/cloudsdktool/cloud-sdk&quot; failed: step exited with non-zero status: 1
</code></pre>
<p>cloudbuild.yaml</p>
<pre><code>steps:
- name: 'bash'
  args: ['echo', 'Backing up DNS zone to Storage!']
- name: 'bash'
  args: ['ls']
- name: 'bash'
  args: ['chmod', '+x', 'scripts/dnszone_backup.bash']
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: &quot;bash&quot;
  args:
    - &quot;-c&quot;
    - |
      echo &quot;calling script&quot;
      ls -la
      gcloud version
      scripts/dnszone_backup.bash
</code></pre>
<p>I would appreciate if someone can help me with this.</p>","<p>I needed to activate &quot;Cloud Build API&quot;.</p>
<p>I found it by testing it via <code>gcloud dns record-sets list</code> and got a clear error message for activating this API.</p>"
"Share (parameterized) cloudbuild.yaml between multiple GitHub projects?<p>I've managed to trigger a Google Cloud Build on every commit to GitHub successfully. However, we have many different source code repositories (projects) on GitHub, that all use Maven and Spring Boot, and I would like all of these projects to use the same <code>cloudbuild.yaml</code> (or a shared template). This way we don't need to duplicate the <code>cloudbuild.yaml</code> in all projects (it'll be essentially the same in most projects).</p>
<p>For example, let's just take two different projects on GitHub, A and B. Their <code>cloudbuild.yaml</code> files could look something like this (but much more complex in our actual projects):</p>
<p>Project A:</p>
<pre><code>steps:
  - name: maven:3.8.6-eclipse-temurin-17-alpine
    entrypoint: mvn
    args: [ 'test' ]
  - name: maven:3.8.6-eclipse-temurin-17-alpine
    entrypoint: mvn
    args: [ 'package', '-Dmaven.test.skip=true' ]
  - name: gcr.io/cloud-builders/docker
    args: [ &quot;build&quot;, &quot;-t&quot;, &quot;europe-west1-docker.pkg.dev/projectname/repo/project-a&quot;, &quot;--build-arg=JAR_FILE=target/project-a.jar&quot;, &quot;.&quot; ]
images: [ &quot;europe-west1-docker.pkg.dev/projectname/repo/project-a&quot; ]
</code></pre>
<p>Project B:</p>
<pre><code>steps:
  - name: maven:3.8.6-eclipse-temurin-17-alpine
    entrypoint: mvn
    args: [ 'test' ]
  - name: maven:3.8.6-eclipse-temurin-17-alpine
    entrypoint: mvn
    args: [ 'package', '-Dmaven.test.skip=true' ]
  - name: gcr.io/cloud-builders/docker
    args: [ &quot;build&quot;, &quot;-t&quot;, &quot;europe-west1-docker.pkg.dev/projectname/repo/project-a&quot;, &quot;--build-arg=JAR_FILE=target/project-b.jar&quot;, &quot;.&quot; ]
images: [ &quot;europe-west1-docker.pkg.dev/projectname/repo/project-b&quot; ]
</code></pre>
<p>The only thing that is different is the jar file and image name, the steps are the same. Now imagine having hundreds of such projects, it can become a maintenance nightmare if we need to change or add a build step for each project.</p>
<p>A better approach, in my mind, would be to have a template file that could be shared:</p>
<pre><code>steps:
  - name: maven:3.8.6-eclipse-temurin-17-alpine
    entrypoint: mvn
    args: [ 'test' ]
  - name: maven:3.8.6-eclipse-temurin-17-alpine
    entrypoint: mvn
    args: [ 'package', '-Dmaven.test.skip=true' ]
  - name: gcr.io/cloud-builders/docker
    args: [ &quot;build&quot;, &quot;-t&quot;, &quot;europe-west1-docker.pkg.dev/projectname/repo/${PROJECT_NAME}&quot;, &quot;--build-arg=JAR_FILE=target/${PROJECT_NAME}.jar&quot;, &quot;.&quot; ]
images: [ &quot;europe-west1-docker.pkg.dev/projectname/repo/${PROJECT_NAME}&quot; ]
</code></pre>
<p>It would then be great if such a template file could be uploaded to GCS and then reused  in the <code>cloudbuild.yaml</code> file in each project:</p>
<p>Project A:</p>
<pre><code>steps:
  import:
    gcs: gs:/my-build-bucket/cloudbuild-template.yaml
    parameters: 
      PROJECT_NAME: project-a
</code></pre>
<p>Project B:</p>
<pre><code>steps:
  import:
    gcs: gs:/my-build-bucket/cloudbuild-template.yaml
    parameters: 
      PROJECT_NAME: project-b
</code></pre>
<p>Does such a thing exists for Google Cloud Build? How do you import/reuse build steps in different builds as I described above? What's the recommended way to achieve this?</p>","<p>I contacted Google Cloud support and they told me that this is not currently available. They are aware of the issue and it's something that they're working on (no eta on when it's going to be available).</p>
<p>Their recommendation, in the meantime, is to use <a href=""https://tekton.dev/"" rel=""nofollow noreferrer"">Tekton</a>.</p>"
"Google Cloud Build Generating cloud_run_revision warnings<p>I have a system built by Google Cloud Build and running in Google Cloud Run. It detects when I've pushed new code into Bitbucket and automatically builds from there.</p>
<p>The last change I made to code was a few days ago. Today I noticed a number of messages in the logs explorer containing the following kind of info...</p>
<pre><code>  {
  &quot;insertId&quot;: &quot;62eec4ec00054bed93ba2600&quot;,
  &quot;httpRequest&quot;: {
    &quot;requestMethod&quot;: &quot;GET&quot;,
    &quot;requestUrl&quot;: &quot;https://gx-txxx-ylb2wvn3ma-uc.a.run.app/internal/jolokia/&quot;,
    &quot;requestSize&quot;: &quot;776&quot;,
    &quot;status&quot;: 404,
    &quot;responseSize&quot;: &quot;858&quot;,
    &quot;userAgent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36&quot;,
    &quot;remoteIp&quot;: &quot;216.131.109.149&quot;,
    &quot;serverIp&quot;: &quot;216.239.32.53&quot;,
    &quot;latency&quot;: &quot;0.006097606s&quot;,
    &quot;protocol&quot;: &quot;HTTP/1.1&quot;
  },
  &quot;resource&quot;: {
    &quot;type&quot;: &quot;cloud_run_revision&quot;,
    &quot;labels&quot;: {
      &quot;service_name&quot;: &quot;gr-test&quot;,
      &quot;configuration_name&quot;: &quot;gx-txxx&quot;,
      &quot;location&quot;: &quot;us-central1&quot;,
      &quot;project_id&quot;: &quot;central-segment-190xxx&quot;,
      &quot;revision_name&quot;: &quot;gx-txxx-00086-nis&quot;
    }
  },
  &quot;timestamp&quot;: &quot;2022-08-06T19:45:48.347117Z&quot;,
  &quot;severity&quot;: &quot;WARNING&quot;,
  &quot;labels&quot;: {
    &quot;instanceId&quot;: &quot;00c527f6d4848dafa449ff8b561a21d622e3e35600254dea2fa54a4e9c1fddd150910889f9a9e0c94769abb5b9e29520fc3b0822a31d84b121acbd5bbd0d65b186&quot;,
    &quot;managed-by&quot;: &quot;gcp-cloud-build-deploy-cloud-run&quot;,
    &quot;commit-sha&quot;: &quot;33745757dcc97bdfa08d37ba1e4b238607634fe2&quot;,
    &quot;gcb-trigger-id&quot;: &quot;8511bbc7-62c8-4f52-bf85-cfd9cd026d9f&quot;,
    &quot;gcb-build-id&quot;: &quot;3290e3e0-7b22-4034-9d96-5af1d0c3cf6c&quot;
  },
  &quot;logName&quot;: &quot;projects/central-segment-190216/logs/run.googleapis.com%2Frequests&quot;,
  &quot;trace&quot;: &quot;projects/central-segment-190216/traces/45562a3a3ea82892a4453ad4edc98a89&quot;,
  &quot;receiveTimestamp&quot;: &quot;2022-08-06T19:45:48.390028733Z&quot;,
  &quot;spanId&quot;: &quot;14733853662024484454&quot;
}
</code></pre>
<p>I haven't made any changes to my code in the past few days and Cloud Build doesn't point to any new builds since the last one I did. So, what could be causing these warnings? Is this a security issue about which I should be concerned?</p>
<p>Thank you!</p>
<p>&lt;&lt;Mod1 - I've posted the complete JSON message&gt;&gt;</p>","<p>The log entry shown corresponds to a <code>GET</code> against <code>https://gx-txxx-ylb2wvn3ma-uc.a.run.app/internal/jolokia/</code> which is not found.</p>
<p>In this case, with a publicly available service (and as OP points out) corroborating the request against the unfamiliar logged remote IP, the request is indicative of probing by &quot;others&quot;.</p>
<p>Such requests are not always malicious in intent but, in this case, it's likely a bot searching for some weakness. In this case, I suspect, looking for a site that supports <a href=""https://jolokia.org/reference/html/protocol.html"" rel=""nofollow noreferrer""><code>jolokia</code> protocol</a>.</p>
<p>It's prudent to pay attention to these requests and they will increase your costs. They may do so intentionally e.g. <a href=""https://en.wikipedia.org/wiki/Denial-of-service_attack"" rel=""nofollow noreferrer"">denial of service</a>. Google provides <a href=""https://cloud.google.com/armor/"" rel=""nofollow noreferrer"">Google Cloud Armor</a> to help protect sites. There are many so-called <a href=""https://en.wikipedia.org/wiki/Web_application_firewall"" rel=""nofollow noreferrer"">Web Application Firewall</a> tools available.</p>
<p>It's curious that Google is reporting <a href=""https://en.m.wikipedia.org/wiki/HTTP_404"" rel=""nofollow noreferrer""><code>404</code></a> as <code>warning</code> but this may be a way to avoid flooding logs with errors in cases like this which are unfortunate but expected.</p>"
"GCP deploy cloud function failed<p>I was trying to deploy a GCP cloud function using git hub actions and I got this error:</p>
<p><code>ERROR: (gcloud.functions.deploy) OperationError: code=3, message=Build failed: ERROR: Invalid requirement: 'Creating virtualenv xxxxxx-nJ6OriN1-py3.8 in /github/home/.cache/pypoetry/virtualenvs' (from line 1 of requirements.txt)</code></p>
<p><code>Hint: It looks like a path. File 'Creating virtualenv retry-failed-reports-nJ6OriN1-py3.8 in /github/home/.cache/pypoetry/virtualenvs' does not exist.; Error ID: 0ea8a540</code></p>
<p>Up until today, when all git hub workflows failed with this problem, my CI-CD pipeline was working as expected.</p>
<p>I was able to deploy the function manually using <code>gcloud</code>, it fails on cloud build step</p>
<p><a href=""https://i.stack.imgur.com/pVXCw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pVXCw.png"" alt=""enter image description here"" /></a></p>
<p>How can I solve this issue?</p>","<p>We were using this command to generate the <code>requirements.txt</code>:</p>
<p><code>poetry export -f requirements.txt --without-hashes &gt; requirements.txt</code></p>
<p>However, this stopped working.</p>
<p>Now I tried this command and is working fine:</p>
<p><code>poetry export -f requirements.txt --output requirements.txt --without-hashes</code></p>"
"Google Cloud Build show no logs<p>I have a Google Cloud Trigger that triggers cloud build on Github push.</p>
<p>The problem is that the Cloud Build shows no logs. I followed <a href=""https://cloud.google.com/build/docs/securing-builds/store-manage-build-logs#yaml"" rel=""nofollow noreferrer"">this doc</a> but can not find any logs on neither the Cloud Build log nor the Logs Explorer (see the image below)</p>
<p><a href=""https://i.stack.imgur.com/Rs5VQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rs5VQ.png"" alt=""enter image description here"" /></a></p>
<p>This is my <code>cloudbuild.yaml</code></p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
# install dependencies
- name: node:16
  entrypoint: yarn
  args: []
# create .env file
- name: 'ubuntu'
  args: ['bash', './makeEnv.sh']
  env: 
    - 'GCP_SHOPIFY_STOREFRONT_ACCESS_TOKEN=$_GCP_SHOPIFY_STOREFRONT_ACCESS_TOKEN'
    - 'GCP_SHOPIFY_DOMAIN=$_GCP_SHOPIFY_DOMAIN'
# build code
- name: node:16
  entrypoint: yarn
  args: [&quot;build&quot;]
# deploy to gcp
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args: ['-c', 'gcloud config set app/cloud_build_timeout 1600 &amp;&amp; gcloud app deploy --promote']
timeout: &quot;1600s&quot;
options:
  logging: CLOUD_LOGGING_ONLY
</code></pre>
<p>The build failed but it actually create a subsequence App Engine build that successfully deploy a version to App Engine. But that version is not auto-promoted (see the image below)</p>
<p><a href=""https://i.stack.imgur.com/KtuEj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KtuEj.png"" alt=""enter image description here"" /></a></p>","<p>I do not have all the details, so trying to help with all the above information mentioned.</p>
<p>As I can see you are using <code>CLOUD_LOGGING_ONLY</code> and not been able to see the log in the log explorer and considering you have all the permissions to access the logs.</p>
<p>I would suggest you to look into the service account that you are using for cloud build must at least have the role:
<a href=""https://cloud.google.com/logging/docs/access-control#:%7E:text=roles/logging.logWriter%20(Logs%20Writer)%20grants%20principals%20the%20minimal%20permissions%20required%20to%20write%20logs%20to%20the%20Logging%20API.%20This%20role%20doesn%27t%20grant%20viewing%20permissions."" rel=""nofollow noreferrer"">role/logging.logWriter</a> or permission:<code>logging.logEntries.create</code> permission if it is <em>not</em> the default cloud build SA <code>project-number@cloudbuild.gserviceaccount.com</code>.</p>
<p>Hope this helps :)</p>"
"Access to private Maven Repository from Google Cloud Build?<p>We're using Google Cloud Build to build a Spring Boot application written in Java. However, we have a private Maven repository (hosted on Artifact Registry if that matters), and the application won't build unless it has access to this repository.</p>
<p>The <code>cloudbuild.yaml</code> file looks like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: maven:3.8.6-eclipse-temurin-17-alpine
    entrypoint: mvn
    args: [ 'clean', 'verify', '-Dmaven.test.skip=true' ]
</code></pre>
<p>Usually, I add the credentials to the private maven repository to the <code>~/.m2/settings.xml</code> file.</p>
<p>What's the recommended approach to give Maven access to a private Maven repository when building the project with Google Cloud Build?</p>","<p>You can create an additional <a href=""https://cloud.google.com/build/docs/build-config-file-schema"" rel=""nofollow noreferrer"">step in Google Cloud Build</a> to generate credentials and store them in the file (<code>~/.m2/settings.xml</code>), before running the maven step:</p>
<pre class=""lang-yaml prettyprint-override""><code>###### previous Cloud Build Steps ###

- name: 'bash'
  args: ['./cloudbuild_credentials.sh'] ### &lt;--- script to generate creds
  dir: 'src'                            ### &lt;--- directory might be different
  id: 'generate-credentials'
  env:
      - PRIVATE_REPO_PASS=$_PRIVATE_REPO_PASS ### &lt;--- keys might be passed to Cloud Build via Triggers

###### next Cloud Build Steps ###
</code></pre>
<p>An example how the script (<code>cloudbuild_credentials.sh</code>) might look like (generates and saves <code>~/.m2/settings.xml</code> file with the sensitive data):</p>
<pre class=""lang-bash prettyprint-override""><code>printf '
&lt;settings&gt;
    &lt;servers&gt;
        &lt;server&gt;
            &lt;id&gt;private-repo&lt;/id&gt;
            &lt;username&gt;xyz&lt;/username&gt;
            &lt;password&gt;%s&lt;/password&gt;
        &lt;/server&gt;
    &lt;/servers&gt;
&lt;/settings&gt;
' &quot;${PRIVATE_REPO_PASS}&quot; &gt; ~/.m2/settings2.xml
</code></pre>
<p>This way you commit only non-sensitive data to the repo, and you pass the key from the outside. For example via <a href=""https://cloud.google.com/build/docs/automating-builds/create-manage-triggers"" rel=""nofollow noreferrer"">Google Cloud Build Triggers</a>.</p>"
"Is there a Google CloudBuild equivalent for Manual Approval Action in AWS CodePipeline?<p>I am looking to setup a manual approval before the production build can be triggered from staging.</p>
<p>Is there any workaround to setup manual approval if Google doesn't support it natively?</p>
<p>AWS Manual Approval Action : <a href=""https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html</a></p>","<p>Unfortunately there is no equivalent of AWS Manual Approval Action for Cloud Build, however, you can integrate Cloud Build with a 3rd party tool to implement a deployment approval for different enviroments. <a href=""https://medium.com/segmentstream/simple-ci-cd-on-gke-using-cloudbuild-and-spinnaker-8470c7832c6"" rel=""nofollow noreferrer"">Here is an article</a> that uses Spinnaker for that.</p>
<p>You could also open a Feature Request so that an implementation of a native approval for deployment can be considered by Google's Product Team by following this <a href=""https://issuetracker.google.com/issues/new?component=190802&amp;template=0"" rel=""nofollow noreferrer"">link</a>.</p>"
"Connect to a gRPC service on a GKE cluster from Cloud Build<p>We’re using a managed Kubeflow Pipelines (KFP) instance, created with GCP’s AI Platform Pipelines, with a managed MySQL instance, created with Cloud SQL. We’re also using Cloud Build to build and run our KFP pipelines. We’d like to add a Cloud Build step that, after a KFP pipeline run finishes, runs a script that uses the MLMD API to query for that pipeline run’s metadata. We have an MLMD script that can successfully query for metadata when run manually on a GCP VM. The issue has been getting that script to run in Cloud Build.</p>
<p>The first approach was to create the <code>mlmd.metadata_store.MetadataStore</code> object using MySQL connection information, e.g.,</p>
<pre><code>connection_config = metadata_store_pb2.ConnectionConfig()
connection_config.mysql.host = [IP address]
connection_config.mysql.port = 3306
connection_config.mysql.database = &quot;kubeflow_experiments_metadata_metadata&quot;
connection_config.mysql.user = [user]
connection_config.mysql.password = [password]
store = mlmd.metadata_store.MetadataStore(connection_config)
</code></pre>
<p>This runs fine on a VM. It appears, however, that the Cloud SQL Proxy is required for this to work in Cloud Build. Using this <a href=""https://codelabs.developers.google.com/codelabs/connecting-to-cloud-sql-with-cloud-functions#2"" rel=""nofollow noreferrer"">Cloud Functions codelab</a> as an example, I was able to run a script in Cloud Build that used sqlalchemy to connect to MySQL via the proxy. The method for connecting sqlalchemy with the proxy, however, seems to be incompatible with the above MLMD API. It looks something like this:</p>
<pre><code>driver_name = &quot;mysql+pymysql&quot;
query_string = dict({&quot;unix_socket&quot;: &quot;/cloudsql/{}&quot;.format(connection_name)})
db = sqlalchemy.create_engine(
    sqlalchemy.engine.url.URL(drivername=driver_name, username=[user], password=[password], database=&quot;kubeflow_experiments_metadata_metadata&quot;, query=query_string),
    pool_size=5,
    max_overflow=2,
    pool_timeout=30,
    pool_recycle=1800,
)
</code></pre>
<p>The second approach uses the MLMD gRPC service that’s deployed with KFP. First, I port forward the service:</p>
<pre><code>kubectl port-forward svc/metadata-grpc-service 8080:8080
</code></pre>
<p>then create the <code>mlmd.metadata_store.MetadataStore</code> using the MLMD gRPC API:</p>
<pre><code>connection_config = metadata_store_pb2.MetadataStoreClientConfig(
    host=&quot;127.0.0.1&quot;,
    port=8080,
)
store = mlmd.metadata_store.MetadataStore(connection_config)
</code></pre>
<p>Again, this works fine on a VM. I’m not sure, however, how to connect to the gRPC service from Cloud Build. My experience with gRPC and Kubernetes is limited, so I wouldn’t be surprised if there was a straightforward solution.</p>
<p>Any advice would be greatly appreciated!</p>","<p>Through other avenues, I was pointed to this <a href=""https://fabiouechi.medium.com/using-google-cloud-build-for-kubeflow-pipelines-ci-cd-b1459b08b158"" rel=""nofollow noreferrer"">article</a>, which contains an example of how to port-forward KFP's ml-pipeline service in Cloud Build.  I had to make one minor modification, which was to remove the <code>&quot;-n&quot;</code> and <code>&quot;kubeflow&quot;</code> arguments to the <code>kubectl port-forward</code> command.  This specifies to <code>kubectl</code> to use the <code>&quot;kubeflow&quot;</code> namespace.  GCP's AI Platform Pipelines, however, appears to create a <code>&quot;default&quot;</code> namespace when it deploys your KFP instance.</p>"
"CICD on GCP for Firebase Functions failing due to ""replace /workspace/.runtimeconfig.json?"" error<p>I have CICD for cloud functions as some of the functions require token (collection deletion), my pipeline takes care of that.</p>
<p>Since last release, the cloud build is failing to deploy functions as it's expecting some key press which can't be done during cloud build. The actual log error says the following:</p>
<p>&quot;replace /workspace/.runtimeconfig.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: NULL&quot;</p>
<p>Locally I am able to run the Cloud Functions with Node 14, but on GCP I'm not able to find a fix for GCP Cloud Build.</p>
<p>**Note: **</p>
<ul>
<li><p>I'm using latest packages as of today. firebase-tools package: 9.4.0</p>
</li>
<li><p>There are similar questions for deploying functions locally but none of them work for me as it's during Cloud Build. I'm using the guide here: <a href=""https://cloud.google.com/build/docs/deploying-builds/deploy-firebase"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/deploying-builds/deploy-firebase</a></p>
<ul>
<li><a href=""https://github.com/firebase/firebase-tools/issues/3120"" rel=""nofollow noreferrer"">https://github.com/firebase/firebase-tools/issues/3120</a></li>
<li><a href=""https://stackoverflow.com/questions/65950774/firebase-functions-deploy-requires-keyboard-input-in-gcp"">Firebase Functions deploy requires keyboard input in GCP</a></li>
</ul>
</li>
</ul>","<p>Well the answers on other links mentioned above were correct indeed. What I didn't realize was that Docker image of node during build takes the latest automatically. Thus, now it was taking Node 15.xx version.</p>
<p>To fix this, just update Node version in the cloudbuild.yaml. For example following during npm install on GCP:</p>
<pre><code>- name: node:14
  entrypoint: npm
  waitFor: ['-']
  id: 'npm-install'
  dir: 'functions'
  args: ['install']
</code></pre>"
"Automatic job restart<p>I have a job that can take up to several hours. It is possible that for some reason (like out of memory, or cluster rebalance) it just fails. The problem is that the job is usually run overnight, and someone needs to check on it in the morning, and manually restart it (which most of the time is enough). I was wondering if this problem can be solved using spring cloud data flow.</p>
<p>Ideally, I would want SCDF to send an email (or call a webhook) when a job is done (failed or success), and retry an entire job if it fails. Is it possible to do that?</p>","<p>SCDF is a lightweight Spring Boot application, and it exposes a set of <a href=""https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#api-guide-resources"" rel=""nofollow noreferrer"">RESTful APIs</a>, so you can leverage the APIs to build the desired automation.</p>
<p>There's currently no in-built email functionality to automate this workflow out-of-the-box.</p>
<p>You could, however, write a small application that periodically interacts with SCDF's RESTful APIs, and depending on the desired stateful scenarios, you could kick off the email and/or relaunch operations.</p>"
"How to download Docker image after a failed Cloud Build step<p>I have a Cloud Build configuration with the following steps:</p>
<ol>
<li>Build a Docker image.</li>
<li>Run some test on that image.</li>
<li>Push the image to Container Registry.</li>
<li>Do some more stuff.</li>
</ol>
<p>The problem is that the build failed at the push step (number 3).<br />
Is there any way to download the created docker image or to change the command (args) for step 3 so that push won't fail and so I can retry the build from the step 3?</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  args: [
          'build',
          '--build-arg', 'base_image=${_DOCKER_BASE_IMAGE}',
          '--build-arg', 'cuda=${_CUDA}',
          '--build-arg', 'python_version=${_PYTHON_VERSION}',
          '--build-arg', 'cloud_build=true',
          '--build-arg', 'release_version=${_RELEASE_VERSION}',
          '-t', 'gcr.io/aluminiumponey/xla:${_IMAGE_NAME}',
          '-f', 'docker/Dockerfile', '.'
        ]
  timeout: 14400s
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: bash
  args: ['-c', 'docker tag gcr.io/aluminiumponey/xla:${_IMAGE_NAME} gcr.io/repsmate/xla:${_IMAGE_NAME}_$(date -u +%Y%m%d)']
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/aluminiumponey/xla']
  timeout: 1800s
- name: 'gcr.io/aluminiumponey/xla:${_IMAGE_NAME}'
  entrypoint: 'bash'
  args: ['-c', 'source /pytorch/xla/docker/common.sh &amp;&amp; collect_wheels ${_RELEASE_VERSION}']
</code></pre>
<p>The push failed because I didn't had permissions to push to gcr.io.</p>","<p>Cloud Build is a serverless environment and it is ephemeral. At the end of the build, the environment in cleaned up. So, NO, you can't get the container after the Cloud Build crash</p>
<p>However, you can imagine adding a new step between the step 1 and 2 (name it 1.5) to push the image to <a href=""https://cloud.google.com/container-registry/docs/pushing-and-pulling"" rel=""nofollow noreferrer"">Google Container Registry</a> (or the newest one <a href=""https://cloud.google.com/artifact-registry/docs/docker/pushing-and-pulling"" rel=""nofollow noreferrer"">Google Artifact Registry</a>)</p>"
"Google Cloud Build with kaniko cache fails<p>I'm using Google Cloud Build along with kaniko cache for speedup. Until recently if worked perfectly, but now it's failing with</p>
<pre><code>ERROR: build step 2 &quot;gcr.io/kaniko-project/executor:latest&quot; failed: step exited with non-zero status: 137
</code></pre>
<p>I assume, it's because builder is running out of memory</p>
<p>Is there a way to select higher level VM for this or increase memory?</p>
<p>I'm using the same standard configuration that is described <a href=""https://cloud.google.com/build/docs/kaniko-cache#:%7E:text=cache%20expiration%20time-,Overview,the%20open%20source%20tool%20Kaniko"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Thanks in advance</p>
<p>P.s. Regular builds with <code>gcr.io/cloud-builders/docker</code> are going through</p>","<p>Yes, this is possible, all you have to do is specify what machine type you want to use during your build at the build command, as you can see in this <a href=""https://cloud.google.com/build/docs/speeding-up-builds#using_custom_virtual_machine_sizes"" rel=""nofollow noreferrer"">documentation</a> where there are also examples with how to start builds with this special machines is gcloud, Yaml and JSON .</p>
<p>Currently the machine types available for this are</p>
<pre><code>UNSPECIFIED     Standard machine type.
N1_HIGHCPU_8    Highcpu machine with 8 CPUs.
N1_HIGHCPU_32   Highcpu machine with 32 CPUs.
E2_HIGHCPU_8    Highcpu e2 machine with 8 CPUs.
E2_HIGHCPU_32   Highcpu e2 machine with 32 CPUs.
</code></pre>
<p>And you can find more information on those specific types of machines in <a href=""https://cloud.google.com/compute/docs/machine-types"" rel=""nofollow noreferrer"">here</a>.</p>"
"Making authorised requests from Google Cloud Build<p>I'm trying to set up a deployment street in Google Cloud Build. To do this, I want to:</p>
<ol>
<li>Run unit test</li>
<li>Deploy to Cloud Run without traffic</li>
<li>Run integration tests</li>
<li>Migrate traffic in Cloud Run</li>
</ol>
<p>I've got this mostly set up, but my integration tests include a couple of calls to Cloud Run to validate that authenticated calls return 200 and unauthenticated return 401. The thing I'm having difficulties with is to make signed requests from Cloud Build. When deploying by hand and running integration tests, they work, but not from Cloud Build.</p>
<p>Ideally, I would like to use the Cloud Build Service Account for invoking Cloud Run like I usually do in AWS, but I can't find how to get access to that from the Cloud Runner. So instead, I retrieve a credentials file from Secret Manager. This credentials file is from a newly created Service Account with Cloud Run Invoker role:</p>
<pre><code>steps:
  - name: gcr.io/cloud-builders/gcloud
    id: get-github-ssh-secret
    entrypoint: 'bash'
    args: [ '-c', 'gcloud secrets version access latest --secret=name-of-secret &gt; /root/service-account/credentials.json' ]
    volumes:
      - name: 'service-account'
        path: /root/service-account
...
  - name: python:3.8.7
    id: integration-tests
    entrypoint: /bin/sh
    args:
      - '-c'
      - |-
        if [ $_STAGE != &quot;prod&quot; ]; then 
          python -m pip install -r requirements.txt
          python -m pytest test/integration --disable-warnings ; 
        fi
    volumes:
      - name: 'service-account'
        path: /root/service-account
</code></pre>
<p>For the integration tests, I've created a class called Authorizer and I have <code>__get_authorized_header_for_cloud_build</code> and <code>__get_authorized_header_for_cloud_build2</code> as attempts:</p>
<pre><code>import json
import time
import urllib
from typing import Optional

import google.auth
import requests
from google import auth
from google.auth.transport.requests import AuthorizedSession
from google.oauth2 import service_account
import jwt


class Authorizer(object):
    cloudbuild_credential_path = &quot;/root/service-account/credentials.json&quot;

    # Permissions to request for Access Token
    scopes = [&quot;https://www.googleapis.com/auth/cloud-platform&quot;]

    def get_authorized_header(self, receiving_service_url) -&gt; dict:
        auth_header = self.__get_authorized_header_for_current_user() \
                      or self.__get_authorized_header_for_cloud_build(receiving_service_url)
        return auth_header

    def __get_authorized_header_for_current_user(self) -&gt; Optional[dict]:
        credentials, _ = auth.default()
        auth_req = google.auth.transport.requests.Request()
        credentials.refresh(auth_req)
        if hasattr(credentials, &quot;id_token&quot;):
            authorized_header = {&quot;Authorization&quot;: f'Bearer {credentials.id_token}'}
            auth_req.session.close()
            print(&quot;Got auth header for current user with auth.default()&quot;)
            return authorized_header

    def __get_authorized_header_for_cloud_build2(self, receiving_service_url) -&gt; dict:
        credentials = service_account.Credentials.from_service_account_file(
            self.cloudbuild_credential_path, scopes=self.scopes)
        auth_req = google.auth.transport.requests.Request()
        credentials.refresh(auth_req)
        return {&quot;Authorization&quot;: f'Bearer {credentials.token}'}

    def __get_authorized_header_for_cloud_build(self, receiving_service_url) -&gt; dict:
        with open(self.cloudbuild_credential_path, 'r') as f:
            data = f.read()
        credentials_json = json.loads(data)

        signed_jwt = self.__create_signed_jwt(credentials_json, receiving_service_url)
        token = self.__exchange_jwt_for_token(signed_jwt)
        return {&quot;Authorization&quot;: f'Bearer {token}'}

    def __create_signed_jwt(self, credentials_json, run_service_url):
        iat = time.time()
        exp = iat + 3600
        payload = {
            'iss': credentials_json['client_email'],
            'sub': credentials_json['client_email'],
            'target_audience': run_service_url,
            'aud': 'https://www.googleapis.com/oauth2/v4/token',
            'iat': iat,
            'exp': exp
        }
        additional_headers = {
            'kid': credentials_json['private_key_id']
        }
        signed_jwt = jwt.encode(
            payload,
            credentials_json['private_key'],
            headers=additional_headers,
            algorithm='RS256'
        )
        return signed_jwt

    def __exchange_jwt_for_token(self, signed_jwt):
        body = {
            'grant_type': 'urn:ietf:params:oauth:grant-type:jwt-bearer',
            'assertion': signed_jwt
        }
        token_request = requests.post(
            url='https://www.googleapis.com/oauth2/v4/token',
            headers={
                'Content-Type': 'application/x-www-form-urlencoded'
            },
            data=urllib.parse.urlencode(body)
        )
        return token_request.json()['id_token']

</code></pre>
<p>So when running locally, the <code>__get_authorized_header_for_current_user</code> is being used and works. When running in Cloud Build, <code>__get_authorized_header_for_cloud_build</code> is used. But even when temporarily disabling <code>__get_authorized_header_for_current_user</code> and let cloudbuild_credential_path reference to a json-file on my local pc, it keep getting 401s. Even when I give the service account from the credentials-file Owner rights. Another attempt is <code>__get_authorized_header_for_cloud_build</code> where I try to get the token more by myself instead of a package, but still 401.</p>
<p>For completeness, the integration test look somewhat like this:</p>
<pre><code>class NameOfViewIntegrationTestCase(unittest.TestCase):
    base_url = &quot;https://**.a.run.app&quot;
    name_of_call_url = base_url + &quot;/name-of-call&quot;

    def setUp(self) -&gt; None:
        self._authorizer = Authorizer()

    def test_name_of_call__authorized__ok_result(self) -&gt; None:
        # Arrange
        url = self.name_of_call_url 

        # Act
        response = requests.post(url, headers=self._authorizer.get_authorized_header(url))

        # Arrange
        self.assertTrue(response.ok, msg=f'{response.status_code}: {response.text}')
</code></pre>
<p>Any idea what I'm doing wrong here? Let me know if you need any clarification on something. Thanks in advance!</p>","<p>Firstly, your code is too complex. If you want to leverage the Application Default Credential (ADC) according with the runtime environment, only these lines are enough</p>
<pre><code>from google.oauth2.id_token import fetch_id_token
from google.auth.transport import requests
r = requests.Request()
print(fetch_id_token(r,&quot;&lt;AUDIENCE&gt;&quot;))
</code></pre>
<p>On Google Cloud Platform, the environment service account will be used thanks to the <a href=""https://cloud.google.com/compute/docs/storing-retrieving-metadata"" rel=""noreferrer"">metadata server</a>. On your local environment,  you need to set the environment variable <code>GOOGLE_APPLICATION_CREDENTIALS</code> with as value the path of the service account key file</p>
<p><em>Note: you can generate id_token only with service account credential (on GCP or on your environment), it's not possible with your user account</em></p>
<hr />
<p>The problem here, it's that doesn't work on Cloud Build. I don't know why, but it's not possible to generate an id_token with the Cloud Build metadata server. So, I wrote <a href=""https://medium.com/google-cloud/service-account-credentials-api-a-solution-to-different-issues-dc7434037115"" rel=""noreferrer"">an article on this</a> with a possible workaround</p>"
"How to deploy Laravel 8 google cloud run with google cloud database<p>Iam looking for help to containerize a laravel application with docker, running it locally and make it deployable to gcloud Run, connected to a gcloud database.
My application is an API, build with laravel, and so far i have just used the docker-compose/sail package, that comes with laravel 8, in the development.</p>
<p>Here is what i want to achieve:</p>
<ul>
<li>Laravel app running on gcloud Run.</li>
<li>Database in gcloud, Mysql, PostgreSQL or SQL server. (prefer Mysql).</li>
<li>Enviroment stored in gcloud.</li>
</ul>
<p>My problem is can find any info if or how to use/rewrite the docker-composer file i laravel 8, create a Dockerfile or cloudbuild file, and build it for gcloud.</p>
<p>Maybe i could add something like this in a cloudbuild.yml file:</p>
<pre><code>#cloudbuild.yml
  steps:
  # running docker-compose
  - name: 'docker/compose:1.26.2'
    args: ['up', '-d']
</code></pre>
<p>Any help/guidanceis is appreciated.</p>","<p>As mentioned in the comments to this question you can check this <a href=""https://www.youtube.com/watch?v=KCWGJV3x1Rs"" rel=""nofollow noreferrer"">video</a> that explains how you can use docker-composer, laravel to deploy an app to Cloud Run with a step-by-step tutorial.</p>
<p>As per database connection to said app, the <a href=""https://cloud.google.com/sql/docs/mysql/connect-run"" rel=""nofollow noreferrer"">Connecting from Cloud Run (fully managed)  to Cloud SQL documentation</a> is quite complete on that matter and for secret management I found <a href=""https://medium.com/google-cloud/secret-manager-improve-cloud-run-security-without-changing-the-code-634f60c541e6"" rel=""nofollow noreferrer"">this article</a> that explains how to implement secret manager into Cloud Run.</p>
<p>I know this answer is basically just links to the documentation and articles, but I believe all the information you need to implement your app into Cloud Run is in those.</p>"
"How can I run beta gcloud component like ""gcloud beta artifacts docker images scan"" within Cloud Build?<p>I am trying to include the Container Analyis API <a href=""https://cloud.google.com/container-analysis/docs/ods-quickstart"" rel=""nofollow noreferrer"">link</a> in a Cloud Build pipeline.This is a beta component and with command line I need to install it first:</p>
<pre><code>gcloud components install beta local-extract
</code></pre>
<p>then I can run the on demand container analyis (if the container is present locally):</p>
<pre><code>gcloud beta artifacts docker images scan ubuntu:latest
</code></pre>
<p>My question is how I can use component like <strong>beta local-extract</strong> within Cloud Build ?</p>
<p>I tried to do a fist step and install the missing componentL</p>
<pre><code>## Update components
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['components', 'install', 'beta', 'local-extract', '-q']
  id: Update component
</code></pre>
<p>but as soon as I move to the next step the update is gone (since it is not in the container)</p>
<p>I also tried to install the component and then run the scan using (&amp; or ;) but it is failling:</p>
<pre><code>## Run vulnerability scan
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['components', 'install', 'beta', 'local-extract', '-q', ';', 'gcloud', 'beta', 'artifacts', 'docker', 'images', 'scan', 'ubuntu:latest', '--location=europe']

  id: Run vulnaribility scan
</code></pre>
<p>and I get:</p>
<pre><code>Already have image (with digest): gcr.io/cloud-builders/gcloud
ERROR: (gcloud.components.install) unrecognized arguments:
  ;
  gcloud
  beta
  artifacts
  docker
  images
  scan
  ubuntu:latest
  --location=europe (did you mean '--project'?)
  To search the help text of gcloud commands, run:
  gcloud help -- SEARCH_TERMS
</code></pre>
<p>so my question are:</p>
<ol>
<li>how can I run &quot;gcloud beta artifacts docker images scan ubuntu:latest&quot; within Cloud Build ?</li>
<li>bonus: from the previous command how can I get the &quot;scan&quot; output value that I will need to pass as a parameter to my next step ? (I guess it should be something with --format)</li>
</ol>","<p>You should try the <code>cloud-sdk</code> docker image:</p>
<p><a href=""https://github.com/GoogleCloudPlatform/cloud-sdk-docker"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-sdk-docker</a></p>
<p>The Cloud Build team (implicitly?) recommends it:</p>
<p><a href=""https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/gcloud"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/gcloud</a></p>
<p>With the <code>cloud-sdk-docker</code> container you can change the entrypoint to <code>bash</code> pipe <code>gcloud</code> commands together.  Here is an (ugly) example:</p>
<p><a href=""https://github.com/GoogleCloudPlatform/functions-framework-cpp/blob/d3a40821ff0c7716bfc5d2ca1037bcce4750f2d6/ci/build-examples.yaml#L419-L432"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/functions-framework-cpp/blob/d3a40821ff0c7716bfc5d2ca1037bcce4750f2d6/ci/build-examples.yaml#L419-L432</a></p>
<p>As to your bonus question.  Yes, <code>--format=value(the.name.of.the.field)</code> is probably what you want. The trick is to know the name of the field. I usually start with <code>--format=json</code> on my development workstation to figure out the name.</p>"
"How to increase the cloud build timeout when using ```gcloud run deploy```?<p>When attempting to deploy to Cloud Run using the <code>gcloud run deploy</code> I am hitting the 10m Cloud Build timeout limit.  <code>gcloud run deploy</code> is working well as long as the build step does not exceed 10m.  When the build step exceeds 10m, the build fails with the &quot;Timed out&quot; status as shown in below screenshot.  AFAIK there are no arguments to <code>gcloud run deploy</code> that can set the Cloud Build timeout limit.  <code>gcloud run deploy</code> docs are here: <a href=""https://cloud.google.com/sdk/gcloud/reference/run/deploy"" rel=""nofollow noreferrer"">https://cloud.google.com/sdk/gcloud/reference/run/deploy</a></p>
<p>I've attempted to increase the Cloud Build timeout limit using <code>gcloud config set builds/timeout 20m</code> and <code>gcloud config set container/build_timeout 20m</code>, but these settings are not reflected in the execution details of the cloud build process when using <code>gcloud run deploy</code>.</p>
<p>In the GUI, this is the setting I want to change:
<a href=""https://i.stack.imgur.com/po4s5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/po4s5.png"" alt=""enter image description here"" /></a></p>
<p>Is it possible to increase the Cloud Build timeout limit using <code>gcloud run deploy</code>?</p>","<p>How about splitting the command into (more easily configured) constituents?</p>
<p><em>[I've not tried this]</em></p>
<p>Build the container image specifying the <a href=""https://cloud.google.com/sdk/gcloud/reference/builds/submit#--timeout"" rel=""nofollow noreferrer"">timeout</a>
:</p>
<pre class=""lang-sh prettyprint-override""><code>gcloud builds submit --source=.... --timeout=...
</code></pre>
<p>Then reference the <a href=""https://cloud.google.com/sdk/gcloud/reference/run/deploy#--image"" rel=""nofollow noreferrer"">image</a> that results when you <code>gcloud run deploy</code>:</p>
<pre class=""lang-sh prettyprint-override""><code>gcloud run deploy ... --image=...
</code></pre>"
"Do I need to add node_modules to .gcloudignore when running ""gcloud builds submit ./project-folder""?<p>This is my project structure:</p>
<pre><code>project/
  node_modules/
  src/
  .gcloudignore
  cloudbuild.yaml
  Dockerfile
  package.json
</code></pre>
<p>Here is how I'm building it:</p>
<pre><code>gcloud builds submit ./project --config=./project/cloudbuild.yaml --project=$PROJECT_ID   // AND SOME SUBSTITUTIONS
</code></pre>
<p>This is my <code>cloudbuild.yaml</code> file:</p>
<pre><code>steps:
  # BUILD IMAGE
  - name: &quot;gcr.io/cloud-builders/docker&quot;
    args:
      - &quot;build&quot;
      - &quot;--tag&quot;
      - &quot;gcr.io/$PROJECT_ID/$_SERVICE_NAME:$_TAG_NAME&quot;
      - &quot;.&quot;
    timeout: 180s

    # PUSH IMAGE TO REGISTRY
  - name: &quot;gcr.io/cloud-builders/docker&quot;
    args:
      - &quot;push&quot;
      - &quot;gcr.io/$PROJECT_ID/$_SERVICE_NAME:$_TAG_NAME&quot;
    timeout: 180s

    # DEPLOY CONTAINER WITH GCLOUD
  - name: &quot;gcr.io/google.com/cloudsdktool/cloud-sdk&quot;
    entrypoint: gcloud
    args:
      - &quot;run&quot;
      - &quot;deploy&quot;
      - &quot;$_SERVICE_NAME&quot;
      - &quot;--image=gcr.io/$PROJECT_ID/$_SERVICE_NAME:$_TAG_NAME&quot;
      - &quot;--platform=managed&quot;
      - &quot;--region=$_REGION&quot;
      - &quot;--port=8080&quot;
      - &quot;--allow-unauthenticated&quot;
    timeout: 180s

# DOCKER IMAGES TO BE PUSHED TO CONTAINER REGISTRY
images: 
  - &quot;gcr.io/$PROJECT_ID/$_SERVICE_NAME:$_TAG_NAME&quot;
</code></pre>
<p>And here is my <code>Dockerfile</code>:</p>
<pre><code>FROM node:12-slim

WORKDIR /

COPY ./package.json ./package.json
COPY ./package-lock.json ./package-lock.json

COPY ./src ./src

RUN npm ci
</code></pre>
<p>From my configuration file, since nothing is being told to <code>copy</code> the <code>node_modules</code> folder, it seems unnecessary to add <code>node_modules</code>  to <code>.gcloudignore</code>. But is it?</p>
<p>I'm asking this because I saw this <a href=""https://stackoverflow.com/a/64083115/10128619"">answer</a> that said:</p>
<blockquote>
<p>When you run gcloud builds submit... you provide some source code and either a Dockerfile or a configuration file. The former is a simple case of the second, a configuration file containing a single step that runs docker build....</p>
<p>Configuration files (YAML) list a series of container images with parameters that are run in series. Initially Cloud Build <em><strong>copies a designated source</strong></em> (can be the current directory) to a Compute Engine VM (created by the service) as a directory (that's automatically mounted into each container) as /workspace.</p>
</blockquote>
<p>If it copies the source, will it copy <code>node_modules</code> as well? Should I add it to <code>.gcloudignore</code> or is it not necessary?</p>","<p>Yes you can skip the node module because you don't use them in your build (and it's huge and long to upload). In your command <code>npm ci</code>, I'm sure you download the dependencies, so, add the <code>node_modules</code> to your <code>.gcloudignore</code> (and <code>.gitignore</code> also)</p>"
"Set HTTPS flag in a Cloud Function with in a gcloud deploy instrucction<p>I'm trying to deploy a GCP Cloud Function</p>
<blockquote>
<p>gcloud functions deploy MyFuntion --set-env-vars
EXPRESSPORT=0000,CODEENV=PRE --region=europe-west1 --entry-point MyPoint
--runtime nodejs12 --trigger-http --allow-unauthenticated --ingress-settings=all --memory=256 --timeout=30</p>
</blockquote>
<p>But I can't find the flag to set the &quot;Required HTTPS&quot; that you can set in the Web Console:</p>
<p><a href=""https://i.stack.imgur.com/ikEvm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ikEvm.png"" alt=""enter image description here"" /></a></p>
<p>Does anyone know the instruction?, I can't find it in de Google Cloud documentation.</p>
<p>Thanks in advance.</p>","<p><a href=""https://cloud.google.com/functions/docs/writing/http#security_levels"" rel=""nofollow noreferrer"">https://cloud.google.com/functions/docs/writing/http#security_levels</a></p>
<pre class=""lang-sh prettyprint-override""><code>gcloud beta functions deploy \
--security-level=secure-always \
...
</code></pre>
<blockquote>
<p><strong>NOTE</strong> <code>gcloud beta</code></p>
<p><a href=""https://cloud.google.com/sdk/gcloud/reference/beta/functions/deploy#--security-level"" rel=""nofollow noreferrer"">https://cloud.google.com/sdk/gcloud/reference/beta/functions/deploy#--security-level</a></p>
</blockquote>"
"How to use Kaniko in cloudbuild.yaml?<p>I just learned that one can speed up the build process in Google Cloud build by using Kaniko cache. I looked at the docs and it provided a small example. However, I'm not sure how to apply it in my use case. I am basically pushing a Nuxt app into my Github repo and cloud builds it every time I make a push. The docs example says we need to replace <code>cloud-builders/docker</code> with <code>kaniko-project/executor:latest</code>. Below is a snippet of my <code>cloudbuild.yaml</code></p>
<pre><code>steps:
# Create .npmrc file from Fontawesome secret
- name: gcr.io/cloud-builders/gcloud
  entrypoint: 'bash'
  args: [ '-c', 'gcloud secrets versions access latest --secret=fontawesome &gt; .npmrc' ]
# Build the container image
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/PROJECTNAME/IMAGENAME:$COMMIT_SHA', '.']
# Push the image to Container Registry
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/PROJECTNAME/IMAGENAME:$COMMIT_SHA']
</code></pre>
<p>Kaniko docs says I need the following:</p>
<pre><code>steps:
- name: 'gcr.io/kaniko-project/executor:latest'
  args:
  - --destination=gcr.io/$PROJECT_ID/image
  - --cache=true
  - --cache-ttl=XXh
</code></pre>
<p>This is what I tried (but not sure if that's how it should be):</p>
<pre><code>steps:
    # Create .npmrc file from Fontawesome secret
    - name: gcr.io/cloud-builders/gcloud
      entrypoint: 'bash'
      args: [ '-c', 'gcloud secrets versions access latest --secret=fontawesome &gt; .npmrc' ]
    # Build the container image
    - name: 'gcr.io/kaniko-project/executor:latest'
      args: ['--destination=gcr.io/$PROJECT_ID/image', '--cache=true', '--cache-ttl=6h'
,'build', '-t', 'gcr.io/PROJECTNAME/IMAGENAME:$COMMIT_SHA', '.']
    # Push the image to Container Registry
    - name: 'gcr.io/kaniko-project/executor:latest'
      args: ['--destination=gcr.io/$PROJECT_ID/image', '--cache=true', '--cache-ttl=6h'
, 'push', 'gcr.io/PROJECTNAME/IMAGENAME:$COMMIT_SHA']
</code></pre>","<p>Kaniko doesn't have push and build command. It will do that implicitly (build and push) when you specify it as a build step in cloudbuild.yaml.</p>
<p>an example would be:</p>
<pre><code>steps:
  # Build the container image and push it with Kaniko
  - name: 'gcr.io/kaniko-project/executor:latest'
    args:
      [
        &quot;--dockerfile=&lt;DOCKER-FILE-DIST&gt;&quot;,
        &quot;--context=dir://&lt;BUILD_CONTEXT&gt;&quot;,
        &quot;--cache=true&quot;,
        &quot;--cache-ttl=6h&quot;,
        &quot;--destination=gcr.io/$PROJECT_ID/hello:$COMMIT_SHA&quot;
      ]
  # Deploy image to Cloud Run
  - name: &quot;gcr.io/cloud-builders/gcloud&quot;
    args:
      - &quot;run&quot;
      - &quot;deploy&quot;
      - &quot;hello&quot;
      - &quot;--image&quot;
      - &quot;gcr.io/$PROJECT_ID/hello:$COMMIT_SHA&quot;
      - &quot;--region&quot;
      - &quot;us-central1&quot;
      - &quot;--platform&quot;
      - &quot;managed&quot;
</code></pre>"
"Google Cloud Build - Pass environment variable for Dockerfile<p>I´m trying to dockerize my Angular application for Cloud Run and then conditionally build it with the production or the development configuration based on an environment variable.</p>
<p><em>cloud build command</em>:</p>
<p><code>gcloud builds submit --tag gcr.io/project-id/image-id --timeout=1200</code></p>
<p><em>Dockerfile</em>:</p>
<pre><code>FROM node:14

WORKDIR usr/src/app

COPY package*.json ./

RUN npm install

# Copy local code to the container
COPY . .

# Build app
RUN if [ &quot;$ENV&quot; = &quot;development&quot; ] ; then npm run build-dev:ssr ; else npm run build-prod:ssr ; fi

CMD [&quot;npm&quot;, &quot;run&quot;, &quot;serve:ssr&quot;]
</code></pre>
<p>At the moment, the <code>$ENV</code> doesn´t exist, but is there a way to pass this with the <code>gcloud build submit</code> command?</p>","<p>You can't pass build argument as you do with docker build command. To achieve this, you need to create a simple Cloud Build file</p>
<pre><code>steps:
  - name: 'gcr.io/cloud-builders/docker'
    args: [ 'build', '-t', 'gcr.io/project-id/image-id', '--build-arg=ENV=$_MY_VARIABLE', '.' ]
  # push the container image to Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/project-id/image-id']
images:
  - 'gcr.io/project-id/image-id'
timeout: 1200s
substitutions:
  _MY_VARIABLE: default value if not passed in the build command
</code></pre>
<p>And run it like this</p>
<pre><code>gcloud builds submit --substitutions=_MY_VARIABLE=specific_value
</code></pre>"
"PROJECT_ID env and Secret Manager Access<p>I would like to use the Secret Manager to store a credential to our artifactory, within a cloud build step. I have it working using a build similar to:</p>
<pre><code>steps:
- name: 'busybox:glibc'
  entrypoint: 'sh'
  args: ['-c', 'env']
  secretEnv: ['SECRET_VALUE']
availableSecrets:
  secretManager:
  - versionName: &quot;projects/PROJECT_ID/secrets/TEST-SECRET/versions/1&quot;
    env: 'SECRET_VALUE'
</code></pre>
<p>All great, no problems - I then try and slightly improve it to:</p>
<pre><code>steps:
- name: 'busybox:glibc'
  entrypoint: 'sh'
  args: ['-c', 'env']
  secretEnv: ['SECRET_VALUE']
availableSecrets:
  secretManager:
  - versionName: &quot;projects/$PROJECT_ID/secrets/TEST-SECRET/versions/1&quot;
    env: 'SECRET_VALUE'
</code></pre>
<p>But then it throws the error:
<code>ERROR: (gcloud.builds.submit) INVALID_ARGUMENT: failed to get secret name from secret version &quot;projects/$PROJECT_ID/secrets/TEST-SECRET/versions/1&quot;</code></p>
<p>I have been able to add a TRIGGER level env var (SECRET_MANAGER_PROJECT_ID), and that works fine. The only issue that as that is a trigger env, it is not available on rebuild, which breaks a lot of things.</p>
<p>Does anyone know how to get the PROJECT_ID of a Secret Manager from within CloudBuild without using a Trigger Param?</p>","<p>For now, it's not possible to set dynamic value in the secret field. I already provided this feedback directly to the Google Cloud PM, it has been take into account, but I don't have more info to share, especially for the availability.</p>
<hr />
<p><strong>EDIT 1</strong></p>
<p><em>(January 22)</em>. Thanks to Seza443 comment, I tested again and now it works with automatically populated variable (PROJECT_ID and PROJECT_NUMBER), but also with customer defined substitution variables!</p>"
"Unity cloud build bash script to modify files, using the $WORKSPACE variable<p>I'm trying to create a shell script that runs only on the prod versions of the cloud build targets. Its goal is to replace the bundle id and firebase config files for the production values. I'm using the $WORKSPACE environment variable to achieve some basic file manipulation using bash. <strong>It seems to be working fine for the android build, but it's erroring for the ios build.</strong></p>
<p>Here's the simple script:</p>
<pre class=""lang-sh prettyprint-override""><code> 
#!/bin/sh
 
echo $WORKSPACE
 
mv $WORKSPACE/client/Assets/google-services.json_prod $WORKSPACE/client/Assets/google-service.json
mv $WORKSPACE/client/Assets/GoogleService-Info.plist_prod $WORKSPACE/client/Assets/GoogleService-Info.plist
 
sed -i 's/Android: redacted/Android: redacted/g' $WORKSPACE/client/ProjectSettings/ProjectSettings.asset
sed -i 's/iPhone: redacted/iPhone: hredacted/g' $WORKSPACE/client/ProjectSettings/ProjectSettings.asset
sed -i 's/overrideDefaultApplicationIdentifier: 0/overrideDefaultApplicationIdentifier: 1/g' $WORKSPACE/client/ProjectSettings/ProjectSettings.asset
sed -i 's/productName: redacted: redacted/g' $WORKSPACE/client/ProjectSettings/ProjectSettings.asset

</code></pre>
<p>On ios build, here's the output:</p>
<pre><code>Executing Pre-Build Script at Assets/Shell/pre_build.sh
73: /BUILD_PATH/unity_4b7b6722eef28df6b99d.baseball-rivalszk5qkewb
74: sed: orkspace/workspace/unity_4b7b6722eef28df6b99d.baseball-rivalszk5qkewb/client/ProjectSettings/ProjectSettings.asset: No such file or directory
75: sed: orkspace/workspace/unity_4b7b6722eef28df6b99d.baseball-rivalszk5qkewb/client/ProjectSettings/ProjectSettings.asset: No such file or directory
76: sed: orkspace/workspace/unity_4b7b6722eef28df6b99d.baseball-rivalszk5qkewb/client/ProjectSettings/ProjectSettings.asset: No such file or directory
77: sed: orkspace/workspace/unity_4b7b6722eef28df6b99d.baseball-rivalszk5qkewb/client/ProjectSettings/ProjectSettings.asset: No such file or directory
78: ! build of 'ios-prod' failed. Pre-Build script exited with status 1. Aborting.
</code></pre>",<p>I ended up removing the $WORKSPACE variable from the script and using only a relative path like this: <code>./client/Assets...</code> and it worked.</p>
"--source in cloudbuild.yaml<p>I have my current Cloud Build working, I connect my github repo to trigger the Cloud Build when I push to the <code>main</code> branch which then creates my Cloud Function, but I am confused about the the <code>--source</code> flag.  I have read <a href=""https://cloud.google.com/sdk/gcloud/reference/functions/deploy#--source"" rel=""nofollow noreferrer"">the google cloud function docs</a>.  They state that the
minimal source repository URL is: <code>https://source.developers.google.com/projects/${PROJECT}/repos/${REPO}</code>.  If I were to input this into my <code>cloudbuild.yaml</code> file, does this mean that I am mimicking the complete path of my github url?  I am currently just using <code>.</code> which I believe is just the entire root directory.</p>
<p>my cloudbuild.yaml file:</p>
<pre><code>steps:
  - name: &quot;gcr.io/cloud-builders/gcloud&quot;
    id: &quot;deploypokedex&quot;
    args:
      - functions
      - deploy
      - my_pokedex_function
      - --source=.
      - --entry-point=get_pokemon
      - --trigger-topic=pokedex
      - --timeout=540s
      - --runtime=python39
      - --region=us-central1
</code></pre>","<p>Yes you are mimicking the complete path of the Github URL. <code>--source=.</code> means that you are calling the source code in your current working directory. You can check this link on <a href=""https://cloud.google.com/build/docs/deploying-builds/deploy-functions#configuring_the_deployment"" rel=""nofollow noreferrer"">how to configure the Cloud Build deployment</a>.</p>
<p>Also based on the <a href=""https://cloud.google.com/sdk/gcloud/reference/functions/deploy#--source"" rel=""nofollow noreferrer"">documentation</a> you provided,</p>
<blockquote>
<p>If you do not specify the <code>--source</code> flag:</p>
<ul>
<li>The current directory will be used for new function deployments.</li>
<li>If the function was previously deployed using a local filesystem path, then the function's source code will be updated using the current directory.</li>
<li>If the function was previously deployed using a Google Cloud Storage location or a source repository, then the function's source code will not be updated.</li>
</ul>
</blockquote>
<p>Let me know if you have questions or clarifications.</p>"
"Cloudbuild - how to set a variable, looping multiples paths (folders)<p>i am trying to configure a variable in this case _PATH using cloudbuild. I've multiple paths (folders) on my github repo with tf files, and want to the terraform recognize any change that have been made on any folder  at the moment to push and trigger.</p>
<p>I was wondering if there is any way to looping values ​​separated by &quot;comma&quot; on trigger options and then use &quot;for&quot; in bash script.., or perhaps exists another better way that i dont really know yet,</p>
<p>Thanks for the help!</p>
<p><a href=""https://i.stack.imgur.com/cmMXi.jpg"" rel=""nofollow noreferrer"">code cloudbuild</a></p>
<p><a href=""https://i.stack.imgur.com/GI12Y.jpg"" rel=""nofollow noreferrer"">cloudbuild sample</a></p>","<p>Sadly, I haven't found a way yet to set variables at the <code>cloudbuild.yaml</code> level.</p>
<p>Note <code>CloudBuild</code> was originally called <code>Cloud Container Builder</code>, which is why it acts differently than other CI/CD tools.</p>
<p>I think there may be another way to get the behavior you want though:</p>
<ol>
<li>Implement the <code>bash looping logic</code> in a script (ie. <code>sh/run_terraform_applys.sh</code>), and create a Dockerfile for it in your repo:</li>
</ol>
<pre><code>FROM hashicorp/terraform:1.0.0

WORKDIR /workspace

COPY sh/                           /workspace/sh/
COPY requirements.txt              /workspace/
RUN pip install -r requirements.txt
RUN . sh/run_terraform_applys.sh

COPY .                            /workspace/
RUN . sh/other_stuff_to_do.sh
</code></pre>
<ol start=""2"">
<li>Use the cloud-builders image to build your image and as a consequence the <code>docker build</code> will run <code>sh/run_terraform_applys.sh</code> within the Docker image (You can <code>push</code> your image to GCR to allow for layer-caching):</li>
</ol>
<pre><code>  - name: 'gcr.io/cloud-builders/docker'
    entrypoint: 'bash'
    args: 
      - '-c'
      - |-
        # Build Dockerfile
        docker build -t ${MY_GCR_CACHE}  --cache-from ${MY_GCR_CACHE} -f cicd/Dockerfile .
        docker push ${MY_GCR_CACHE}
    id: 'Run Terraform Apply'
    waitFor: ['-']
</code></pre>"
"How to use docker image from jfrog repository on Google App Engine?<p>How to deploy a docker image from jfrog repository on Google App Engine?</p>
<p>What I am trying to achieve is deploying my docker image that is stored on a jfrog arctifatory to a Good App Engine. Though all the examples I find are pushing the image to Artifact Registry which is redundant as I only want to store the artifact on jfrog. Did anyone try to do it before.
Here is the further I could go using Cloud Build:</p>
<pre><code>  - name: 'gcr.io/cloud-builders/docker'
    dir: /workspace/app
    args: [ 'pull', 'myjfrogurl.jfrog.io/$PROJECT_ID:$BRANCH_NAME' ]
  
</code></pre>
<p>Then I use terraform later to deploy:</p>
<pre><code>resource &quot;google_app_engine_flexible_app_version&quot; &quot;app_deploy&quot; {

  version_id = &quot;v1&quot;
  service    = var.service_name
  runtime    = &quot;nodejs&quot;

   ... 

  deployment {

    container {
      # Here is the problem as it needs to be a google URI
      image = &quot;myjfrogurl.jfrog.io/${var.project_id}:${var.branch_name}&quot;
    }
  }
</code></pre>
<p>Maybe there is a way of doing that, it doesn't need to be via terraform or cloud build.</p>
<p><strong>Edit</strong>
With the following code is possible to pull the image from jfrog and push to Container Registry where it will be visible for App Engine or Cloud Run, though as the answer says it is not possible to keep the image stored in only one place</p>
<pre><code>  # Pull from external repository
  - name: 'gcr.io/cloud-builders/docker'
    args: [ 'pull', 'myjfrogurl.jfrog.io/$PROJECT_ID:$BRANCH_NAME' ]

  # Do a fast build using --cache-from
  - name: 'gcr.io/cloud-builders/docker'
    args: [ 'build',
            '-t', 'gcr.io/$PROJECT_ID/appName:$BRANCH_NAME',
            '--cache-from', 'gcr.io/$PROJECT_ID/appName:$BRANCH_NAME',
            '.' ]
            
  # Tag the image for Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args: ['tag',
           'gcr.io/$PROJECT_ID/appName:$BRANCH_NAME']

  # Push to the Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push',  'gcr.io/$PROJECT_ID/appName:$BRANCH_NAME']
</code></pre>","<p>Posting Guillaume Blaquiere's upvoted comment as a Community Wiki answer for better visibility for the community:</p>
<p>This is not possible for App Engine, and there is the same limitation with Cloud Run.</p>
<p>Edit: To deploy an image to the App Engine it requires to push the image to the Google <a href=""https://cloud.google.com/container-registry/docs/overview"" rel=""nofollow noreferrer"">Container Registry</a>. Under the hood container registry is a GCP bucket called <code>eu.artifacts.projectId.appspot.com</code> or <code>artifacts.projectId.appspot.com</code> (according to the region - <a href=""https://support.terra.bio/hc/en-us/articles/360035638032-Publish-a-Docker-container-image-to-Google-Container-Registry-GCR-"" rel=""nofollow noreferrer"">more</a>). Artifacty Registry is a service under the Container Registry to help managing the images.</p>"
"Getting error 'Cannot find module' on server but local works<p>I'm creating this error in order to help other developer who could have the same problem. I spent some time on it and the solution is pretty easy.</p>
<p>The scenario was: everything was working fine locally and I double checked if the configuration in cloubuild was the same. (In my case, it happened on cloudbuild, but it could happen on any server)</p>
<p>But there, I got the following error:</p>
<pre class=""lang-sh prettyprint-override""><code>./src/app/shared/markdown-editor/markdown-editor.constant.ts:56:0-35 - Error: Module not found: Error: Can't resolve 'easyMDE' in '/workspace/frontend/src/app/shared/markdown-editor'

./src/app/shared/markdown-editor/markdown-editor.directive.ts:620:0-35 - Error: Module not found: Error: Can't resolve 'easyMDE' in '/workspace/frontend/src/app/shared/markdown-editor'

Error: src/app/shared/markdown-editor/markdown-editor.constant.ts:1:26 - error TS2307: Cannot find module 'easyMDE' or its corresponding type declarations.

1 import * as EasyMDE from 'easyMDE';
                           ~~~~~~~~~


Error: src/app/shared/markdown-editor/markdown-editor.directive.ts:2:26 - error TS2307: Cannot find module 'easyMDE' or its corresponding type declarations.

2 import * as EasyMDE from 'easyMDE';
</code></pre>
<p><strong>The point is:</strong>
Local everything was working but I was in a macOS and it is not case sensitive, but cloudbuild is linux based.</p>","<p>In order to solve it, check if the library you are using has capital letters and use only lower-case letters:</p>
<pre class=""lang-js prettyprint-override""><code>import * as EasyMDE from 'easymde';
</code></pre>
<p>instead of:</p>
<pre class=""lang-js prettyprint-override""><code>import * as EasyMDE from 'easyMDE';
</code></pre>"
"How can I authorize Cloud Build to deploy to Firebase Hosting in a different project?<p>I have GCP project <code>A</code> and Firebase project <code>B</code> (in a separate GCP project). I'm trying to use Cloud Build in <code>A</code> to build a web app and deploy it to Firebase Hosting in <code>B</code>.</p>
<p>In <code>B</code>'s IAM page, I have granted <code>A</code>'s <code>&lt;id&gt;@cloudbuild.gserviceaccount.com</code> service account the <code>API Keys Admin</code>, <code>Firebase Admin</code>, and <code>Service Account User</code> roles as described in e.g. <a href=""https://stackoverflow.com/questions/62676357/proper-permission-for-cloud-build-to-deploy-to-firebase"">this question</a>.</p>
<p>The final step in the Cloud Build config used by <code>A</code> is the following:</p>
<pre class=""lang-yaml prettyprint-override""><code>  - id: firebase_deploy
    name: gcr.io/$PROJECT_ID/firebase
    entrypoint: sh
    args:
      - '-c'
      - |
        firebase use $_FIREBASE_PROJECT_ID
        firebase target:apply hosting prod $_FIREBASE_HOSTING_TARGET
        firebase deploy --project=$_FIREBASE_PROJECT_ID --only=hosting,firestore:rules
</code></pre>
<p>I set the <code>_FIREBASE_PROJECT_ID</code> substitution variable to <code>B</code> and the <code>_FIREBASE_HOSTING_TARGET</code> variable to a Hosting alias that I use for the site.</p>
<p>When I trigger a build, it fails with the following error:</p>
<pre><code>...
Step #3 - &quot;firebase_deploy&quot;: Error: Invalid project selection, please verify project B exists and you have access.
Step #3 - &quot;firebase_deploy&quot;:
Step #3 - &quot;firebase_deploy&quot;: Error: Must have an active project to set deploy targets. Try firebase use --add
Step #3 - &quot;firebase_deploy&quot;:
Step #3 - &quot;firebase_deploy&quot;: Error: Failed to get Firebase project B. Please make sure the project exists and your account has permission to access it.
Finished Step #3 - &quot;firebase_deploy&quot;
</code></pre>
<p>I suspect that the problem may be that I'm not running the Firebase CLI's extra login step first. To do that, it seems that I would need to run <code>firebase login:ci</code> locally to generate a token and then pass it via the <code>FIREBASE_TOKEN</code> environment variable <a href=""https://github.com/firebase/firebase-tools#using-with-ci-systems"" rel=""nofollow noreferrer"">as described in the docs</a>, but the permissions associated with the token appear to be much broader than needed:</p>
<p><a href=""https://i.stack.imgur.com/ebsGp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ebsGp.png"" alt=""screenshot from token generation page"" /></a></p>
<p>The build process should only have access to Firebase project <code>B</code>, rather than &quot;all my Firebase data and settings&quot; and &quot;my Google Cloud data&quot;.</p>
<ol>
<li>Is there any way to avoid needing to run <code>firebase login</code> here? It seems like the service account should already have sufficient access to deploy to Firebase Hosting.</li>
<li>If I need to run <code>firebase login</code>, is there any way to create a token with a limited scope (assuming that my understanding of the default scope is correct)?</li>
</ol>
<p>(I've also given <code>B</code>'s service account the <code>Cloud Functions Developer</code> role in <code>A</code> and am able to successfully run <code>gcloud --project=$_FIREBASE_PROJECT_ID functions deploy ...</code> in a different build config. I'm also using a Cloud Build config similar to the one described above to deploy to Firebase Hosting in the same GCP project, so I suspect that <code>firebase login</code> isn't necessary in all cases.)</p>","<p>I found an approach that lets project <code>A</code>'s Cloud Build service account deploy to <code>B</code> without needing excessive permissions.</p>
<p>First, I created a service account named <code>deploy</code> under <code>B</code> and granted it the <code>Firebase Hosting Admin</code>, <code>Firebase Rules Admin</code>, and <code>Cloud Datastore Index Admin</code> roles. (I'm not sure whether the Datastore role is needed, but the console showed it as being used recently so I left it.)</p>
<p>Next, I generated a JSON key for the new service account, pasted it (including newlines and double-quotes) as a substitution variable named <code>_DEPLOY_CREDENTIALS</code>, and updated the build step to copy it to the environment:</p>
<pre class=""lang-yaml prettyprint-override""><code> - id: firebase_deploy
    name: gcr.io/$PROJECT_ID/firebase
    entrypoint: bash
    args: ['-e', '--', 'build/deploy_hosting.sh']
    env:
      - DEPLOY_CREDENTIALS=$_DEPLOY_CREDENTIALS
      - FIREBASE_PROJECT_ID=$_FIREBASE_PROJECT_ID
      - FIREBASE_HOSTING_TARGET=$_FIREBASE_HOSTING_TARGET
</code></pre>
<p>In <code>deploy_hosting.sh</code>, I write the credentials to a temporary file and then pass them to the <code>firebase</code> command via the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable:</p>
<pre class=""lang-sh prettyprint-override""><code>#!/bin/bash

set -e
    
CREDS=$(mktemp -t creds.json.XXXXXXXXXX)
printenv DEPLOY_CREDENTIALS &gt;&quot;$CREDS&quot;
export GOOGLE_APPLICATION_CREDENTIALS=$CREDS
      
firebase --debug use &quot;$FIREBASE_PROJECT_ID&quot;
firebase target:apply hosting prod &quot;$FIREBASE_HOSTING_TARGET&quot;
firebase deploy --project=&quot;$FIREBASE_PROJECT_ID&quot; --only=hosting,firestore:rules
</code></pre>
<p>I created a separate shell script for the step since I ran into problems with quotes being stripped from the credentials when writing them directly from the build step. It would likely be possible to store the credentials in <a href=""https://cloud.google.com/secret-manager"" rel=""nofollow noreferrer"">Secret Manager</a> instead, but that felt like overkill for my use case.</p>
<p>I'm still curious about whether there's a way to let <code>A</code>'s service account deploy to <code>B</code> without using a service account in <code>B</code> while running the <code>firebase</code> executable.</p>"
"Remove cloud build trigger from github checks<p>I had a cloud build trigger that would run tests on pull request.
Then I disabled the trigger and then deleted it. But it still shows up in Github PR. How can I remove it?</p>
<p><a href=""https://i.stack.imgur.com/vKXPl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vKXPl.png"" alt=""Github Pr screen shot"" /></a></p>","<p>After deleting the trigger from Cloud Build, I also had to remove it from Github repository settings:
<a href=""https://i.stack.imgur.com/piTRV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/piTRV.png"" alt=""enter image description here"" /></a></p>"
"How to ignore ""image not found"" error in cloudbuild.yaml when deleting images from the artifact registry?<p>Currently the cloudbuild.yaml looks like this:</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args: [ 'artifacts', 'docker', 'images', 'delete', 'location-docker.pkg.dev/$PROJECT_ID/repository/image' ]
- name: 'gcr.io/cloud-builders/docker'
  args: [ 'build', '-t', 'location-docker.pkg.dev/$PROJECT_ID/repository/image:latest', './' ]
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'location-docker.pkg.dev/$PROJECT_ID/reporitory/image:latest']
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: gcloud
  args: ['run', 'deploy', 'image', '--image', 'us-east1-docker.pkg.dev/$PROJECT_ID/cb-area52-ar/area52-image:latest', '--region', 'region']


images:
- 'location-docker.pkg.dev/$PROJECT_ID/registry/image:latest'
</code></pre>
<p>That does basically the following:</p>
<ol>
<li>delete the existsing image in the artifact registry</li>
<li>build the new image</li>
<li>pushes it back to the artifact registry</li>
<li>deploys it to google cloud run</li>
</ol>
<p>My problem is now that the first step fails whenever there is no image in the registry.</p>
<p>How can i prevent it from cancelling the whole build process when this occurs?</p>","<p>You can create an inline script to check whether the image exists or not. This assumes you always want to delete the image with the <code>latest</code> tag.</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args:
  - '-eEuo'
  - 'pipefail'
  - '-c'
  - |-
    if [[ -z `gcloud artifacts docker images describe location-docker.pkg.dev/$PROJECT_ID/repository/image:latest --verbosity=none --format=text` ]]
    then
      echo &quot;Image does not exist. Continue with the build&quot;
    else
      echo &quot;Deleting Image&quot;
      gcloud artifacts docker images delete location-docker.pkg.dev/$PROJECT_ID/repository/image
    fi
</code></pre>"
"Run php artisan commands from Cloud Build<p>I'm using Cloud Build to deploy my app on Cloud Run. I'd like to set <code>php artisan</code> commands in my cloudbuild.yaml to run migrations, init passport library... But I got this error on my Laravel init step:</p>
<pre><code>Starting Step #3 - &quot;Laravel Init&quot;
Step #3 - &quot;Laravel Init&quot;: Already have image (with digest): gcr.io/cloud-builders/gcloud
Step #3 - &quot;Laravel Init&quot;: bash: php: command not found
Step #3 - &quot;Laravel Init&quot;: bash: line 1: php: command not found
Step #3 - &quot;Laravel Init&quot;: bash: line 2: php: command not found
Step #3 - &quot;Laravel Init&quot;: bash: line 3: php: command not found
Step #3 - &quot;Laravel Init&quot;: bash: line 4: php: command not found
Finished Step #3 - &quot;Laravel Init&quot;
ERROR
ERROR: build step 3 &quot;gcr.io/cloud-builders/gcloud&quot; failed: step exited with non-zero status: 127
</code></pre>
<p>And here is my cloudbuild.yaml</p>
<pre><code>steps:
  # Build the container image
  - name: 'gcr.io/cloud-builders/docker'
    args:
    ...
    id: Build

  # Push the container image to Artifacts Registry
  - name: 'gcr.io/cloud-builders/docker'
    ...
    id: Push

  # Deploy container image to Cloud Run
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: Deploy
    entrypoint: gcloud
    ...

 # Laravel Init
  - name: 'gcr.io/cloud-builders/gcloud'
    id: Laravel Init
    entrypoint: &quot;bash&quot;
    args:
      - &quot;-c&quot;
      - |
          php artisan migrate --force
          php artisan db:seed --force
          php artisan db:seed --class=Database\\Seeders\\UsersTableSeeder --force
          php artisan passport:install

images:
  - 'europe-west3-docker.pkg.dev/$PROJECT_ID/.....'
tags:
  - latest
</code></pre>
<p>How can I do to execute my php artisan commands ?</p>","<p>I found a solution. I used the helper exec-wrapper. With this helper I can use my laravel container env and connect to Cloud SQL with the embedded cloud sql proxy. So I just have to pass my latest current image previously built in the first step in <code>cloudbuild.yaml</code>. I set the database socket connection and then I pass the <code>migration.sh</code> file where I can run all my <code>php artisan</code> commands.</p>
<p>I'm using mysql so you have to adjust port and connection name if you are using another Database.</p>
<p>cloudbuild.yaml:</p>
<pre><code>steps:
  # Build the container image
  - name: 'gcr.io/cloud-builders/docker'
    args:
    ...
    id: Build

  # Push the container image to Artifacts Registry
  - name: 'gcr.io/cloud-builders/docker'
    ...
    id: Push

  # Deploy container image to Cloud Run
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: Deploy
    entrypoint: gcloud
    ...

 # Laravel Init
  - name: 'gcr.io/google-appengine/exec-wrapper'
    id: Laravel Init
    args: [
      '-i', '&lt;YOUR_IMAGE_URL&gt;',
      '-e', 'DB_CONNECTION=mysql',
      '-e', 'DB_SOCKET=/cloudsql/&lt;YOUR_CLOUD_SQL_INSTANCE&gt;',
      '-e', 'DB_PORT=3306',
      '-e', 'DB_DATABASE=&lt;YOUR_DATABASE_NAME&gt;',
      '-e', 'DB_USERNAME=&lt;YOUR_DB_USER&gt;',
      '-e', 'DB_PASSWORD=&lt;YOUR_DB_PASS&gt;',
      '-s', '&lt;YOUR_CLOUD_SQL_INSTANCE&gt;',
      '--', '/app/scripts/migration.sh'
    ]
images:
  - 'europe-west3-docker.pkg.dev/$PROJECT_ID/.....'
</code></pre>
<p>Care about <code>/app</code> folder in <code>/app/scripts/migration.sh</code>. <code>/app</code> is my <code>WORKDIR</code> that I set in my <code>Dockerfile</code></p>
<p>migration.sh look like this:</p>
<pre><code>#!/bin/bash

php artisan migrate --force
php artisan db:seed --force
#... add more commands
</code></pre>
<p>Don't forget to Add the permission <code>Client Cloud SQL</code> to the Cloud Build service in the IAM section else Cloud Build cannot connect to your Cloud SQL instance.</p>
<p>And care about if your image has an entrypoint file. You have to use <code>exec $@</code> to execute the <code>--</code> command from app engine exec wrapper. If you don't use it the commands will be ignored.</p>"
"Streamlit with Poetry running in Docker locally but not on Cloud Run<p>I encountered that I was able to run the Dockerfile below on my local device without any issues, but on Cloud Run it fails with the following error in the logs:</p>
<p><a href=""https://i.stack.imgur.com/XP3y9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XP3y9.png"" alt=""enter image description here"" /></a></p>
<p>The Dockerfile looks like this:</p>
<pre><code>FROM debian:stable-slim

WORKDIR /root
RUN apt-get update &amp;&amp; apt-get upgrade -y &amp;&amp; apt-get install -y curl python3 python3-dev python3-venv build-essential \
  libgmp3-dev &amp;&amp; curl -sSL https://install.python-poetry.org | python3 -
ADD poetry.lock .
ADD pyproject.toml .
ADD main.py .
ENV PATH &quot;/root/.local/bin:$PATH&quot;
RUN poetry install -vvv
ENV STREAMLIT_SERVER_PORT 8080
RUN poetry run echo $PATH
RUN poetry run which python
RUN poetry run which streamlit
CMD [&quot;poetry&quot;, &quot;run&quot;, &quot;streamlit&quot;, &quot;run&quot;, &quot;main.py&quot;]
</code></pre>
<p>I was expecting Docker to ensure the same execution environment locally and on Cloud Run.</p>
<p>In addition the <code>echo $PATH / which python / which streamlit</code> commands return similar things locally and on Cloud Build.</p>
<p>Locally I get:</p>
<pre><code>Step 10/13 : RUN poetry run echo $PATH
 ---&gt; Running in 1e00651356b2
/root/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
Removing intermediate container 1e00651356b2
 ---&gt; bd9b9acd4152
Step 11/13 : RUN poetry run which python
 ---&gt; Running in 58b5348154a1
/root/.cache/pypoetry/virtualenvs/eso-website-lKa0R1gD-py3.9/bin/python
Removing intermediate container 58b5348154a1
 ---&gt; d3a8588626f0
Step 12/13 : RUN poetry run which streamlit
 ---&gt; Running in 72ea21408e0c
/root/.cache/pypoetry/virtualenvs/eso-website-lKa0R1gD-py3.9/bin/streamlit
Removing intermediate container 72ea21408e0c
 ---&gt; 3f7fa5ded8ec
Step 13/13 : CMD [&quot;poetry&quot;, &quot;run&quot;, &quot;streamlit&quot;, &quot;run&quot;, &quot;main.py&quot;]
 ---&gt; Running in ea06f67b0930
Removing intermediate container ea06f67b0930
 ---&gt; ae2349318933
Successfully built ae2349318933
Successfully tagged esoapp:latest

In Cloud Build I see:
Step #0: INFO[0026] RUN poetry run echo $PATH                    
Step #0: INFO[0026] Taking snapshot of full filesystem...        
Step #0: INFO[0034] cmd: /bin/sh                                 
Step #0: INFO[0034] args: [-c poetry run echo $PATH]             
Step #0: INFO[0034] Running: [/bin/sh -c poetry run echo $PATH]  
Step #0: /root/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
Step #0: INFO[0035] Taking snapshot of full filesystem...        
Step #0: INFO[0036] No files were changed, appending empty layer to config. No layer added to image. 
Step #0: INFO[0036] RUN poetry run which python                  
Step #0: INFO[0036] cmd: /bin/sh                                 
Step #0: INFO[0036] args: [-c poetry run which python]           
Step #0: INFO[0036] Running: [/bin/sh -c poetry run which python] 
Step #0: INFO[0036] Pushing layer gcr.io/personal-313102/eso-app/cache:88561d733a1fd5ca03e6481ff992723526198d61ba0abe93ab912eaa155da06a to cache now 
Step #0: INFO[0036] Pushing image to gcr.io/personal-313102/eso-app/cache:88561d733a1fd5ca03e6481ff992723526198d61ba0abe93ab912eaa155da06a 
Step #0: /root/.cache/pypoetry/virtualenvs/eso-website-lKa0R1gD-py3.9/bin/python
Step #0: INFO[0037] Taking snapshot of full filesystem...        
Step #0: INFO[0038] No files were changed, appending empty layer to config. No layer added to image. 
Step #0: INFO[0038] RUN poetry run which streamlit               
Step #0: INFO[0038] cmd: /bin/sh                                 
Step #0: INFO[0038] args: [-c poetry run which streamlit]        
Step #0: INFO[0038] Running: [/bin/sh -c poetry run which streamlit] 
Step #0: INFO[0038] Pushing layer gcr.io/personal-313102/eso-app/cache:498bfad933dca72234af0985b0ed384effed32958351e5727a4cbc56a2670d13 to cache now 
Step #0: INFO[0038] Pushing image to gcr.io/personal-313102/eso-app/cache:498bfad933dca72234af0985b0ed384effed32958351e5727a4cbc56a2670d13 
Step #0: INFO[0038] Pushed gcr.io/personal-313102/eso-app/cache@sha256:32f47b0699c392a7be4e8243545975348475956e35fd51347348168a07a714db 
Step #0: /root/.cache/pypoetry/virtualenvs/eso-website-lKa0R1gD-py3.9/bin/streamlit
Step #0: INFO[0038] Taking snapshot of full filesystem...        
Step #0: INFO[0040] No files were changed, appending empty layer to config. No layer added to image. 
Step #0: INFO[0040] CMD [&quot;poetry&quot;, &quot;run&quot;, &quot;streamlit&quot;, &quot;run&quot;, &quot;main.py&quot;] 
Step #0: INFO[0040] No files changed in this command, skipping snapshotting. 
Step #0: INFO[0040] Pushing layer gcr.io/personal-313102/eso-app/cache:7a16d1344c60a8740190534cfa44fed333547d60231720fcea3820ebd42ecf87 to cache now 
Step #0: INFO[0040] Pushing image to gcr.io/personal-313102/eso-app/cache:7a16d1344c60a8740190534cfa44fed333547d60231720fcea3820ebd42ecf87 
Step #0: INFO[0040] Pushed gcr.io/personal-313102/eso-app/cache@sha256:c4e57c77bd9dccfbbe3aca001bdf3fc26bccca20ed2879fc143a91ccdbee895a 
Step #0: INFO[0041] Pushed gcr.io/personal-313102/eso-app/cache@sha256:4198599d679af55fda0cf5cc8407bac0eb0ddb110e1e503a06663df9947523a6 
Step #0: INFO[0041] Pushing image to gcr.io/personal-313102/eso-app:0a1bd22ce692a52f231221778abea5561ac736e7 
Step #0: INFO[0043] Pushed gcr.io/personal-313102/eso-app@sha256:72473a2099329f7a4fb2a5933c7d3c749906840b27967752d06c24ca3f71acc5 
Finished Step #0
Starting Step #1
</code></pre>
<p>Any ideas on why the streamlit binary cannot be found in Cloud Run?</p>","<p>Digging further into this myself, I discovered that the home directory was confused in Cloud Run.</p>
<p>This is apparently a known issue -&gt; <a href=""https://cloud.google.com/run/docs/issues#home"" rel=""nofollow noreferrer"">https://cloud.google.com/run/docs/issues#home</a></p>
<p>It was solved by changing the CMD command in the Dockerfile to
<code>CMD HOME=/root poetry run streamlit run main.py</code></p>"
"How to remove an image from Artifact Registry automatically<p>Using gcloud I can list and remove the images I want through those commands:</p>
<pre><code>gcloud artifacts docker images list LOCATION/PROJECT-ID/RESPOSITORY-ID/IMAGE \
  --include-tags --filter=&quot;tags:IPLA*&quot; --filter=&quot;create_time&gt;2022-04-20T00:00:00&quot;
</code></pre>
<p>and then</p>
<pre><code>gcloud artifacts docker images delete LOCATION/PROJECT-ID/RESPOSITORY-ID/IMAGE:tag
</code></pre>
<p>I am trying to automate that so I can filter by tag name and date and run every day or week.</p>
<p>I've tried to use inside a cloud function, but I don't think that is allowed.</p>
<pre><code>  const { spawn } = require(&quot;child_process&quot;);
  const listening = spawn('gcloud', ['artifacts', 'docker', 'images', 'list', 
     'LOCATION/PROJECT-ID/RESPOSITORY-ID/IMAGE',
     '--include-tags', 
     '--filter=&quot;tags:IPLA*&quot;', 
     '--filter=&quot;create_time&gt;2022-04-20T00:00:00&quot;'
  ]);

  listening.stdout.on(&quot;data&quot;, data =&gt; {
      console.log(`stdout: ${data}`);
  });

  listening.stderr.on(&quot;data&quot;, data =&gt; {
      console.log(`stderr: ${data}`);
  });

  listening.on('error', (error) =&gt; {
      console.log(`error: ${error.message}`);
  });
</code></pre>
<p>I get this error when running the cloud function:</p>
<pre><code>error: spawn gcloud ENOENT
</code></pre>
<p>I accept any other solution like trigger on cloud build, terraform as long is it can live on google cloud.</p>","<p>You use Cloud Functions, a serverless product where you deploy your code that run somewhere, on something that you don't manage.</p>
<p>Here, in your code, you assume that gcloud is installed in the runtime. It's a mistake, you can't perform that assumption (that is wrong!)</p>
<hr />
<p>However, you can use another serverless product where you manage your runtime environemnt: Cloud Run. The principle is to create your container (and therefore install what you want in it) and then deploy it. That time you can use gcloud command, because you know it exists on the VM.</p>
<hr />
<p>However, it's not the right option. You have 2 better things</p>
<ul>
<li>First of all, use something already done for you by a Google Cloud Developer Advocate (Seth Vargo). It's named <a href=""https://github.com/GoogleCloudPlatform/gcr-cleaner"" rel=""nofollow noreferrer"">GCR cleaner</a> and remove images older than something</li>
<li>Or you can use directly the API to perform the exact same operation than GCLOUD bur without gcloud, by invoking the <a href=""https://cloud.google.com/artifact-registry/docs/reference/rest"" rel=""nofollow noreferrer"">Artifact registry REST API</a>. If you want to cheat and go faster, you can use the gcloud command with the <code>--log-http</code> parameter to display all the API call performed by the CLI. Copy the URL and parameters, and enjoy!!</li>
</ul>"
"How to skip irrelevant steps on multi-stage docker build<p>I am attempting to run a multi-step build process with two parallel steps (<code>test</code> and <code>release</code>) and running into issues where Google Cloud Build does not skip the irrelevant intermediary steps when running a targeted multi-stage.</p>
<p>Dockerfile</p>
<pre><code>FROM node:18-slim AS base

WORKDIR /app

COPY package.json yarn.lock /app/
ADD src/ /app/src

RUN yarn install

FROM base AS test
RUN yarn test

FROM base AS release
EXPOSE 3000

ENTRYPOINT node src/index.js
</code></pre>
<p>cloudbuild.json</p>
<pre><code>{
    &quot;steps&quot;: [
        {
            &quot;name&quot;: &quot;gcr.io/cloud-builders/docker&quot;,
            &quot;id&quot;: &quot;Install&quot;,
            &quot;args&quot;: [
                &quot;build&quot;,
                &quot;--target&quot;,
                &quot;base&quot;,
                &quot;-t&quot;,
                &quot;australia-southeast1-docker.pkg.dev/${PROJECT_ID}/cloud-build-tutorial/${REPO_NAME}-install:${SHORT_SHA}&quot;,
                &quot;.&quot;
            ]
        },
        {
            &quot;name&quot;: &quot;gcr.io/cloud-builders/docker&quot;,
            &quot;id&quot;: &quot;Test&quot;,
            &quot;args&quot;: [
                &quot;build&quot;,
                &quot;--target&quot;,
                &quot;test&quot;,
                &quot;-t&quot;,
                &quot;australia-southeast1-docker.pkg.dev/${PROJECT_ID}/cloud-build-tutorial/${REPO_NAME}-test:${SHORT_SHA}&quot;,
                &quot;.&quot;
            ],
            &quot;waitFor&quot;: [&quot;Install&quot;]
        },
        {
            &quot;name&quot;: &quot;gcr.io/cloud-builders/docker&quot;,
            &quot;id&quot;: &quot;Build&quot;,
            &quot;args&quot;: [
                &quot;build&quot;,
                &quot;--target&quot;,
                &quot;release&quot;,
                &quot;-t&quot;,
                &quot;australia-southeast1-docker.pkg.dev/${PROJECT_ID}/cloud-build-tutorial/${REPO_NAME}:${SHORT_SHA}&quot;,
                &quot;.&quot;
            ],
            &quot;waitFor&quot;: [&quot;Install&quot;]
        }
    ]
}
</code></pre>
<p>Step Outcomes:</p>
<ul>
<li><strong>Install</strong>: Runs only the <code>base</code> step from Dockerfile.</li>
<li><strong>Test</strong>: Runs only the <code>test</code> step from Dockerfile after utilizing cache from <strong>Install</strong> step.</li>
<li><strong>Build</strong>: Runs both the <code>test</code> and <code>release</code> steps from Dockerfile after utilizing cache from <strong>Install</strong> step.</li>
</ul>
<p>My intended outcome is:</p>
<ul>
<li><strong>Install</strong>: Runs only the <code>base</code> step from Dockerfile.</li>
<li><strong>Test</strong>: Runs only the <code>test</code> step from Dockerfile after utilizing cache from <strong>Install</strong> step.</li>
<li><strong>Build</strong>: Runs only the<code>release</code> step from Dockerfile after utilizing cache from <strong>Install</strong> step.</li>
</ul>
<p>Have I misconfigured the configurations in any way or does Google Cloud Build not support skipping steps as part of a targetted multi-stage build?</p>
<p><em><strong>EDIT:</strong></em>
The following cloudbuild config works, thanks to the provided <strong>Answer</strong>:</p>
<pre><code>{
    &quot;steps&quot;: [
        {
            &quot;name&quot;: &quot;gcr.io/cloud-builders/docker&quot;,
            &quot;id&quot;: &quot;Install&quot;,
            &quot;args&quot;: [
                &quot;build&quot;,
                &quot;--target&quot;,
                &quot;base&quot;,
                &quot;-t&quot;,
                &quot;australia-southeast1-docker.pkg.dev/${PROJECT_ID}/cloud-build-tutorial/${REPO_NAME}-install:${SHORT_SHA}&quot;,
                &quot;.&quot;
            ],
            &quot;env&quot;: [&quot;DOCKER_BUILDKIT=1&quot;]
        },
        {
            &quot;name&quot;: &quot;gcr.io/cloud-builders/docker&quot;,
            &quot;id&quot;: &quot;Test&quot;,
            &quot;args&quot;: [
                &quot;build&quot;,
                &quot;--target&quot;,
                &quot;test&quot;,
                &quot;-t&quot;,
                &quot;australia-southeast1-docker.pkg.dev/${PROJECT_ID}/cloud-build-tutorial/${REPO_NAME}-test:${SHORT_SHA}&quot;,
                &quot;.&quot;
            ],
            &quot;env&quot;: [&quot;DOCKER_BUILDKIT=1&quot;],
            &quot;waitFor&quot;: [&quot;Install&quot;]
        },
        {
            &quot;name&quot;: &quot;gcr.io/cloud-builders/docker&quot;,
            &quot;id&quot;: &quot;Build&quot;,
            &quot;args&quot;: [
                &quot;build&quot;,
                &quot;--target&quot;,
                &quot;release&quot;,
                &quot;-t&quot;,
                &quot;australia-southeast1-docker.pkg.dev/${PROJECT_ID}/cloud-build-tutorial/${REPO_NAME}:${SHORT_SHA}&quot;,
                &quot;.&quot;
            ],
            &quot;env&quot;: [&quot;DOCKER_BUILDKIT=1&quot;],
            &quot;waitFor&quot;: [&quot;Install&quot;]
        }
    ]
}
</code></pre>","<p>Building the dependency graph of stages is a feature of buildkit. By default, google's cloud builder image is still using the classic docker builder, but you may be able to switch to buildkit with:</p>
<pre class=""lang-yaml prettyprint-override""><code>  env:
    - &quot;DOCKER_BUILDKIT=1&quot;
</code></pre>
<p>within your build steps. However, at last check, <a href=""https://issuetracker.google.com/issues/119753486"" rel=""nofollow noreferrer"">upgrading the docker engine is still pending</a>.</p>"
"How to get source branch from google cloud build trigger on push when merging<p>I am merging 'feature-branch' onto 'dev-branch' from a github pull request. I am using a google cloud build trigger that is triggered on push to 'dev-branch'. From what I can tell in the <a href=""https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values"" rel=""nofollow noreferrer"">documentation of substitution variables</a>, there are no substitution variables to get the name of the branch that I am merging from - 'feature-branch' and only the branch that I merging to - 'dev-branch'. Is there a way or a workaround to get information (name, sha, id, etc.) of the branch that is being merged from on a google cloud build trigger from a push to branch event?</p>","<p>Presumably you can have some naming conventions on the first line of the commit message (happens at <code>merge pull request</code> &quot;event&quot;), so that this line includes the source (or <code>head</code>) branch name (the source of the merge - in your words - 'feature-branch').</p>
<p>Then, you can create a substitution variable:</p>
<pre><code>substitutions:
  _COMMIT_MESSAGE: $(commit.commit.message)
</code></pre>
<p>here is a documentation link: <a href=""https://cloud.google.com/build/docs/configuring-builds/use-bash-and-bindings-in-substitutions#creating_substitutions_using_payload_bindings"" rel=""nofollow noreferrer"">Creating substitutions using payload bindings</a></p>
<p>And use that variable in some build step to get the the <code>head</code> branch name:</p>
<pre><code>    mapfile -t commit_lines &lt;&lt;&lt; &quot;${_COMMIT_MESSAGE}&quot;
    source_branch=&quot;$(echo ${commit_lines[0]} | &lt;&lt;add your command here following naming convention for the commit message&gt;&gt; )&quot;
    echo &quot;=&gt; My source branch name: ${source_branch}&quot;
</code></pre>
<p>After that you can use the source branch name.</p>"
"MySQL Not Binding to Correct Port When Running docker-compose in CloudBuild<p>I am working on a CloudBuild script that needs to instantiate a local DB for testing of a backend service. I am using <code>docker/compose:1.25.3</code> for this purpose. The script works locally but when running on CloudBuild the service is unable to connect to the database. The relevant scripts can be observed below:</p>

<p><strong>docker-compose.test.yml</strong></p>

<pre><code>version: '3.4'

services:
  app:
    image: gcr.io/${PROJECT_ID}/&lt;REDACTED&gt;/test:${SHORT_SHA}
    command: [""bundle"", ""exec"", ""rails"", ""t""]
    ports:
      - 3000:3000
    depends_on:
      - db      
  db:
    image: mysql:5.7.24
    environment:
      - MYSQL_ALLOW_EMPTY_PASSWORD=true
      - MYSQL_DATABASE=sales_test
      - MYSQL_HOST=0.0.0.0
      - MYSQL_TCP_PORT=3306

networks:
  default:
      external:
          name: cloudbuild
</code></pre>

<p><strong>cloudbuild.yaml</strong></p>

<pre><code>steps:
  # Run Tests
  - name: 'docker/compose:1.25.3'
    id: test
    args: ['-f', 'docker-compose.test.yml', 'up', '--exit-code-from', 'app']
    env:
      - 'PROJECT_ID=$PROJECT_ID'
      - 'SHORT_SHA=$SHORT_SHA'
</code></pre>

<p>The error log (with minor redactions) can be seen below:</p>

<pre><code>Pulling image: docker/compose:1.25.3
1.25.3: Pulling from docker/compose
89d9c30c1d48: Already exists
8c7adae20efb: Pulling fs layer
c8bcdc0a450b: Pulling fs layer
cee865d72b79: Pulling fs layer
8c7adae20efb: Verifying Checksum
8c7adae20efb: Download complete
8c7adae20efb: Pull complete
cee865d72b79: Verifying Checksum
cee865d72b79: Download complete
c8bcdc0a450b: Verifying Checksum
c8bcdc0a450b: Download complete
c8bcdc0a450b: Pull complete
cee865d72b79: Pull complete
Digest: sha256:3ab5ae464a8a428085729d6a0f2ab7a5ccc5524f6ca8649855c7d21fb86148cd
Status: Downloaded newer image for docker/compose:1.25.3
docker.io/docker/compose:1.25.3
using --exit-code-from implies --abort-on-container-exit
Pulling db (mysql:5.7.24)...
5.7.24: Pulling from library/mysql
Digest: sha256:bf17a7109057494c45fba5aab7fc805ca00ac1eef638dfdd42b38d5a7190c9bb
Status: Downloaded newer image for mysql:5.7.24
Pulling app (gcr.io/&lt;REDACTED&gt;)...
e98c184: Pulling from &lt;REDACTED&gt;
Digest: sha256:55f6679af82d7ca5ccac5a2d2aefe0b1c75e4d65b49cd6c6a768c074dfafc483
Status: Downloaded newer image for gcr.io/&lt;REDACTED&gt;
Creating workspace_db_1 ... 

Attaching to workspace_db_1, workspace_app_1
db_1   | Initializing database
db_1   | 2020-02-09T16:53:45.954586Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
db_1   | 2020-02-09T16:53:47.683244Z 0 [Warning] InnoDB: New log files created, LSN=45790
db_1   | 2020-02-09T16:53:47.892451Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.
db_1   | 2020-02-09T16:53:47.953412Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: bdad19cb-4b5c-11ea-a027-0242c0a80a03.
db_1   | 2020-02-09T16:53:47.957113Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened.
db_1   | 2020-02-09T16:53:47.958298Z 1 [Warning] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
db_1   | 2020-02-09T16:53:48.693564Z 1 [Warning] 'user' entry 'root@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:48.694594Z 1 [Warning] 'user' entry 'mysql.session@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:48.695129Z 1 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:48.695592Z 1 [Warning] 'db' entry 'performance_schema mysql.session@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:48.695985Z 1 [Warning] 'db' entry 'sys mysql.sys@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:48.696090Z 1 [Warning] 'proxies_priv' entry '@ root@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:48.696300Z 1 [Warning] 'tables_priv' entry 'user mysql.session@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:48.696425Z 1 [Warning] 'tables_priv' entry 'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.
db_1   | Database initialized
db_1   | Initializing certificates
db_1   | Generating a RSA private key
db_1   | .............................................+++++
db_1   | .+++++
db_1   | unable to write 'random state'
db_1   | writing new private key to 'ca-key.pem'
db_1   | -----
db_1   | Generating a RSA private key
db_1   | .........................+++++
db_1   | ................................+++++
db_1   | unable to write 'random state'
db_1   | writing new private key to 'server-key.pem'
db_1   | -----
db_1   | Generating a RSA private key
db_1   | ........................................................+++++
db_1   | .......................................................................................................+++++
db_1   | unable to write 'random state'
db_1   | writing new private key to 'client-key.pem'
db_1   | -----
db_1   | Certificates initialized
db_1   | MySQL init process in progress...
db_1   | 2020-02-09T16:53:52.331939Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
db_1   | 2020-02-09T16:53:52.338264Z 0 [Note] mysqld (mysqld 5.7.24) starting as process 88 ...
db_1   | 2020-02-09T16:53:52.342658Z 0 [Note] InnoDB: PUNCH HOLE support available
db_1   | 2020-02-09T16:53:52.345876Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
db_1   | 2020-02-09T16:53:52.346533Z 0 [Note] InnoDB: Uses event mutexes
db_1   | 2020-02-09T16:53:52.347011Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
db_1   | 2020-02-09T16:53:52.347439Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11
db_1   | 2020-02-09T16:53:52.347864Z 0 [Note] InnoDB: Using Linux native AIO
db_1   | 2020-02-09T16:53:52.348711Z 0 [Note] InnoDB: Number of pools: 1
db_1   | 2020-02-09T16:53:52.349446Z 0 [Note] InnoDB: Using CPU crc32 instructions
db_1   | 2020-02-09T16:53:52.352053Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
db_1   | 2020-02-09T16:53:52.374298Z 0 [Note] InnoDB: Completed initialization of buffer pool
db_1   | 2020-02-09T16:53:52.377886Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
db_1   | 2020-02-09T16:53:52.389720Z 0 [Note] InnoDB: Highest supported file format is Barracuda.
db_1   | 2020-02-09T16:53:52.406894Z 0 [Note] InnoDB: Creating shared tablespace for temporary tables
db_1   | 2020-02-09T16:53:52.409937Z 0 [Note] InnoDB: Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
db_1   | 2020-02-09T16:53:52.616442Z 0 [Note] InnoDB: File './ibtmp1' size is now 12 MB.
db_1   | 2020-02-09T16:53:52.617610Z 0 [Note] InnoDB: 96 redo rollback segment(s) found. 96 redo rollback segment(s) are active.
db_1   | 2020-02-09T16:53:52.618823Z 0 [Note] InnoDB: 32 non-redo rollback segment(s) are active.
db_1   | 2020-02-09T16:53:52.619566Z 0 [Note] InnoDB: Waiting for purge to start
db_1   | 2020-02-09T16:53:52.670260Z 0 [Note] InnoDB: 5.7.24 started; log sequence number 2591440
db_1   | 2020-02-09T16:53:52.671455Z 0 [Note] Plugin 'FEDERATED' is disabled.
db_1   | 2020-02-09T16:53:52.682070Z 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
db_1   | 2020-02-09T16:53:52.684590Z 0 [Note] InnoDB: Buffer pool(s) load completed at 200209 16:53:52
db_1   | 2020-02-09T16:53:52.690301Z 0 [Note] Found ca.pem, server-cert.pem and server-key.pem in data directory. Trying to enable SSL support using them.
db_1   | 2020-02-09T16:53:52.691413Z 0 [Warning] CA certificate ca.pem is self signed.
db_1   | 2020-02-09T16:53:52.696830Z 0 [Warning] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
db_1   | 2020-02-09T16:53:52.698713Z 0 [Warning] 'user' entry 'root@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:52.699389Z 0 [Warning] 'user' entry 'mysql.session@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:52.699887Z 0 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:52.700362Z 0 [Warning] 'db' entry 'performance_schema mysql.session@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:52.700817Z 0 [Warning] 'db' entry 'sys mysql.sys@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:52.701498Z 0 [Warning] 'proxies_priv' entry '@ root@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:52.703369Z 0 [Warning] 'tables_priv' entry 'user mysql.session@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:52.703956Z 0 [Warning] 'tables_priv' entry 'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:52.713772Z 0 [Note] Event Scheduler: Loaded 0 events
db_1   | 2020-02-09T16:53:52.721653Z 0 [Note] mysqld: ready for connections.
db_1   | Version: '5.7.24'  socket: '/var/run/mysqld/mysqld.sock'  port: 0  MySQL Community Server (GPL)
db_1   | Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
db_1   | Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it.
db_1   | Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
db_1   | Warning: Unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. Skipping it.
db_1   | 2020-02-09T16:53:56.959302Z 4 [Warning] 'user' entry 'root@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:56.960220Z 4 [Warning] 'user' entry 'mysql.session@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:56.960744Z 4 [Warning] 'user' entry 'mysql.sys@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:56.961208Z 4 [Warning] 'db' entry 'performance_schema mysql.session@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:56.961636Z 4 [Warning] 'db' entry 'sys mysql.sys@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:56.962216Z 4 [Warning] 'proxies_priv' entry '@ root@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:56.962711Z 4 [Warning] 'tables_priv' entry 'user mysql.session@localhost' ignored in --skip-name-resolve mode.
db_1   | 2020-02-09T16:53:56.963322Z 4 [Warning] 'tables_priv' entry 'sys_config mysql.sys@localhost' ignored in --skip-name-resolve mode.
db_1   | 
db_1   | 2020-02-09T16:53:56.981051Z 0 [Note] Giving 0 client threads a chance to die gracefully
db_1   | 2020-02-09T16:53:56.981952Z 0 [Note] Shutting down slave threads
db_1   | 2020-02-09T16:53:56.982615Z 0 [Note] Forcefully disconnecting 0 remaining clients
db_1   | 2020-02-09T16:53:56.983096Z 0 [Note] Event Scheduler: Purging the queue. 0 events
db_1   | 2020-02-09T16:53:56.983631Z 0 [Note] Binlog end
db_1   | 2020-02-09T16:53:56.984775Z 0 [Note] Shutting down plugin 'ngram'
db_1   | 2020-02-09T16:53:56.985319Z 0 [Note] Shutting down plugin 'partition'
db_1   | 2020-02-09T16:53:56.985783Z 0 [Note] Shutting down plugin 'BLACKHOLE'
db_1   | 2020-02-09T16:53:56.986270Z 0 [Note] Shutting down plugin 'ARCHIVE'
db_1   | 2020-02-09T16:53:56.986693Z 0 [Note] Shutting down plugin 'PERFORMANCE_SCHEMA'
db_1   | 2020-02-09T16:53:56.987126Z 0 [Note] Shutting down plugin 'MRG_MYISAM'
db_1   | 2020-02-09T16:53:56.987689Z 0 [Note] Shutting down plugin 'MyISAM'
db_1   | 2020-02-09T16:53:56.989856Z 0 [Note] Shutting down plugin 'INNODB_SYS_VIRTUAL'
db_1   | 2020-02-09T16:53:56.990394Z 0 [Note] Shutting down plugin 'INNODB_SYS_DATAFILES'
db_1   | 2020-02-09T16:53:56.990818Z 0 [Note] Shutting down plugin 'INNODB_SYS_TABLESPACES'
db_1   | 2020-02-09T16:53:56.991482Z 0 [Note] Shutting down plugin 'INNODB_SYS_FOREIGN_COLS'
db_1   | 2020-02-09T16:53:56.991930Z 0 [Note] Shutting down plugin 'INNODB_SYS_FOREIGN'
db_1   | 2020-02-09T16:53:56.992046Z 0 [Note] Shutting down plugin 'INNODB_SYS_FIELDS'
db_1   | 2020-02-09T16:53:56.992159Z 0 [Note] Shutting down plugin 'INNODB_SYS_COLUMNS'
db_1   | 2020-02-09T16:53:56.992229Z 0 [Note] Shutting down plugin 'INNODB_SYS_INDEXES'
db_1   | 2020-02-09T16:53:56.992287Z 0 [Note] Shutting down plugin 'INNODB_SYS_TABLESTATS'
db_1   | 2020-02-09T16:53:56.992387Z 0 [Note] Shutting down plugin 'INNODB_SYS_TABLES'
db_1   | 2020-02-09T16:53:56.992455Z 0 [Note] Shutting down plugin 'INNODB_FT_INDEX_TABLE'
db_1   | 2020-02-09T16:53:56.992513Z 0 [Note] Shutting down plugin 'INNODB_FT_INDEX_CACHE'
db_1   | 2020-02-09T16:53:56.992577Z 0 [Note] Shutting down plugin 'INNODB_FT_CONFIG'
db_1   | 2020-02-09T16:53:56.992685Z 0 [Note] Shutting down plugin 'INNODB_FT_BEING_DELETED'
db_1   | 2020-02-09T16:53:56.992755Z 0 [Note] Shutting down plugin 'INNODB_FT_DELETED'
db_1   | 2020-02-09T16:53:56.992811Z 0 [Note] Shutting down plugin 'INNODB_FT_DEFAULT_STOPWORD'
db_1   | 2020-02-09T16:53:56.992874Z 0 [Note] Shutting down plugin 'INNODB_METRICS'
db_1   | 2020-02-09T16:53:56.992939Z 0 [Note] Shutting down plugin 'INNODB_TEMP_TABLE_INFO'
db_1   | 2020-02-09T16:53:56.993063Z 0 [Note] Shutting down plugin 'INNODB_BUFFER_POOL_STATS'
db_1   | 2020-02-09T16:53:56.993134Z 0 [Note] Shutting down plugin 'INNODB_BUFFER_PAGE_LRU'
db_1   | 2020-02-09T16:53:56.993204Z 0 [Note] Shutting down plugin 'INNODB_BUFFER_PAGE'
db_1   | 2020-02-09T16:53:56.993274Z 0 [Note] Shutting down plugin 'INNODB_CMP_PER_INDEX_RESET'
db_1   | 2020-02-09T16:53:56.993340Z 0 [Note] Shutting down plugin 'INNODB_CMP_PER_INDEX'
db_1   | 2020-02-09T16:53:56.993408Z 0 [Note] Shutting down plugin 'INNODB_CMPMEM_RESET'
db_1   | 2020-02-09T16:53:56.993465Z 0 [Note] Shutting down plugin 'INNODB_CMPMEM'
db_1   | 2020-02-09T16:53:56.993534Z 0 [Note] Shutting down plugin 'INNODB_CMP_RESET'
db_1   | 2020-02-09T16:53:56.993644Z 0 [Note] Shutting down plugin 'INNODB_CMP'
db_1   | 2020-02-09T16:53:56.993712Z 0 [Note] Shutting down plugin 'INNODB_LOCK_WAITS'
db_1   | 2020-02-09T16:53:56.993768Z 0 [Note] Shutting down plugin 'INNODB_LOCKS'
db_1   | 2020-02-09T16:53:56.993902Z 0 [Note] Shutting down plugin 'INNODB_TRX'
db_1   | 2020-02-09T16:53:56.993974Z 0 [Note] Shutting down plugin 'InnoDB'
db_1   | 2020-02-09T16:53:56.997425Z 0 [Note] InnoDB: FTS optimize thread exiting.
db_1   | 2020-02-09T16:53:56.997740Z 0 [Note] InnoDB: Starting shutdown...
app_1  | DEPRECATION WARNING: ActiveSupport.halt_callback_chains_on_return_false= is deprecated and will be removed in Rails 5.2. (called from &lt;top (required)&gt; at /app/config/initializers/new_framework_defaults.rb:25)
app_1  | /app/vendor/bundle/ruby/2.5.0/gems/mysql2-0.4.10/lib/mysql2/client.rb:89:in `connect': Can't connect to MySQL server on 'db' (115) (Mysql2::Error)
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/mysql2-0.4.10/lib/mysql2/client.rb:89:in `initialize'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_adapters/mysql2_adapter.rb:21:in `new'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_adapters/mysql2_adapter.rb:21:in `mysql2_connection'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_adapters/abstract/connection_pool.rb:759:in `new_connection'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_adapters/abstract/connection_pool.rb:803:in `checkout_new_connection'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_adapters/abstract/connection_pool.rb:782:in `try_to_checkout_new_connection'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_adapters/abstract/connection_pool.rb:743:in `acquire_connection'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_adapters/abstract/connection_pool.rb:500:in `checkout'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_adapters/abstract/connection_pool.rb:374:in `connection'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_adapters/abstract/connection_pool.rb:931:in `retrieve_connection'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_handling.rb:116:in `retrieve_connection'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/connection_handling.rb:88:in `connection'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activerecord-5.1.2/lib/active_record/model_schema.rb:331:in `table_exists?'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/bundler/gems/spree-5ff46f2f5990/core/app/models/spree/preferences/store.rb:89:in `should_persist?'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/bundler/gems/spree-5ff46f2f5990/core/app/models/spree/preferences/store.rb:74:in `persist'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/bundler/gems/spree-5ff46f2f5990/core/app/models/spree/preferences/store.rb:21:in `set'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/bundler/gems/spree-5ff46f2f5990/core/app/models/spree/preferences/scoped_store.rb:17:in `[]='
app_1  |    from /app/vendor/bundle/ruby/2.5.0/bundler/gems/spree-5ff46f2f5990/core/app/models/spree/preferences/preferable_class_methods.rb:20:in `block in preference'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/bundler/gems/spree-5ff46f2f5990/core/app/models/spree/preferences/preferable.rb:48:in `set_preference'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/bundler/gems/spree-5ff46f2f5990/core/app/models/spree/preferences/configuration.rb:61:in `method_missing'
app_1  |    from /app/config/initializers/spree.rb:13:in `block in &lt;top (required)&gt;'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/bundler/gems/spree-5ff46f2f5990/core/lib/spree/core.rb:59:in `config'
app_1  |    from /app/config/initializers/spree.rb:12:in `&lt;top (required)&gt;'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activesupport-5.1.2/lib/active_support/dependencies.rb:286:in `load'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activesupport-5.1.2/lib/active_support/dependencies.rb:286:in `block in load'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activesupport-5.1.2/lib/active_support/dependencies.rb:258:in `load_dependency'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activesupport-5.1.2/lib/active_support/dependencies.rb:286:in `load'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/engine.rb:655:in `block in load_config_initializer'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/activesupport-5.1.2/lib/active_support/notifications.rb:168:in `instrument'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/engine.rb:654:in `load_config_initializer'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/engine.rb:612:in `block (2 levels) in &lt;class:Engine&gt;'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/engine.rb:611:in `each'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/engine.rb:611:in `block in &lt;class:Engine&gt;'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/initializable.rb:30:in `instance_exec'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/initializable.rb:30:in `run'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/initializable.rb:59:in `block in run_initializers'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:228:in `block in tsort_each'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:350:in `block (2 levels) in each_strongly_connected_component'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:422:in `block (2 levels) in each_strongly_connected_component_from'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:431:in `each_strongly_connected_component_from'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:421:in `block in each_strongly_connected_component_from'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/initializable.rb:48:in `each'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/initializable.rb:48:in `tsort_each_child'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:415:in `call'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:415:in `each_strongly_connected_component_from'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:349:in `block in each_strongly_connected_component'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:347:in `each'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:347:in `call'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:347:in `each_strongly_connected_component'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:226:in `tsort_each'
app_1  |    from /usr/local/lib/ruby/2.5.0/tsort.rb:205:in `tsort_each'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/initializable.rb:58:in `run_initializers'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/application.rb:353:in `initialize!'
app_1  |    from /app/config/environment.rb:5:in `&lt;top (required)&gt;'
app_1  |    from /app/test/test_helper.rb:4:in `require'
app_1  |    from /app/test/test_helper.rb:4:in `&lt;top (required)&gt;'
app_1  |    from /app/test/controllers/admin/login_controller_test.rb:1:in `require'
app_1  |    from /app/test/controllers/admin/login_controller_test.rb:1:in `&lt;top (required)&gt;'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/test_unit/test_requirer.rb:14:in `require'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/test_unit/test_requirer.rb:14:in `block in require_files'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/test_unit/test_requirer.rb:13:in `each'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/test_unit/test_requirer.rb:13:in `require_files'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/test_unit/minitest_plugin.rb:94:in `plugin_rails_init'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/minitest-5.11.3/lib/minitest.rb:81:in `block in init_plugins'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/minitest-5.11.3/lib/minitest.rb:79:in `each'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/minitest-5.11.3/lib/minitest.rb:79:in `init_plugins'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/minitest-5.11.3/lib/minitest.rb:130:in `run'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/railties-5.1.2/lib/rails/test_unit/minitest_plugin.rb:77:in `run'
app_1  |    from /app/vendor/bundle/ruby/2.5.0/gems/minitest-5.11.3/lib/minitest.rb:63:in `block in autorun'
db_1   | 2020-02-09T16:53:57.100166Z 0 [Note] InnoDB: Dumping buffer pool(s) to /var/lib/mysql/ib_buffer_pool
db_1   | 2020-02-09T16:53:57.100642Z 0 [Note] InnoDB: Buffer pool(s) dump completed at 200209 16:53:57
app_1  | Coverage report generated for MiniTest to /app/coverage. 278 / 716 LOC (38.83%) covered.
workspace_app_1 exited with code 1

Aborting on container exit...

</code></pre>

<p>The log line that reads <code>Version: '5.7.24'  socket: '/var/run/mysqld/mysqld.sock'  port: 0  MySQL Community Server (GPL)</code> has called my attention since the port is set to 0 despite me setting it to 3306 (and 3306 being the default). When running locally, the port is correctly set to 3306. Am I misunderstanding how the networking works inside CloudBuild? Is using docker-compose the recommended method to accomplish this? </p>","<p>The issue was related to a race condition due to the docker-compose <code>depends_on</code> attribute waiting only for a container (in this case <code>db</code>) to be ready but having no guarantee on the actual code in the container (in this case a MySQL instance) running. </p>

<p>The solution was to add the script <code>wait-for-mysql.sh</code> shown below:</p>

<pre class=""lang-sh prettyprint-override""><code>#!/bin/sh
# wait-for-mysql.sh

set -e

host=""$1""
shift
cmd=""$@""

until mysql -h ""db"" -u ""root"" --connect_timeout 1 -e ""\q""; do
  &gt;&amp;2 echo ""MySQL is unavailable - sleeping for 1 second""
  sleep 1
done

&gt;&amp;2 echo ""MySQL is up - executing command""
exec $cmd
</code></pre>

<p>Then I edited the <code>docker-compose.test.yml</code> as shown below:</p>

<pre class=""lang-yaml prettyprint-override""><code>version: '3.4'

services:
  app:
    image: gcr.io/${PROJECT_ID}/&lt;REDACTED&gt;/test:${SHORT_SHA}
    command: [""./scripts/wait-for-mysql.sh"", ""&amp;&amp;"", ""bundle"", ""exec"", ""rails"", ""t""]
    ports:
      - 3000:3000
    depends_on:
      - db
    environment:
      - TEST_DATABASE_URL=mysql2://root@db/sales_test
  db:
    image: mysql:5.7.24
    ports:
      - 3306:3306
    environment:
      - MYSQL_ALLOW_EMPTY_PASSWORD=true
      - MYSQL_DATABASE=&lt;REDACTED&gt;

networks:
  default:
      external:
          name: cloudbuild
</code></pre>"
"Using PR Build as a Subdomain to Google Cloud Triggered Builds<p>We are using Google Cloud triggered builds (<a href=""https://cloud.google.com/appengine/docs/standard/python/how-requests-are-routed#default_routing"" rel=""nofollow noreferrer"">refer documentation</a>) and are successfully able to see results like: </p>

<pre><code>https://VERSION_ID-dot-PROJECT_ID.appspot.com
</code></pre>

<p>We use API keys for Maps etc. and would like to restrict access to websites. For this, there's wild card allowed in API Credentials page (<a href=""https://cloud.google.com/docs/authentication/api-keys#api_key_restrictions"" rel=""nofollow noreferrer"">refer documentation</a>) for ex: </p>

<pre><code>https://*.example.com
</code></pre>

<p>however, it doesn't allow:</p>

<pre><code>https://*-some-random-string.example.com
</code></pre>

<p>We would like to overcome this issue so we can restrict the keys to our PR builds only, how do we do this?</p>

<p>One option would be to have PR builds like:</p>

<pre><code>https://VERSION_ID.PROJECT_ID.appspot.com
</code></pre>

<p>so we could use <code>https://*.PROJECT_ID.appspot.com</code> in API Credential restrictions, but I can't figure how to create PR builds as sub domains.</p>

<p>Any help would be much appreciated!</p>","<p>Answering my own question:</p>

<p>GCP does indeed allow patterns in URLs for Credentials:</p>

<pre><code>*-some-random-string.example.com/*
</code></pre>

<p>The reason it wasn't working for us was something else, and not this capability.</p>"
"Go mod private repo reads wrong path in cloudbuild<p>I'm attempting to build a go project in google cloudbuild that uses a private repo but when go attempts to download the module it fails to find the revision and it's looking in the wrong path. I followed the instructions provided by google <a href=""https://cloud.google.com/cloud-build/docs/access-private-github-repos"" rel=""nofollow noreferrer"">https://cloud.google.com/cloud-build/docs/access-private-github-repos</a>. And I tested it by running the same build in docker on cloudbuild which succeeds. Here's the error I'm seeing. </p>

<pre><code>go: github.com/company/repo/logging/v2@v2.0.6: reading github.com/company/repo/logging/logging/go.mod at revision logging/v2.0.6: unknown revision logging/v2.0.6
</code></pre>

<p>I'm not sure why it's attempt to go to logging/logging/go.mod that never existed.</p>",<p>As a workaround I ended up switching to a docker build and that worked. </p>
"Google Cloud Build & Firebase Deploy - ""An unexpected error has occurred""<p>I'm using Google Cloud Build to do a deployment to Firebase Hosting when a commit is on master. I'm using the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/firebase"" rel=""nofollow noreferrer"">Firebase</a> Cloud Builder, deployed to my project. I've checked permissions in the Cloud Console and Firebase access is enabled.</p>

<p>Everything seems to go well in generating the static HTML for hosting, but at the final build step it fails suddenly with this problem:</p>

<p><code>Error: An unexpected error has occurred.</code></p>

<p>Here is the step in my cloudbuild.yaml that fails:</p>

<pre><code>- name: gcr.io/$PROJECT_ID/firebase
  args: ['deploy', '--project', '$PROJECT_ID']
  id: Deploy to Firebase
</code></pre>","<p>The last time that I installed the container was in October. The ""An unexpected error has occurred"" was solved by others by updating their version of the Firebase CLI. I had to do the same thing in my CD environment in order to get this to work.</p>

<p>Following these instructions <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community"" rel=""nofollow noreferrer"">in the README</a>:</p>

<pre><code>cd cloud-builders-community/firebase
gcloud builds submit --config cloudbuild.yaml .
</code></pre>

<p>The Firebase CLI version was reinstalled and published to my project. Then I was able to retry the build and it worked successfully.</p>

<p>This tripped me up for about an hour yesterday, and I thought it may be helpful to just have the answer documented somewhere.</p>"
"How to deploy multiple cloud functions that are newly pushed using google cloud build and Source Repository?<p>I have a project folder with different cloud functions folders e.g.</p>

<pre><code>Project_Folder
    -Cloud-Function-Folder1
         -main.py
         -requirements.txt
         -cloudbuild.yaml
    -Cloud-Function-Folder2
         -main.py
         -requirements.txt
         -cloudbuild.yaml
    -Cloud-Function-Folder3
         -main.py
         -requirements.txt
         -cloudbuild.yaml
            --------- and so on!
</code></pre>

<p>Now what i have right now is. I push the code to the Source Repository one by one from the Cloud Fucntions folder to Source Repository(Separate Repos for each function folder). And then it has a Trigger enabled which trigger the cloud-build and then deploy the function.
The cloudbuild.yaml file i have is like this below..</p>

<pre><code> steps:

 - name: 'python:3.7'
 entrypoint: 'bash'
 args: 
   - '-c'
   - |
       pip3 install -r requirements.txt
       pytest

 - name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions 
  - deploy
  - Function
  - --runtime=python37
  - --source=.
  - --entry-point=function_main
  - --trigger-topic=Function
  - --region=europe-west3  
</code></pre>

<p>Now, what I would like to do is I would like to make a single source repo and whenever i change the code in one cloud function and push it then only it get deploys and rest remains like before.</p>

<hr>

<h2>Update</h2>

<p>Now i have also tried something like this below but it also deploy all the functions at the same time even though i am working on a single function.</p>

<pre><code>Project_Folder
    -Cloud-Function-Folder1
         -main.py
         -requirements.txt
    -Cloud-Function-Folder2
         -main.py
         -requirements.txt
    -Cloud-Function-Folder3
         -main.py
         -requirements.txt
    -cloudbuild.yaml
    -requirements.txt
</code></pre>

<p>cloudbuild.yaml file looks like this below</p>

<pre><code> steps:

 - name: 'python:3.7'
 entrypoint: 'bash'
 args: 
   - '-c'
   - |
       pip3 install -r requirements.txt
       pytest

 - name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions 
  - deploy
  - Function1
  - --runtime=python37
  - --source=./Cloud-Function-Folder1
  - --entry-point=function1_main
  - --trigger-topic=Function1
  - --region=europe-west3  

 - name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions 
  - deploy
  - Function2
  - --runtime=python37
  - --source=./Cloud-Function-Folder2
  - --entry-point=function2_main
  - --trigger-topic=Function2
  - --region=europe-west3 
</code></pre>","<p>It's more complex et you have to play with limit and constraint of Cloud Build.</p>

<p>I do this:</p>

<ul>
<li>get the directory updated since the previous commit</li>
<li>loop on this directory and do what I want</li>
</ul>

<hr>

<p><strong>Hypothesis 1</strong>: all the subfolders are deployed by using the same commands</p>

<p>So, for this I put a <code>cloudbuild.yaml</code> at the root of my directory, and not in the subfolders</p>

<pre><code>steps:
- name: 'gcr.io/cloud-builders/git'
  entrypoint: /bin/bash
  args:
    - -c
    - |
        # Cloud Build doesn't recover the .git file. Thus checkout the repo for this
        git clone --branch $BRANCH_NAME https://github.com/guillaumeblaquiere/cloudbuildtest.git /tmp/repo ;
        # Copy only the .git file
        mv /tmp/repo/.git .
        # Make a diff between this version and the previous one and store the result into a file
        git diff --name-only --diff-filter=AMDR @~..@ | grep ""/"" | cut -d""/"" -f1 | uniq &gt; /workspace/diff

# Do what you want, by performing a loop in to the directory
- name: 'python:3.7'
  entrypoint: /bin/bash
  args:
    - -c
    - |
       for i in $$(cat /workspace/diff); do
       cd $$i
           # No strong isolation between each function, take care of conflicts!!
           pip3 install -r requirements.txt
           pytest
       cd ..
       done

- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: /bin/bash
  args:
    - -c
    - |
       for i in $$(cat /workspace/diff); do
       cd $$i
           gcloud functions deploy .........           
       cd ..
       done
</code></pre>

<hr>

<p><strong>Hypothesis 2</strong>: the deployment is specific by subfolder </p>

<p>So, for this I put a <code>cloudbuild.yaml</code> at the root of my directory, and another one in the subfolders</p>

<pre><code>steps:
- name: 'gcr.io/cloud-builders/git'
  entrypoint: /bin/bash
  args:
    - -c
    - |
        # Cloud Build doesn't recover the .git file. Thus checkout the repo for this
        git clone --branch $BRANCH_NAME https://github.com/guillaumeblaquiere/cloudbuildtest.git /tmp/repo ;
        # Copy only the .git file
        mv /tmp/repo/.git .
        # Make a diff between this version and the previous one and store the result into a file
        git diff --name-only --diff-filter=AMDR @~..@ | grep ""/"" | cut -d""/"" -f1 | uniq &gt; /workspace/diff

# Do what you want, by performing a loop in to the directory. Here launch a cloud build
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: /bin/bash
  args:
    - -c
    - |
       for i in $$(cat /workspace/diff); do
       cd $$i
           gcloud builds submit
       cd ..
       done
</code></pre>

<p>Be careful to the <a href=""https://cloud.google.com/cloud-build/docs/build-config#timeout_2"" rel=""nofollow noreferrer"">timeout</a> here, because you can trigger a lot of Cloud Build and it take times.</p>

<hr>

<p>Want to run manually your build, don't forget to add the $BRANCH_NAME as substitution variable</p>

<pre><code>gcloud builds submit --substitutions=BRANCH_NAME=master
</code></pre>"
"Add timestamp to Google Cloud Build log output<p>We are using <a href=""https://cloud.google.com/cloud-build"" rel=""nofollow noreferrer"">Cloud Build</a> on <a href=""https://console.cloud.google.com/"" rel=""nofollow noreferrer"">Google Cloud Platform</a>.</p>

<p>Is it possible to add a timestamp to every line of the log output?</p>","<p>you cannot do this with Cloud Build directly but you can workaround by using <a href=""http://joeyh.name/code/moreutils/"" rel=""nofollow noreferrer"">moreutils</a> package on linux:</p>

<pre><code>gcloud builds submit --config cloudbuild.yaml | ts '[%Y-%m-%d %H:%M:%S]'
</code></pre>

<p>This will give you an output like:</p>

<pre><code>[2020-04-08 10:53:53] starting build ""50e00fbb-2224-46d4-b13a-6b15a9fbbe3c""
[2020-04-08 10:53:53]
[2020-04-08 10:53:53] FETCHSOURCE
[2020-04-08 10:53:53] Fetching storage object xxxxxxx
[2020-04-08 10:53:53] Copying xxxxxx
/ [1 files][  331.0 B/  331.0 B]                      
[2020-04-08 10:53:53] Operation completed over 1 objects/331.0 B.
[2020-04-08 10:53:53] BUILD
[2020-04-08 10:53:53] Starting Step #0
[2020-04-08 10:53:53] Step #0: Pulling image: ubuntu
[2020-04-08 10:53:53] Step #0: Using default tag: latest
[2020-04-08 10:53:53] Step #0: latest: Pulling from library/ubuntu
[2020-04-08 10:53:53] Step #0: Digest: xxxxxxx
[2020-04-08 10:53:53] Step #0: Status: Downloaded newer image for ubuntu:latest
[2020-04-08 10:53:53] Step #0: docker.io/library/ubuntu:latest
</code></pre>"
"In GCP, through Cloudbuild how can I ensure that only those steps get triggered for which changes have happened in file<p>My issue is that, given the below yaml file, if I'm making changes for example in any file of ""dir: process/cbd-bu-data"", Cloud Build runs all the steps serially when triggered. This leads to wastage of time.</p>

<p>I want that only that step runs in cloudbuild for which the changes have been made in file of that directory. What should I do to achieve this?</p>

<p>Here's my <code>cloudbuild.yaml</code> file:</p>

<pre><code>steps: 
  - args: 
      - beta
      - functions
      - deploy
      - ""--runtime=python37""
      - ""--trigger-http""
      - ""--entry-point=process_cbd_group_data""
      - process_cbd_group_data
      - ""--region=us-central1""
    dir: process/cbd-group-data
    name: gcr.io/cloud-builders/gcloud
  - args: 
      - beta
      - functions
      - deploy
      - ""--runtime=python37""
      - ""--trigger-http""
      - ""--entry-point=process_cbd_bu_data""
      - process_cbd_bu_data
      - ""--region=us-central1""
    dir: process/cbd-bu-data
    name: gcr.io/cloud-builders/gcloud
  - args: 
      - beta
      - functions
      - deploy
      - ""--runtime=python37""
      - ""--trigger-http""
      - ""--entry-point=process_cbd_structure_data""
      - process_cbd_structure_data
      - ""--region=us-central1""
    dir: process/cbd-structure-data
    name: gcr.io/cloud-builders/gcloud  
</code></pre>","<p>With your use case then the best approach would be having different triggers (3 in your use case) that listen to different tags or branches, being each of these specific for the file changes that you want to listen to. At the moment making Cloud Build steps execute when a certain file changes is not available.</p>"
"Can google cloud build tag the built container as latest without cloudbuild.yaml?<p>I am successfully using Google Cloud Build to build my app into a container upon a push to the master branch, and store that built container in Google Cloud Registry.</p>

<p>I'm trying to simplify my setup, so have removed the cloudbuild.yaml in favor of Cloud Build's ""auto detect docker"" feature. </p>

<p>The container still gets built, but the difference is that the built image is not tagged as <code>latest</code>. <code>latest</code> is still assigned to an older build that had been done while the <code>cloudbuild.yaml</code> was still present.</p>

<p>How can I make it so that each new container build gets tagged as <code>latest</code> without using <code>cloudbuild.yaml</code>?</p>","<p>The answer is yes, here's how:</p>

<ul>
<li>Edit the trigger used to build the container upon push to master</li>
<li>Under ""Build type configuration"", explicitly select ""Dockerfile"" rather than ""Autodetected""</li>
<li>Under ""Image name"", edit the value to replace <code>$COMMIT_SHA</code> at the end with <code>latest</code></li>
</ul>

<p>Run the trigger and you should see that your most recent container build is tagged with <code>latest</code>.</p>"
"Can you use ""expo build:ios"" on a CI environment with apple 2 factor authentication and how<p>Is there a way to use <code>expo build:ios</code> on a CI environment and pass 2 factor authentication ?</p>

<p>I'm having troubles to start a build using expo on our CI environment due to the 2 factor authentication required for my apple developer account.</p>

<p>Is the <code>expo build:ios</code> command supposed to only be used locally or without 2 factor authentication ?</p>

<p>Running the build <strong>locally</strong> in interactive mode works - password and 2 factor auth was required only the first time I chose to build the app. For consecutive build I only use <code>expo build:ios --release-channel test</code> and that's it </p>

<p>My CI environment is google's Cloud Build - it runs in a Docker container, so I can't go and do a one time manual login to have the runner persist some session information like it happens locally can I?</p>

<hr>

<p>After reading the cli usage information for <code>expo build:ios</code> and that you can setup your account password as ENV variable like <code>EXPO_APPLE_PASSWORD</code> I though it should be possible to use it in a CI environment </p>

<p>But then I get the following result:</p>

<pre class=""lang-sh prettyprint-override""><code>expo login --non-interactive -u XXX

Success. You are now logged in as XXX.
</code></pre>

<pre class=""lang-sh prettyprint-override""><code>expo build:ios --non-interactive --no-wait --release-channel test --apple-id XXXXX --team-id F7NE7X9ZFW
</code></pre>

<pre class=""lang-sh prettyprint-override""><code>- Making sure project is set up correctly...
[16:43:00] Checking if there is a build in progress...
[16:43:00] Trying to authenticate with Apple Developer Portal...
Two-factor Authentication (6 digits code) is enabled for account 'XXXXX'
More information about Two-factor Authentication: https://support.apple.com/en-us/HT204915
If you're running this in a non-interactive session (e.g. server or CI)
check out https://github.com/fastlane/fastlane/tree/master/spaceship#2-step-verification
...
Please enter the 6 digit code:
[16:43:02] Authentication with Apple Developer Portal failed!
[16:43:02] Reason: Unknown reason, raw: ""The input stream is exhausted.""
</code></pre>

<p>So I follow the link for non-interactive sessions: <a href=""https://github.com/fastlane/fastlane/tree/master/spaceship#2-step-verification"" rel=""noreferrer"">https://github.com/fastlane/fastlane/tree/master/spaceship#2-step-verification</a></p>

<p>And I see that I have to create and use a <code>FASTLANE_SESSION</code> environment variable. 
But at that point I'm, not sure whether it will work as it seems this is coming from some tooling that expo uses internally. </p>

<p>I guess I can install <code>fastlane</code> locally and try to generate this variable with the command <code>fastlane spaceauth -u user@example.org</code>. I didn't find anything about this in the expo documentation though</p>

<p>There's information on using <code>expo build:ios</code> to build standalone apps, it covers only how to build them locally <a href=""https://docs.expo.io/versions/v36.0.0/distribution/building-standalone-apps/"" rel=""noreferrer"">https://docs.expo.io/versions/v36.0.0/distribution/building-standalone-apps/</a></p>

<p>They have a guide for a CI flow (""on Your CI"") - but it's using <code>turtle-cli</code> and for iOS it requres to run on a mac environment - it covers how to do builds yourself on your own machine and not using <code>expo.io</code> <a href=""https://docs.expo.io/versions/v36.0.0/distribution/turtle-cli/"" rel=""noreferrer"">https://docs.expo.io/versions/v36.0.0/distribution/turtle-cli/</a></p>

<hr>

<p>Another thing that I've tried is running like:</p>

<pre class=""lang-sh prettyprint-override""><code>expo build:ios \
  --team-id YOUR_TEAM_ID \
  --dist-p12-path /path/to/your/dist/cert.p12 \
  --provisioning-profile-path /path/to/your/provisioning/profile.mobileprovision
</code></pre>

<p>With the <code>EXPO_IOS_DIST_P12_PASSWORD</code> set as env variable but the build would fail as it still needs apple id and password </p>

<hr>

<p>Is there a way that I can share or copy my local session - the session that expo created the first time I ran <code>build:ios</code> where I'm authenticated and I'm not prompted for password anymore to the CI machine? </p>","<p>Here's what worked for me in the end:</p>
<p>I've created another bundle identifier <code>com.myawesome.stuff</code>
I've also generated/created:</p>
<ul>
<li>Distribution Certificate P12  <code>--dist-p12-path</code></li>
<li>Push Key .p8 file - <code>--push-id</code> and <code>--push-p8-path</code></li>
<li>Provisioning Profile. - <code>--push-p8-path</code></li>
</ul>
<p>Then <strong>locally</strong> I've run the following:</p>
<p>Login interactively</p>
<pre class=""lang-sh prettyprint-override""><code>expo login
</code></pre>
<p>Authenticate with apple interactively</p>
<pre class=""lang-sh prettyprint-override""><code>expo build:ios --clear-credentials --apple-id my@appleId.com --team-id=TheTeam --dist-p12-path=dist.p12 --provisioning-profile-path=profile.mobileprovision --push-p8-path=push.p8 --push-id=THEID --release-channel test
</code></pre>
<p><em>Tobe fair I've run those locally but in the google cloud console terminal</em></p>
<p>Then <strong>on the CI environment</strong> we just use:</p>
<pre class=""lang-sh prettyprint-override""><code>expo login --non-interactive &quot;--username=XXXX&quot;
</code></pre>
<pre class=""lang-sh prettyprint-override""><code>expo build:ios --non-interactive --no-wait &quot;--release-channel=$BRANCH_NAME&quot;
</code></pre>
<p>Which works and produces the following log</p>
<pre><code>- Making sure project is set up correctly...
[17:48:39] Checking if there is a build in progress...

[17:48:40] Fetching available credentials
[17:48:40] Unable to validate distribution certificate due to insufficient Apple Credentials
[17:48:40] Unable to validate Push Keys due to insufficient Apple Credentials
- Performing best effort validation of Provisioning Profile...

[xmldom error]  element parse error: Error: invalid tagName: 
@#[line:99,col:125]
[xmldom error]  element parse error: Error: invalid tagName: 
@#[line:114,col:75]
[xmldom error]  element parse error: Error: invalid tagName: 
@#[line:143,col:84]
✔ Successfully performed best effort validation of Provisioning Profile.
[17:48:40]
[17:48:40] Project Credential Configuration:
[17:48:40]   Experience: @XXXX/XXXX, bundle identifier: com.XXXXXXXX
[17:48:40]     Provisioning profile is missing. It will be generated during the next build
[17:48:40]     Apple Team ID: XXXXXXX,  Apple Team Name: ---------
[17:48:40]
[17:48:40]   Distribution Certificate - Certificate ID: -----
[17:48:40]     Apple Team ID: XXXXXXX,  Apple Team Name: ---------
[17:48:40]     used by
      @XXXX/XXXX (com.XXXXXXX)
[17:48:40]   Push Notifications Key - Key ID: XXXXXXXX
[17:48:40]     Apple Team ID: XXXXXXX,  Apple Team Name: ---------
[17:48:40]     used by
      @XXXX/XXXX (com.XXXXXXX)
[17:48:40] Unable to find an existing Expo CLI instance for this directory, starting a new one...
[17:48:42] Starting Metro Bundler on port 19001.
[17:48:46] Tunnel ready.
[17:48:46] Publishing to channel 'test'...
[17:48:47] Building iOS bundle
[17:50:13] Finished building JavaScript bundle in 60785ms.
[17:50:13] Building Android bundle
[17:51:04] Finished building JavaScript bundle in 51597ms.
[17:51:04] Analyzing assets
[17:51:06] Finished building JavaScript bundle in 1669ms.
[17:51:08] Finished building JavaScript bundle in 1526ms.
[17:51:08] Uploading assets
[17:51:08] No assets changed, skipped.
[17:51:08] Processing asset bundle patterns:
[17:51:08] - /workspace/**/*
[17:51:08] Uploading JavaScript bundles
[17:51:12] Published
[17:51:12] Your URL is

https://exp.host/@XXXX/XXXX?release-channel=test

[17:51:12] › Closing Expo server
[17:51:12] › Stopping Metro bundler
[17:51:13] Checking if this build already exists...

[17:51:13] Build started, it may take a few minutes to complete.
[17:51:13] You can check the queue length at https://expo.io/turtle-status

[17:51:13] You can make this faster. 
Get priority builds at: https://expo.io/settings/billing

[17:51:13] You can monitor the build at

 https://expo.io/dashboard/XXXX/builds/e5c32814-8613-4fef-889a-05ca982e952f

[17:51:13] Alternatively, run `expo build:status` to monitor it from the command line.
</code></pre>
<p><img src=""https://i.stack.imgur.com/F4iYG.png"" alt=""check"" /> Despite the troublesome warnings at the start the build works and produces an <code>.ipa</code> that we've successfully submitted to test flight</p>
<blockquote>
<p>[xmldom error]    element parse error: Error: invalid tagName: @#[line:99,col:125]</p>
<p>[17:48:40]     Provisioning profile is missing. It will be generated during the next build</p>
</blockquote>
<hr />
<p>I think the problem might be that the google cloud build machine we're using is setup with a US location, but the last time I used the <code>expo build:ios</code> command locally on my pc I was in a different region. So just running <code>expo build:ios -clear-credentials</code> might be enough and you can let expo create and manage all the required certificates.</p>
<p>But nowhere did I found any information that on a <code>CI</code> (non interactive) environment you should use <code>expo build:ios</code> without providing apple id and credentials</p>"
"Google App Engine Deploy Error[12] Cannot allocate memory<p>My Application is running as 
  - Google App Engine Standard Environment : Python 37</p>

<p>I deploy my application using gcloud as following command </p>

<blockquote>
  <p>gcloud app deploy --project [project-name] --version uno</p>
</blockquote>

<p>my build fail as the following error </p>

<pre><code>Step #1 - ""builder"": INFO     gzip_tar_runtime_package took 18 seconds
Step #1 - ""builder"": INFO     Finished gzipping tarfile.
Step #1 - ""builder"": INFO     Building app layer took 47 seconds
Step #1 - ""builder"": INFO     starting: Building app layer
Step #1 - ""builder"": INFO     starting: tar_runtime_package
Step #1 - ""builder"": INFO     tar_runtime_package tar -pcf /tmp/tmpyhSCMU.tar --hard-dereference --transform flags=r;s,^,/.googleconfig/, --exclude *.pyc .
Step #1 - ""builder"": INFO     tar_runtime_package took 0 seconds
Step #1 - ""builder"": INFO     Building app layer took 0 seconds
Step #1 - ""builder"": INFO     build process for FTL image took 50 seconds
Step #1 - ""builder"": INFO     full build took 50 seconds
Step #1 - ""builder"": ERROR    tar_runtime_package tar -pcf /tmp/tmpyhSCMU.tar --hard-dereference --transform flags=r;s,^,/.googleconfig/, --exclude *.pyc .
Step #1 - ""builder"": exited with error [Errno 12] Cannot allocate memory
Step #1 - ""builder"": tar_runtime_package is likely not on the path
Step #1 - ""builder"": Traceback (most recent call last):
Step #1 - ""builder"":   File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
Step #1 - ""builder"":     ""__main__"", fname, loader, pkg_name)
Step #1 - ""builder"":   File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
Step #1 - ""builder"":     exec code in run_globals
Step #1 - ""builder"":   File ""/usr/local/bin/ftl.par/__main__.py"", line 65, in &lt;module&gt;
Step #1 - ""builder"":   File ""/usr/local/bin/ftl.par/__main__.py"", line 60, in main
Step #1 - ""builder"":   File ""/usr/local/bin/ftl.par/__main__/ftl/common/ftl_error.py"", line 77, in InternalErrorHandler
Step #1 - ""builder"": IOError: [Errno 2] No such file or directory: '""""/output'
</code></pre>

<p>More Information</p>

<ul>
<li>My Task Queue is already empty.</li>
<li>I try to up size my instance class from F1 to F2 and F3 and it still fail.</li>
</ul>","<p>The error <code>cannot allocate memory</code> usually appears if the number of files or the size of the content that is being uploaded is too big and the instance used to deploy your code runs out of memory.</p>

<p>The cause of this error might be the fact that you are using too many dependencies or static files.</p>

<p>Are you downloading static files ? I see this command <code>tar_runtime_package tar</code> before the error. </p>

<p>I would suggest to make sure you only use the necessary dependencies or choose to deploy your application on Google Cloud Run.</p>"
"Upload jest code coverage results to codecov during google cloud build<p>I'm trying to figure out how to upload my jest code coverage reports to codecov. 
From there <a href=""https://docs.codecov.io/docs/about-the-codecov-bash-uploader#section-flag"" rel=""nofollow noreferrer"">documentation</a>:</p>

<pre><code>bash &lt;(curl -s https://codecov.io/bash) -t token
</code></pre>

<p>So I tried to run the bash script from a cloud build step witth the following <code>cloudbuild.yaml</code></p>

<pre><code>steps:
  - name: node:10.15.1
    entrypoint: npm
    args: [""install""]
  - name: node:10.15.1
    entrypoint: npm
    args: [""test"", ""--"", ""--coverage""]
  - name: 'gcr.io/cloud-builders/curl'
    entrypoint: bash
    args: ['&lt;(curl -s https://codecov.io/bash)', '-t', '$_CODECOV_TOKEN']
  - name: node:10.15.1
    entrypoint: npm
    args: [""run"", ""build:production""]
</code></pre>

<p>I get the following error: </p>

<pre><code>Step #2: bash: &lt;(curl -s https://codecov.io/bash): No such file or directory
</code></pre>

<p>Obviously because the <code>&lt;(curl -s https://codecov.io/bash)</code> is interpreted as string while I want it to be executed.</p>

<p><strong>Edit:</strong> </p>

<p>I've changed my build step to the following: </p>

<pre><code>  - name: ""gcr.io/cloud-builders/curl""
    entrypoint: bash
    args: [""./scripts/codecov-upload.bash"", ""$_CODECOV_TOKEN""]
</code></pre>

<p>And added a file <code>codecov-upload.bash</code></p>

<pre><code>bash &lt;(curl -s https://codecov.io/bash) -t $1
</code></pre>

<p>When running my cloud build the codecov bash uploader succesfully starts. However, I doesn't manage to upload to reports to clodecov.</p>

<p>Here's the logs from codecov bash uploader:</p>

<pre><code>Step #2: Test Suites: 1 passed, 1 total
Step #2: Tests:       1 passed, 1 total
Step #2: Snapshots:   1 passed, 1 total
Step #2: Time:        28.981s
Step #2: Ran all test suites.
Finished Step #2
Starting Step #3
Step #3: Already have image (with digest): gcr.io/cloud-builders/curl
Step #3: /dev/fd/63: option requires an argument -- t
Step #3: 
Step #3:   _____          _
Step #3:  / ____|        | |
Step #3: | |     ___   __| | ___  ___ _____   __
Step #3: | |    / _ \ / _` |/ _ \/ __/ _ \ \ / /
Step #3: | |___| (_) | (_| |  __/ (_| (_) \ V /
Step #3:  \_____\___/ \__,_|\___|\___\___/ \_/
Step #3:                               Bash-tbd
Step #3: 
Step #3: 
Step #3: x&gt; No CI provider detected.
Step #3:     Testing inside Docker? http://docs.codecov.io/docs/testing-with-docker
Step #3:     Testing with Tox? https://docs.codecov.io/docs/python#section-testing-with-tox
Step #3:     project root: .
Step #3: /dev/fd/63: line 897: git: command not found
Step #3: /dev/fd/63: line 897: hg: command not found
Step #3:     Yaml not found, that's ok! Learn more at http://docs.codecov.io/docs/codecov-yaml
Step #3: ==&gt; Running gcov in . (disable via -X gcov)
Step #3: ==&gt; Python coveragepy not found
Step #3: ==&gt; Searching for coverage reports in:
Step #3:     + .
Step #3:     -&gt; Found 3 reports
Step #3: ==&gt; Detecting git/mercurial file structure
Step #3: ==&gt; Reading reports
Step #3:     + ./coverage/clover.xml bytes=163786
Step #3:     + ./coverage/coverage-final.json bytes=444241
Step #3:     + ./coverage/lcov.info bytes=71582
Step #3: ==&gt; Appending adjustments
Step #3:     http://docs.codecov.io/docs/fixing-reports
Step #3:     + Found adjustments
Step #3: ==&gt; Gzipping contents
Step #3: ==&gt; Uploading reports
Step #3:     url: https://codecov.io
Step #3:     query: branch=&amp;commit=&amp;build=&amp;build_url=&amp;name=&amp;tag=&amp;slug=&amp;service=&amp;flags=&amp;pr=&amp;job=
Step #3:     -&gt; Pinging Codecov
Step #3: https://codecov.io/upload/v4?package=bash-tbd&amp;token=secret&amp;branch=&amp;commit=&amp;build=&amp;build_url=&amp;name=&amp;tag=&amp;slug=&amp;service=&amp;flags=&amp;pr=&amp;job=
Step #3:     -&gt; Uploading
Step #3:     X&gt; Failed to upload
Step #3:     -&gt; Sleeping for 30s and trying again...
Step #3:     -&gt; Pinging Codecov
Step #3: https://codecov.io/upload/v4?package=bash-tbd&amp;token=secret&amp;branch=&amp;commit=&amp;build=&amp;build_url=&amp;name=&amp;tag=&amp;slug=&amp;service=&amp;flags=&amp;pr=&amp;job=
Step #3:     -&gt; Uploading
Step #3:     X&gt; Failed to upload
Step #3:     -&gt; Sleeping for 30s and trying again...
Step #3:     -&gt; Pinging Codecov
Step #3: https://codecov.io/upload/v4?package=bash-tbd&amp;token=secret&amp;branch=&amp;commit=&amp;build=&amp;build_url=&amp;name=&amp;tag=&amp;slug=&amp;service=&amp;flags=&amp;pr=&amp;job=
Step #3:     -&gt; Uploading
Step #3:     X&gt; Failed to upload
Step #3:     -&gt; Sleeping for 30s and trying again...
Step #3:     -&gt; Pinging Codecov
Step #3: https://codecov.io/upload/v4?package=bash-tbd&amp;token=secret&amp;branch=&amp;commit=&amp;build=&amp;build_url=&amp;name=&amp;tag=&amp;slug=&amp;service=&amp;flags=&amp;pr=&amp;job=
Step #3:     -&gt; Uploading
Step #3:     X&gt; Failed to upload
Step #3:     -&gt; Sleeping for 30s and trying again...
Step #3:     -&gt; Uploading to Codecov
Step #3:     HTTP 400
Step #3: missing required properties: [&amp;#39;commit&amp;#39;]
Finished Step #3
Starting Step #4
Step #4: Already have image: node:10.15.1
Step #4: 
</code></pre>

<p>I've noticed two things in the logs:</p>

<pre><code>1. Step #3: /dev/fd/63: option requires an argument -- t
2. Step #3: missing required properties: [&amp;#39;commit&amp;#39;]
</code></pre>

<p>When searching to fix number 2, I found the following on SO: 
<a href=""https://stackoverflow.com/questions/44749241/codecov-io-gives-error-in-combination-with-bitbucket-pipelines"">codecov.io gives error in combination with Bitbucket pipelines</a></p>

<p>Where the answer seems to be to that git is not installed in my container.</p>

<p>So I've tried to make a custom container image with docker:</p>

<p>Dockerfile:</p>

<pre><code>FROM gcr.io/cloud-builders/curl
RUN apt-get update &amp;&amp; \
    apt-get upgrade -y &amp;&amp; \
    apt-get install -y git
</code></pre>

<p>So I build the image: </p>

<pre><code>build -t ""gcr.io/[PROJECT_ID]/builder .
</code></pre>

<p>And update my build step to use this image instead: </p>

<ul>
<li>name: ""gcr.io/$PROJECT_ID/builder""
entrypoint: bash
args: [""./scripts/codecov-upload.bash""]</li>
</ul>

<p>But using the image created with that dockerfile returns the same errors.</p>

<p>Maybe the Dockerfile for that custom image isn't correct? Or I'm missing something else? </p>

<p>My code is available on github: <a href=""https://github.com/thdk/timesheets/tree/feat/112-1"" rel=""nofollow noreferrer"">https://github.com/thdk/timesheets/tree/feat/112-1</a></p>","<p>After the <a href=""https://stackoverflow.com/a/61120917/681803"">answer from Ajordat</a> , <a href=""https://community.codecov.io/t/needed-help-to-upload-coverage-reports-with-google-cloud-build/1246/2?u=thdk"" rel=""noreferrer"">an answer on the community of codecov</a> and looking into the <a href=""https://github.com/codecov/codecov-bash/blob/master/codecov"" rel=""noreferrer"">source code of the codecov batch uploader</a> I discovered that some environment variables are required for the bash uploader to work.</p>

<p>I've changed my build step in <code>cloudbuild.yaml</code> to include environmental variables. The values for these are included in the <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values#using_default_substitutions"" rel=""noreferrer"">default substition variables from google cloud build</a>.</p>

<pre><code>- name: 'gcr.io/cloud-builders/curl'
  entrypoint: bash
  args: ['-c', 'bash &lt;(curl -s https://codecov.io/bash)']
  env:
  - 'VCS_COMMIT_ID=$COMMIT_SHA'
  - 'VCS_BRANCH_NAME=$BRANCH_NAME'
  - 'VCS_PULL_REQUEST=$_PR_NUMBER'
  - 'CI_BUILD_ID=$BUILD_ID'
  - 'CODECOV_TOKEN=$_CODECOV_TOKEN' # _CODECOV_TOKEN is user user substituion variable specified in my cloud build trigger
</code></pre>

<p>This seems to work except for a warning from the bash uploader:</p>

<pre><code>Step #3: /dev/fd/63: line 897: git: command not found
Step #3: /dev/fd/63: line 897: hg: command not found
</code></pre>

<p>So I had to use my own build image starting from the curl image and add git to it.</p>

<p>Dockerfile:</p>

<pre><code>FROM gcr.io/cloud-builders/curl
RUN apt-get update &amp;&amp; \
    apt-get upgrade -y &amp;&amp; \
    apt-get install -y git
</code></pre>

<p>And build the image:</p>

<pre><code>docker build -t ""gcr.io/[PROJECT_ID]/builder .
</code></pre>

<p>So my final <code>cloudbuild.yaml</code> file is:</p>

<pre><code>steps:
  - name: node:10.15.1
    entrypoint: npm
    args: [""install""]
  - name: node:10.15.1
    entrypoint: npm
    args: [""test"", ""--"", ""--coverage""]      
  - name: node:10.15.1
    entrypoint: npm
    args: [""run"", ""build:production""]
  - name: ""gcr.io/$PROJECT_ID/builder""
    entrypoint: bash
    args: ['-c', 'bash &lt;(curl -s https://codecov.io/bash)']
    env:
    - 'VCS_COMMIT_ID=$COMMIT_SHA'
    - 'VCS_BRANCH_NAME=$BRANCH_NAME'
    - 'VCS_PULL_REQUEST=$_PR_NUMBER'
    - 'CI_BUILD_ID=$BUILD_ID'
    - 'CODECOV_TOKEN=$_CODECOV_TOKEN'
</code></pre>"
"How can I call gcloud commands from a shell script during a build step?<p>I have automatic builds set up in Google Cloud, so that each time I push to the master branch of my repository, a new image is built and pushed to Google Container Registry.</p>

<p>These images pile up quickly, and I don't need all the old ones. So I would like to add a build step that runs a bash script which calls <code>gcloud container images list-tags</code>, loops the results, and deletes the old ones with <code>gcloud container images delete</code>.</p>

<p>I have the script written and it works locally. I am having trouble figuring out how to run it as a step in Cloud Builder.</p>

<p>It seems there are 2 options:</p>

<pre><code>- name: 'ubuntu'
  args: ['bash', './container-registry-cleanup.sh']
</code></pre>

<p>In the above step in <code>cloudbuild.yml</code> I try to run the <code>bash</code> command in the <code>ubuntu</code> image. This doesn't work because the <code>gcloud</code> command does not exist in this image.</p>

<pre><code>- name: 'gcr.io/cloud-builders/gcloud'
  args: [what goes here???]
</code></pre>

<p>In the above step in <code>cloudbuild.yml</code> I try to use the <code>gcloud</code> image, but since ""Arguments passed to this builder will be passed to <code>gcloud</code> directly"", I don't know how to call my bash script here.</p>

<p>What can I do?</p>","<p>You can customize the entry point of your build step. If you need gcloud installed, use the gcloud cloud builder and do this</p>

<pre><code>step:
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: ""bash""
    args:
      - ""-c""
      - |
          echo ""enter 1 bash command per line""
          ls -la
          gcloud version
          ...
</code></pre>"
"In GCP, how can I trigger the automatic deployment of Cloud Function in to Production project from the source files present in repo of DEV project<p>I want to automate the deployment of Cloud Function in to Production project through Cloud Build whose source files are present in Cloud Source Repository of DEV project. How can I ensure that the moment I push the code in production branch of Cloud Source Repository of DEV project, the Cloud Function gets created in to Production Project .</p>","<p>If I understand,  you are trying to trigger a build from a repository stored on another project.</p>

<p>This is not possible, the build triggers must be on the same project than the repositories </p>"
"GCP Cloud build with cloud function doesn't deploy the latest code<p>I am trying to setup CI/CD pipeline with Google Cloud Build for deploying the Google Cloud Functions with GitHub repository.</p>

<p>I have managed to create the trigger and whenever i push changes to master branch, the build is triggering. But after the deployment and Cloud Function Version is incremented, when i invoke the cloud function, it still executing the old function.</p>

<p>Following is the buildconfig.yaml</p>

<pre><code>steps:
- name: gcr.io/cloud-builders/git
  args: ['clone', 'https://github.com/mayuran19/GCP-CloudFunction']
- name: gcr.io/cloud-builders/git
  args: ['pull', 'https://github.com/mayuran19/GCP-CloudFunction', 'master']
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['functions', 'deploy', 'function-1', '--trigger-http', '--runtime', 'nodejs8', '--entry-point', 'helloWorld']
  dir: './'
</code></pre>","<p>It's challenging to debug Cloud Build but I think you're missing the correct deployment source.</p>

<p>The <code>git clone ...</code> step creates <code>/workspace/GCP-CloudFunction</code></p>

<p>But you <code>gcloud functions deploy ...</code> from (default == <code>/workspace</code>).</p>

<p>You need to point <code>gcloud functions deploy ... --source=./GCP-CloudFunction</code>. (since you're in <code>/workspace</code>; or <code>--source=/workspace/GCP-CloudFunction</code> to be explicit).</p>

<p>A useful debugging mechanism is to add e.g. a <code>busybox</code> step that <code>ls -la /workspace</code> to ensure that, the workspace contains what you're expecting.</p>"
"google-cloud-build PyPi 400 errors<p>I am currently receiving 400 errors when invoking the <code>list_builds()</code> method (seen here: <a href=""https://googleapis.dev/python/cloudbuild/latest/gapic/v1/api.html?highlight=list_builds#google.cloud.devtools.cloudbuild_v1.CloudBuildClient.list_builds"" rel=""nofollow noreferrer"">https://googleapis.dev/python/cloudbuild/latest/gapic/v1/api.html?highlight=list_builds#google.cloud.devtools.cloudbuild_v1.CloudBuildClient.list_builds</a>)</p>

<p>The following command works using gcloud:</p>

<p><code>gcloud builds list --filter=""status=FAILURE""</code></p>

<p>However, the following API call returns <code>google.api_core.exceptions.InvalidArgument: 400 Error processing filter expression</code></p>

<pre><code>for element in client.list_builds(""REDACTED"", filter_=""status=FAILURE""):
    # process element
    pass
</code></pre>

<p>I'm guessing I'm missing something very obvious and simple here but I can't exactly figure out what I'm doing wrong</p>","<p>The correct way to pass in the filter string to the API call includes using double apostrophes around the actual text like so:</p>

<p><code>filter_='status=""FAILURE""'</code></p>

<p>Unsure of whether or not this will be changed in the future, but this is the same behavior for passing it in via the REST API here: <a href=""https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.builds/list"" rel=""nofollow noreferrer"">https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.builds/list</a></p>

<p>e.g specifiying <code>status=FAILURE</code> will fail, but <code>status=""FAILURE""</code> returns a 200 response.</p>"
"cloudbuild.yaml build pipeline for python flask app on gcloud compute engine<p>I have a Python Flask app which needs a library which is not preinstalled on Gcloud compute instances <code>libsndfile1</code> — also why I can't run this through app engine. I have set up a cloud build pipeline so my <code>cloudbuild.yaml</code> triggers every time I push to master on my repository.</p>
<p>Here are my 2 questions:</p>
<ol>
<li>Where does the cloudbuild.yaml execute it's instructions?</li>
<li>How do I make the cloudbuild.yaml execute <code>apt-get install libsndfile1 python3 python3-pip</code> and then also execute <code>pip3 install requirements.txt</code> where <code>requirements.txt</code> is a file in the repository that triggers the cloud build?</li>
</ol>
<p>Disclaimer: I am an ultra-beginner with docker but if you think that docker is the only way to do it then please explain how something like this can be done.</p>","<p>If you are interested in App Engine, my recommendation would be to migrate your application to the Google App Engine Flexible environment, where you could choose a <a href=""https://cloud.google.com/appengine/docs/flexible/custom-runtimes/build"" rel=""nofollow noreferrer"">Custom runtime</a> for your Flask application. Simply include a step on your Dockerfile to install the required library. It should look similar to this:</p>

<pre><code>FROM python:3.7
WORKDIR /app
COPY . /app
RUN apt-get update &amp;&amp;\
    apt-get install -y libsndfile1
RUN pip install -r requirements.txt
EXPOSE 8080
CMD [""gunicorn"", ""main:app"", ""-b"", "":8080"", ""--timeout"", ""300""]
</code></pre>

<p>And within this context you could use Cloud Build to automate your deployments in a fairly easy manner by following the relevant section of the <a href=""https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-appengine"" rel=""nofollow noreferrer"">documentation</a>.</p>

<p>Now regarding your questions.</p>

<p><strong><em>(1) Where does the cloudbuild.yaml execute it's instructions?</em></strong></p>

<p>Here you can find all the relevant information of how Cloud Build works <a href=""https://cloud.google.com/cloud-build/docs/overview"" rel=""nofollow noreferrer"">here</a> and to answer your first question Cloud Build is a separate service and runs each step of the build in a Docker container within Google's Infrastructure. </p>

<p><strong><em>(2) How do I make the cloudbuild.yaml execute <code>apt-get install libsndfile1 python3 python3-pip</code> and then also execute <code>pip3 install requirements.txt</code> where requirements.txt is a file in the repo that triggers the cloud build.</em></strong></p>

<p>Please notice that Cloud Build is more oriented to be used with other products like Cloud Run, GKE, Cloud Functions, etc. And the only fairly straightforward that I've seen Cloud Build being used in the context of Compute Engine would be to <a href=""https://cloud.google.com/cloud-build/docs/building/build-vm-images-with-packer"" rel=""nofollow noreferrer"">build VM images using Packer</a>. In which case you'll need to <a href=""https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images"" rel=""nofollow noreferrer"">create a custom image</a> with all the dependencies already installed along within your application.</p>

<p>The specifics of the solution will vary depending on the OS of your Compute Engine instance. But you'll basically need to do the following:</p>

<ol>
<li>Upload a startup script that installs <code>libsndfile1</code> as well as all the dependencies specified on the <code>requirements.txt</code> file to a bucket in Cloud Storage.</li>
<li>Take advantage that you can use gcloud to <a href=""https://cloud.google.com/compute/docs/startupscript#startupscriptrunninginstances"" rel=""nofollow noreferrer"">apply a startup script to running instances</a> in a similar fashion to.</li>
</ol>

<pre><code>gcloud compute instances add-metadata example-instance \
    --metadata startup-script-url=gs://bucket/file
</code></pre>

<p>and there is <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/gcloud"" rel=""nofollow noreferrer"">cloud builder for gcloud</a>
3. Build a cloudbuild.yaml file that uses 2. and restart the instance (not recommended if your app is in production since it will cause downtime) in a similar fashion to:</p>

<p>cloudbuild.yaml</p>

<pre><code>steps:
- name: gcr.io/cloud-builders/gcloud
  args: ['compute', 'instances', 'add-metadata', 'example-instance','--metadata startup-script-url=gs://bucket/file']
- name: gcr.io/cloud-builders/gcloud
  args: ['compute', 'instances', 'reset', 'example-instance']
</code></pre>

<ol start=""4"">
<li>Give the cloud build service account enough permissions to execute that task (<a href=""https://cloud.google.com/compute/docs/access/iam#compute.admin"" rel=""nofollow noreferrer"">Compute Admin</a> and <a href=""https://cloud.google.com/storage/docs/access-control/iam-roles#standard-roles"" rel=""nofollow noreferrer"">Storage Admin</a>).</li>
<li>Run the build.</li>
</ol>

<p>I shared the previous steps in aims of answering your concrete question but I would insist that my recommendation would be to migrate the app to App Engine Flexible environment with custom runtime as explained before.</p>"
"How can I specify a region for the Cloud Storage buckets used by Cloud Build for a Cloud Run deployment?<p>When deploying a docker container image to Cloud Run, I can choose a region, which is fine.  Cloud Run delegates the build to Cloud Build, which apparently creates two buckets to make this happen.  The unexpected behavior is that buckets aren't created in the region of the Cloud Run deployment, and instead default to multi-region US.</p>

<p><strong>How do I specify the region as ""us-east1"" so the cost of storage is absorbed by the ""always free"" tier?</strong>  (Apparently US multi-region storage buckets store data in regions outside of the free tier limits, which resulted in a surprise bill - I am trying to avoid that bill.)</p>

<p>If it matters, I am also using Firebase in this project.  I created the Firebase default storage bucket in the us-east1 region with the hopes that it might also become the default for other buckets, but this is not so.  The final bucket list looks like this, where you can see the two buckets created automatically with the undesirable multi-region setting.</p>

<p><a href=""https://i.stack.imgur.com/MRS5H.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MRS5H.png"" alt=""enter image description here""></a></p>

<p>This is the shell script I'm using to build and deploy:</p>

<pre class=""lang-sh prettyprint-override""><code>#!/bin/sh

project_id=$1
service_id=$2

if [ -z ""$project_id"" ]; then
    echo ""First argument must be the Google Cloud project ID"" &gt;&amp;2
    exit 1
fi

if [ -z ""$service_id"" ]; then
    echo ""Second argument must be the Cloud Run app name"" &gt;&amp;2
    exit 1
fi

echo ""Deploying $service_id to $project_id""

tag=""gcr.io/$project_id/$service_id""

gcloud builds submit \
    --project ""$project_id"" \
    --tag ""$tag"" \
&amp;&amp; \
gcloud run deploy ""$service_id"" \
    --project ""$project_id"" \
    --image ""$tag"" \
    --platform managed \
    --update-env-vars ""GOOGLE_CLOUD_PROJECT=$project_id"" \
    --region us-central1 \
    --allow-unauthenticated
</code></pre>","<p>As you mention, Cloud Build creates a bucket or buckets with multi region because when creating the service in Cloud Run, there are only added the needed flags and arguments to deploy the service.</p>
<p>The <a href=""https://cloud.google.com/sdk/gcloud/reference/builds/submit#--gcs-source-staging-dir"" rel=""noreferrer"">documentation</a> for the command <code>gcloud builds submit</code> mentions the following for the flag <code>--gcs-source-staging-dir</code>:</p>
<blockquote>
<pre><code>--gcs-source-staging-dir=GCS_SOURCE_STAGING_DIR
</code></pre>
<p>A directory in Google Cloud Storage to copy the source used for staging the build. If the specified bucket does not exist, Cloud Build will create one. If you don't set this field, gs://[PROJECT_ID]_cloudbuild/source is used.</p>
</blockquote>
<p>As this flag is not set, the bucket is created in <code>multi-region</code> and in <code>us</code>. This behavior also applies for the flag <a href=""https://cloud.google.com/sdk/gcloud/reference/builds/submit#--gcs-log-dir"" rel=""noreferrer""><code>--gcs-log-dir</code></a>.</p>
<p>Now the necessary steps to use the bucket in the dual-region, region or multi-region you want is using a <code>cloudbuild.yaml</code> and using the flag <code>--gcs-source-staging-dir</code>. You can do the following:</p>
<ol>
<li>Create a bucket in the region, dual-region or multi-region you may want. For example I created a bucket called &quot;example-bucket&quot; in <code>australia-southeast1</code>.</li>
<li>Create a <code>cloudbuild.yaml</code> file. This is necessary to store the artifacts of the build in the bucket you want as mentioned <a href=""https://cloud.google.com/cloud-build/docs/building/store-build-artifacts"" rel=""noreferrer"">here</a>. An example is as follows:</li>
</ol>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
    args:
    - 'run'
    - 'deploy'
    - 'cloudrunservice'
    - '--image'
    - 'gcr.io/PROJECT_ID/IMAGE'
    - '--region'
    - 'REGION_TO_DEPLOY'
    - '--platform'
    - 'managed'
    - '--allow-unauthenticated'
artifacts:
    objects:
    location: 'gs://example-bucket'
    paths: ['*']
</code></pre>
<ol start=""3"">
<li>Finally you could run the following command:</li>
</ol>
<pre class=""lang-sh prettyprint-override""><code>gcloud builds submit --gcs-source-staging-dir=&quot;gs://example-bucket/cloudbuild-custom&quot; --config cloudbuild.yaml
</code></pre>
<p>The steps mentioned before can adapted to your script. Please give a try :) and you will see that even if the Cloud Run service is deployed in Asia, Europe or US, the bucket specified before can be in another location.</p>"
"How to deploy multiple functions from a monorepo using Cloud Build but only one at a time<p>I am trying to set up a monorepo with multiple cloud functions written in Python. I am currently using Cloud Build and structure like this:</p>

<pre><code>.
├── deployment
│   └── cloudbuild.yaml
├── main.py
└── requirements.txt
</code></pre>

<p>which with this Cloud Build YAML code deploys well:</p>

<pre><code>steps:
  - name: 'gcr.io/cloud-builders/gcloud'
    args: [
      'functions', 'deploy', '$_FUNCTION_NAME',
      '--trigger-resource', '$_TRIGGER_RESOURCE',
      '--trigger-event', '$_TRIGGER_EVENT',
      '--runtime', 'python37',
      '--memory', '1024MB',
      '--region', 'europe-west1'
    ]
</code></pre>

<p>Now my intent is to move towards this structure:</p>

<pre><code>.
├── first_function
│   ├── main.py
│   └── requirements.txt
├── second_function
│   ├── main.py
│   └── requirements.txt
└── cloudbuild.yaml
</code></pre>

<p>With triggers set up to watch changes within the respective subfolders, injecting the function name as env variable and deploying the correct function. This is the TF idea of setup:</p>

<pre><code>resource ""google_cloudbuild_trigger"" ""first_function_trigger"" {
  project = google_project.my_project.name
  name = ""trigger-first-function""
  description = ""Trigger for deploying first function""

  trigger_template {
    repo_name = google_sourcerepo_repository.functions.name
    branch_name = ""master""
    dir = ""first_function/**""
  }

  substitutions = {
    _TRIGGER_RESOURCE = google_storage_bucket.my_bucket.name
    _TRIGGER_EVENT = ""google.storage.object.finalize""
    _FUNCTION_NAME = ""first_function""
  }

  filename = ""cloudbuild.yaml""
}
</code></pre>

<p>However, here is the catch:</p>

<p>All arrangements, with specifying <code>--source</code> in the <code>gcloud functions deploy</code> command, just keep giving me errors such as:</p>

<blockquote>
  <p>ERROR: (gcloud.functions.deploy) argument <code>--source</code>: Provided directory does not exist</p>
</blockquote>

<p>This error occurrs when I try these values:</p>

<pre><code>1. --source=.
2. --source=./first_function
3. --source=./first_function/
</code></pre>

<p>Number three works locally when <code>gcloud functions deploy</code> is called from the root folder. I read about the approach of specifying the repository in GCP - but that's an extra data loading operation, no? The source code is already there - this is a trigger for changes in the repository.</p>

<p>When no <code>--source</code> is defined, this is the error I get:</p>

<blockquote>
  <p>ERROR: (gcloud.functions.deploy) OperationError: code=3, message=Build failed: Build error details not available</p>
</blockquote>

<p>I know that Cloud Build is a fairly young service and changes very rapidly, but is there a way now to arrange the folders or set up the cloud build YAML so that the functions get deployed correctly? I really don't want to create a separate repository for every single 100-line function.</p>","<p>I was unable to reproduce your issue with just Cloud Functions + Cloud Build. With the following structure:</p>

<pre><code>.
├── cloudbuild.yaml
├── first_function
│   ├── main.py
│   └── requirements.txt
└── second_function
    ├── main.py
    └── requirements.txt
</code></pre>

<p>And the following <code>cloudbuild.yaml</code>:</p>

<pre><code>steps:
  - name: 'gcr.io/cloud-builders/gcloud'
    args: [
      'functions', 'deploy', 'first_function',
      '--trigger-http',
      '--runtime', 'python37',
      '--region', 'us-central1',
      '--source', 'first_function'
    ]
  - name: 'gcr.io/cloud-builders/gcloud'
    args: [
      'functions', 'deploy', 'second_function',
      '--trigger-http',
      '--runtime', 'python37',
      '--region', 'us-central1',
      '--source', 'second_function'
    ]
</code></pre>

<p>I was able to deploy both functions.</p>

<p>Is it possible the <code>source</code> flag is not being set correctly?</p>"
"What does stop streaming builds ans start streaming builds does in google cloud build?<p>In build history I'm seeing a new button saying stop streaming builds, what does it mean in google cloud build<a href=""https://i.stack.imgur.com/xXzrq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xXzrq.png"" alt=""image""></a> </p>","<p>Google seems to be pushing a new UI.
Monday I was getting it every so often, yesterday I got it all day.
The ""Stop Streaming"" stops the page from auto updating.</p>

<p>The old UI, had a ""refresh"" button as new builds would not show automatically </p>"
"How to set list with strings as a yaml value while preserving quotes?<p>I have code like this:</p>

<pre><code>import ruamel.yaml
from ruamel.yaml.scalarstring import DoubleQuotedScalarString as dq    
yaml = ruamel.yaml.YAML()
yaml.indent(sequence=2)
yaml.preserve_quotes = True
yaml.default_flow_style=None

CF2_cloudbuild = {
            'steps':[
            {'name': dq(""gcr.io/cloud-builders/gcloud""),
            'args': [""functions"", ""deploy"", ""publish_resized""],
            'timeout': dq(""1600s"")}
            ]
            }

with open(""file.yaml"", 'w') as fp:
    yaml.dump(CF2_cloudbuild, fp)
</code></pre>

<p>and this is the content of <code>file.yaml</code>:</p>

<pre><code>steps:
- name: ""gcr.io/cloud-builders/gcloud""
  args: [functions, deploy, publish_resized]
  timeout: ""1600s""
</code></pre>

<p>and I need:</p>

<pre><code>steps:
- name: ""gcr.io/cloud-builders/gcloud""
  args: [""functions"", ""deploy"", ""publish_resized""]
  timeout: ""1600s""
</code></pre>

<p>in order to get format compliant with the GCP documentation concerning build configuration files <a href=""https://cloud.google.com/cloud-build/docs/build-config"" rel=""nofollow noreferrer"">GCP Build configuration overview docs</a></p>

<p>How to obtain that?<br>
When I try to use <code>[dq(""functions""), dq(""deploy""), dq(""publish_resized"")]</code> functionality I get:</p>

<pre><code>steps:
- name: ""gcr.io/cloud-builders/gcloud""
  args:
  - ""functions""
  - ""deploy""
  - ""publish_resized""
  timeout: ""1600s""
</code></pre>

<p>which I think is not the same as <code>[""functions"", ""deploy"", ""publish_resized""]</code>.</p>","<p>As @Stephen Rauch indicates, the two outputs are equivalent, the one you ""need"" has a sequence in flow style and the one
you get is a sequence in block style. Any YAML parser should load that in the same way. And if you don't explicitly add the double quotes, <code>ruamel.yaml</code> will add them if they are needed (e.g. to prevent the string <code>true</code> from being loaded as a boolean).</p>

<p>But since you set <code>.default_flow_style</code> you are right to expect a leaf node in the YAML output to be flow-style, and you might have hit on a bug in <code>ruamel.yaml</code>'s round-tripdumper.</p>

<p>When ruamel.yaml loads your expected output, then it preserves</p>

<pre><code>import sys
import ruamel.yaml

yaml_str = """"""
steps:
- name: ""gcr.io/cloud-builders/gcloud""
  args: [""functions"", ""deploy"", ""publish_resized""]
  timeout: ""1600s""
""""""

yaml = ruamel.yaml.YAML()
yaml.preserve_quotes = True

data = yaml.load(yaml_str)
yaml.dump(data, sys.stdout)
</code></pre>

<p>which gives:</p>

<pre><code>steps:
- name: ""gcr.io/cloud-builders/gcloud""
  args: [""functions"", ""deploy"", ""publish_resized""]
  timeout: ""1600s""
</code></pre>

<p>This is because the mapping and sequence nodes are not loaded as <code>dict</code> resp.
<code>list</code>, butsubclasses thereof, that keep information about their original
flow/block style.</p>

<p>You can emulate this by constructing that subclass for your list:</p>

<pre><code>from ruamel.yaml.scalarstring import DoubleQuotedScalarString as dq
from ruamel.yaml.comments import CommentedSeq

def cs(*elements):
     res = CommentedSeq(*elements)
     res.fa.set_flow_style()
     return res


CF2_cloudbuild = {
            'steps':[
            {'name': dq(""gcr.io/cloud-builders/gcloud""),
            'args': cs(dq(l) for l in [""functions"", ""deploy"", ""publish_resized""]),
            'timeout': dq(""1600s"")}
            ]
            }

yaml.dump(CF2_cloudbuild, sys.stdout)
</code></pre>

<p>which gives:</p>

<pre><code>steps:
- name: ""gcr.io/cloud-builders/gcloud""
  args: [""functions"", ""deploy"", ""publish_resized""]
  timeout: ""1600s""
</code></pre>

<p>But, again, if the YAML parser that cloudbuilder software uses is conformant,
neither the flow style, nor any of the double quotes are necessary in your
example. And you can rely on ruamel.yaml on adding the latter if they are
necessary.</p>"
"How do I trigger/call Cloud Build from a Python Cloud Function<p>I want to call Cloud Build from a Cloud Function written in Python 3, and pass my steps directly to be executed. I'd rather not have to roll a http request and do all the low level auth myself like <a href=""https://stackoverflow.com/questions/51915171/how-to-run-a-google-cloud-build-trigger-via-cli-rest-api-cloud-functions"">here</a>.</p>

<p>Is there some sort of client library for Python I can use to make it easier?</p>","<p>There's a new Python client library for Cloud Build that it's in <strong>alpha</strong>. See <a href=""https://github.com/googleapis/python-cloudbuild"" rel=""nofollow noreferrer"">here</a>. Under the covers, it's calling <a href=""https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.builds/create"" rel=""nofollow noreferrer"">this API</a>. You can simply do the following in your Cloud Function to call Cloud Build:</p>

<pre><code>from google.cloud.devtools import cloudbuild_v1

def trigger_cloud_build(request):
    client = cloudbuild_v1.CloudBuildClient()
    project_id = 'YOUR_PROJECT_ID'
    build = {'steps': [{'name': 'gcr.io/cloud-builders/docker',
         'args': ['version'], 'id': 'Docker Version'}]}
    response = client.create_build(project_id, build)
    print(response)
</code></pre>

<p>requirements.txt should include <code>google-cloud-build</code>.</p>

<p><a href=""https://i.stack.imgur.com/kpqaR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kpqaR.png"" alt=""enter image description here""></a></p>"
"Google Cloud Build deploy specific/current image tag<p>I have the following Google Cloud Build pipeline:</p>

<pre><code># gcloud builds submit --config cloud-build/cloudbuild.yaml --substitutions=_GIT_USER=""&lt;your user&gt;"",_GIT_PASS=""&lt;your password here&gt;,_GIT_TAG=""&lt;tag name&gt;""
steps:
# Git
- name: 'gcr.io/cloud-builders/git'
  args: ['clone', 'https://${_GIT_USER}:${_GIT_PASS}@bitbucket.my-company.com/scm/my-project/my-app.git',
         '--branch', '${_GIT_TAG}', '--single-branch']
# Build
- name: 'gcr.io/cloud-builders/mvn'
  args: ['package', '-DskipTests=true']
  dir: my-app/backend
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', 
        '--no-cache', 
        '-t', 'gcr.io/$PROJECT_ID/my-app-test:latest', 
        '-f', './cloud-build/Dockerfile-backend', 
        '--build-arg', 'JAR_FILE=./my-app/backend/target/my-app-0.0.1-SNAPSHOT.jar', 
        '.']
- name: ""gcr.io/cloud-builders/docker""
  args: [""push"", ""gcr.io/$PROJECT_ID/my-app-test:latest""]
# Deploy
# The Deploy step requires the role 'Kubernetes Engine Developer' for the service account `&lt;project_number&gt;@cloudbuild.gserviceaccount.com`
- name: 'gcr.io/cloud-builders/kubectl'
  id: Deploy
  args:
  - 'apply'
  - '-f'
  - 'cloud-build/deployment-backend.yaml'
  env:
  - 'CLOUDSDK_COMPUTE_ZONE=${_K8S_COMPUTE_ZONE}'
  - 'CLOUDSDK_CONTAINER_CLUSTER=${_K8S_CLUSTER}'
substitutions:
    _K8S_COMPUTE_ZONE: us-central1-a
    _K8S_CLUSTER: my-cluster-1
    _GIT_USER: my-git-user
    _GIT_PASS: replace-me-in-cloudbuild-file # default value
</code></pre>

<p>deployment-backend.yaml</p>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-backend
  template:
    metadata:
      labels:
        app: my-backend
    spec:
      containers:
      - name: my-backend
        image: gcr.io/&lt;my-project&gt;/my-app-test:latest
...
</code></pre>

<p>The problem is that in step 3 I have to build the image as <code>my-app-test:latest</code> so I can use again the latest image in the <code>deployment.yaml</code> (<code>image: gcr.io/&lt;my-project&gt;/my-app-test:latest</code>)
I would like to be able to use the tag name for the image tag like this:</p>

<p>step 3:</p>

<pre><code>- name: 'gcr.io/cloud-builders/docker'
  args: ['build', 
        '--no-cache', 
        '-t', 'gcr.io/$PROJECT_ID/my-app-test:${_GIT_TAG}', 
        '-f', './cloud-build/Dockerfile-backend', 
        '--build-arg', 'JAR_FILE=./my-app/backend/target/my-app-0.0.1-SNAPSHOT.jar', 
        '.']
</code></pre>

<p>but in that case what is the best way to tell the Deployment step to use the image named after the tag that is used?</p>

<p>I've found that Kustomize is the idiomatic way to ""parameterize"" kubernetes, but I still have to know the image name upfront and store it in a file.</p>

<p>Replacing the image tag with <code>sed</code> might work, but does not seem like a good solution. </p>","<p>Is there a reason you’re reapplying they deployment? Or are you doing that just one time? you could just use the built in command to replace / update the image instead of reapplying the config (if that’s what you’re doing)</p>

<p><code>kubectl set image deployment/my-deployment mycontainer=myimage</code></p>

<p>Or the other way is like you said, just use sed. (Basically what kustomize does) 
Bash into kubectl and then </p>

<p><code>cat deploy-file | sed “/latest/${_TAG_NAME}/“ | kubectl.bash apply -f -</code></p>"
"Google Cloud Build sub builds<p>Is possible have multiple cloudbuild.yaml files per subdirectory?</p>

<p>For example:</p>

<pre><code>my-app:
  - service1
    - cloudbuild.yaml
  - service2
    - cloudbuild.yaml
  cloudbuild.yaml
</code></pre>","<p>The answer is almost correct. This will not work, because you forgot to include ""."", which tells to upload and build the current directory. The correct way to include a sub/child cloudbuild.yaml would then be:</p>

<pre><code># Include cloudbuild sub step
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - 'builds'
  - 'submit'
  - '.'
  - '--config'
  - 'cloudbuild.yaml'
</code></pre>"
"GCP Cloud build ignores timeout settings<p>I use Cloud Build for copying the configuration file from storage and deploying the app to <code>App Engine flex</code>. 
The problem is that the build fails every time when it lasts more than 10 minutes. I've specified timeout in my cloudbuild.yaml but it looks like it's ignored. Also, I configured <code>app/cloud_build_timeout</code> and set it to 1000. Could somebody explain to me what is wrong here?</p>

<p>My <code>cloudbuild.yaml</code> looks in this way:</p>

<pre><code>steps:
  - name: gcr.io/cloud-builders/gsutil
    args: [""cp"", ""gs://myproj-dev-247118.appspot.com/.env.cloud"", "".env""]
  - name: ""gcr.io/cloud-builders/gcloud""
    args: [""app"", ""deploy""]
    timeout: 1000s
timeout: 1600s
</code></pre>

<p>My <code>app.yaml</code> use custom env that build it from Dockerfile and looks like this:</p>

<pre><code>runtime: custom
env: flex

manual_scaling:
  instances: 1
env_variables:
  NODE_ENV: dev
</code></pre>

<p>Dockerfile also contains nothing special, just installing dependencies and app building:</p>

<pre><code>FROM node:10 as front-builder
WORKDIR /app
COPY front-end .
RUN npm install
RUN npm run build:web

FROM node:12
WORKDIR /app
COPY api .
RUN npm install
RUN npm run build
COPY .env .env
EXPOSE 8080
COPY --from=front-builder /app/web-build web-build
CMD npm start
</code></pre>","<p>When running <code>gcloud app deploy</code> directly for an App Engine Flex app, from your local machine for example, under the hood it spawns a Cloud Build job to build the image that is then deployed to GAE (you can see that build in Cloud Console > Cloud Build). This build has a 10min timeout that can be customized via:</p>

<pre><code>gcloud config set app/cloud_build_timeout 1000
</code></pre>

<p>Now, the issue here is that you're issuing the <code>gcloud app deploy</code> command from within Cloud Build itself. Since each individual Cloud Build step is running in its own Docker container, you can't just add a previous step to customize the timeout since the next one will use the default <code>gcloud</code> setting.</p>

<p>You've got several options to solve this:</p>

<ul>
<li>Add a build step to first build the image with <code>docker build</code>, upload it to Google Cloud Registry. You can set a custom timeout on these steps to fit your needs. Finally, deploy your app with <code>glcoud app deploy --image-url=IMAGE-URL</code>.</li>
<li>Create your own <a href=""https://cloud.google.com/cloud-build/docs/cloud-builders#writing_your_own_custom_builder"" rel=""nofollow noreferrer"">custom gcloud builder</a> where <code>app/cloud_build_timeout</code> is set to your custom value. You can derive it from the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/blob/master/gcloud/Dockerfile"" rel=""nofollow noreferrer"">default gcloud builder Dockerfile</a> and add <code>/builder/google-cloud-sdk/bin/gcloud config set app/cloud_build_timeout 1000</code></li>
</ul>"
"Add docker arguments to gcloud builds submit<p>As described <a href=""https://cloud.google.com/cloud-build/docs/quickstart-build"" rel=""nofollow noreferrer"">here</a>, it's possible to submit a Dockerfile build using <code>gcloud builds submit -t &lt;source&gt;</code>. However, is it possible to pass additional build args to it? I tried <code>gcloud builds sumbit -t &lt;source&gt; --build-arg=FOO=$BAR</code> but it didn't recognize the flags. The old <code>gcloud docker</code> command use to support pass-through of docker args. I don't want to use a <code>cloudbuild.yaml</code> file if I can avoid it since I'm relying on Azure DevOps for the build and don't want to clutter up my repo with more YAML pipeline files.</p>

<p>My use case is that I want to leverage google cloud build's layer caching for faster builds since Azure DevOps doesn't have that built-in.</p>",<p>You can't pass some argument to Cloud Build with the <code>-t</code> argument docker build. Only <code>cloudbuild.yaml</code> config file accepts variables</p>
"Do Firebase Cloud Functions require Billing to be enabled by April 20, 2020 because of the Cloud Build API?<ul>
<li><a href=""https://stackoverflow.com/q/60396232/6509751"">Firebase: Enable the Google Cloud Build API for your Cloud Functions project?</a> &amp; <a href=""https://stackoverflow.com/q/60382362/6509751"">understanding &quot;action required&quot; email from GCP, re: enable Cloud Build API</a> ask for something different and I understand what Cloud Build means for Firebase Functions as described in the following. I am asking about the billing requirement - if that is a thing. This is not clear from the notice.</li>
</ul>

<h2>Cloud Functions &amp; Cloud Build</h2>

<p>I have received notice via email that I need to do the following for my Firebase project:</p>

<blockquote>
  <p>[Action Required] Enable the Google Cloud Build API for your Cloud Functions project(s) before April 20, 2020</p>
</blockquote>

<p>The message further explains that <a href=""https://firebase.google.com/docs/functions/"" rel=""noreferrer"">Cloud Functions</a> will use <a href=""https://cloud.google.com/cloud-build/"" rel=""noreferrer"">Cloud Build</a>, <a href=""https://cloud.google.com/container-registry/"" rel=""noreferrer"">Container Registry</a>, and <a href=""https://cloud.google.com/storage/"" rel=""noreferrer"">Cloud Storage</a> in the future in order to deploy code.</p>

<h2>Billing</h2>

<p>Having received the notice, I followed the described steps:</p>

<blockquote>
  <ol>
  <li>Access the <a href=""https://www.google.com/appserve/mkt/p/AFnwnKWJL2Fzogr3jlEtsWoYocghZBRV5ebzFS68U_0Og3FiUJNeYgUIyNQemAAfApOkDE7GnUMopHoawR7i144wU21W6OCVXikFmnCBw2kX41iY86QBbBSQ0Y0x9ZMayut8VSRQqHvGzsfqte1OXLEkGoy_R6N_hO2VyzjAEyEfqkw965i13D7oqJoK50hLrgiWhEc0FenBDjXPddseWml8QKXijvA-eVbk7c8s12foaU-mFw"" rel=""noreferrer"">Cloud Build API</a> link in the Cloud Console.</li>
  <li>Select your project from the drop-down menu.</li>
  <li>Click <strong>Continue</strong> to enable the Cloud Build API for the project. A box will appear in the lower-left corner to confirm the API is enabled.</li>
  </ol>
</blockquote>

<p>I visited the link and followed the steps and it fails: <code>This API cannot be enabled at the moment. You may lack appropriate permissions.</code>
Using the ""Enable"" button at <a href=""https://console.cloud.google.com/marketplace/details/google/cloudbuild.googleapis.com"" rel=""noreferrer"">Cloud Build in the Google Cloud Console</a> for my Firebase project shows me:</p>

<blockquote>
  <h2>Billing required</h2>
</blockquote>

<hr>

<p>Does this mean that Cloud Functions for Firebase now <em>require billing to be enabled</em>, i.e. the <strong>Blaze plan</strong>?</p>","<p>The answer is <strong>yes</strong> (confirmed by support) and no.</p>

<hr>

<p>If you want to deploy Cloud Functions after April 20, <em>you will have to set up a <strong>billing account</strong> in the GCP console</em>. This is because you need to pay extra for Cloud Build when exceeding the free quota.</p>

<hr>

<p>However, you do <em>not</em> need to select the <em>Blaze plan</em> in Firebase.</p>"
"Firebase CLI error in Google Cloud Build step<p>When I am running the ""firebase use"" command from the Firebase CLI in a cloudbuild step I get the following error:</p>

<pre><code>Starting Step #5 - ""SetFirebaseEnvironment""
Step #5 - ""SetFirebaseEnvironment"": Already have image (with digest): eu.gcr.io/BUCKET_NAME/firebase:7.12-0
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.806Z] ----------------------------------------------------------------------
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.808Z] Command:       /usr/local/bin/node /usr/local/bin/firebase use dev --debug
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.808Z] CLI Version:   7.12.1
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.808Z] Platform:      linux
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.808Z] Node Version:  v13.6.0
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.813Z] Time:          Mon Feb 17 2020 13:09:50 GMT+0000 (Coordinated Universal Time)
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.813Z] ----------------------------------------------------------------------
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.813Z] 
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.820Z] &gt; command requires scopes: [""email"",""openid"",""https://www.googleapis.com/auth/cloudplatformprojects.readonly"",""https://www.googleapis.com/auth/firebase"",""https://www.googleapis.com/auth/cloud-platform""]
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.820Z] &gt; attempting to authenticate via app default credentials
Step #5 - ""SetFirebaseEnvironment"": [2020-02-17T13:09:50.854Z] TypeError: Cannot create property 'refresh_token' on string 'Not Found
Step #5 - ""SetFirebaseEnvironment"": '
Step #5 - ""SetFirebaseEnvironment"":     at /usr/local/lib/node_modules/firebase-tools/node_modules/google-auto-auth/node_modules/google-auth-library/lib/auth/oauth2client.js:208:28
Step #5 - ""SetFirebaseEnvironment"":     at /usr/local/lib/node_modules/firebase-tools/node_modules/google-auto-auth/node_modules/google-auth-library/lib/auth/computeclient.js:85:7
Step #5 - ""SetFirebaseEnvironment"":     at Request._callback (/usr/local/lib/node_modules/firebase-tools/node_modules/google-auto-auth/node_modules/google-auth-library/lib/transporters.js:106:7)
Step #5 - ""SetFirebaseEnvironment"":     at Request.self.callback (/usr/local/lib/node_modules/firebase-tools/node_modules/request/request.js:185:22)
Step #5 - ""SetFirebaseEnvironment"":     at Request.emit (events.js:321:20)
Step #5 - ""SetFirebaseEnvironment"":     at Request.EventEmitter.emit (domain.js:485:12)
Step #5 - ""SetFirebaseEnvironment"":     at Request.&lt;anonymous&gt; (/usr/local/lib/node_modules/firebase-tools/node_modules/request/request.js:1154:10)
Step #5 - ""SetFirebaseEnvironment"":     at Request.emit (events.js:321:20)
Step #5 - ""SetFirebaseEnvironment"":     at Request.EventEmitter.emit (domain.js:485:12)
Step #5 - ""SetFirebaseEnvironment"":     at IncomingMessage.&lt;anonymous&gt; (/usr/local/lib/node_modules/firebase-tools/node_modules/request/request.js:1076:12)
Step #5 - ""SetFirebaseEnvironment"":     at Object.onceWrapper (events.js:427:28)
Step #5 - ""SetFirebaseEnvironment"":     at IncomingMessage.emit (events.js:333:22)
Step #5 - ""SetFirebaseEnvironment"":     at IncomingMessage.EventEmitter.emit (domain.js:485:12)
Step #5 - ""SetFirebaseEnvironment"":     at endReadableNT (_stream_readable.js:1220:12)
Step #5 - ""SetFirebaseEnvironment"":     at processTicksAndRejections (internal/process/task_queues.js:84:21)
Step #5 - ""SetFirebaseEnvironment"": 
Step #5 - ""SetFirebaseEnvironment"": Error: An unexpected error has occurred.
Finished Step #5 - ""SetFirebaseEnvironment""
2020/02/17 14:09:51 Step Step #5 - ""SetFirebaseEnvironment"" finished
2020/02/17 14:09:51 status changed to ""ERROR""
ERROR
ERROR: build step 5 ""eu.gcr.io/BUCKET_NAME/firebase:7.12-0"" failed: exit status 2
2020/02/17 14:09:54 Build finished with ERROR status
</code></pre>

<p>When I run the same command on my local machine with exactly the same Firebase CLI version and Node version I dont get this error. Why do I get this error and how do a solve it?</p>","<p>It seems as it's fixed now (see comments to original question with links), but generally, the logic to fix such an issue is to try rebuilding firebase docker image used in cloud builder step.</p>"
"Dependencies (requirements.txt) not installing correctly when deploying to Google App Engine using Google Cloud Build<p>I am experiencing a strange issue with dependencies when deploying my application to Google App Engine (Python 2.7).</p>

<p>I have specified my dependencies in the requirements.txt file:</p>

<pre><code>Flask==1.0.2
werkzeug&gt;=0.14
flask-cors
twilio
httplib2
gunicorn
Jinja2
google-cloud-ndb
exponent_server_sdk
</code></pre>

<p>These work fine with the local development server when installed locally with pip from the requirements.txt folder.</p>

<p>However, when I deploy to app engine, it appears that the module exponent_server_sdk has not been installed (ImportError: No module named exponent_server_sdk).</p>

<p>I normally deploy by pushing to a git repository which is then deployed with Google Cloud Build (""gcr.io/cloud-builders/gcloud""). There are no errors when I deploy and other dependencies in the requirements.txt file work fine such as twilio or Jinja2.</p>

<p>Whilst investigating this I tried pushing directly from my development machine with gcloud app deploy. When I do this, for some strange reason, exponent_server_sdk is available and works correctly.</p>

<p>This behaviour seems very strange to me and I can't find any documentation of people experiencing similar errors. I wonder if anyone can give me any guidance about what might be causing this issue or places I can look to find errors (for example logs of the process of installing the requirements.txt file during deployment/instance startup).</p>

<p>In response to the comments:</p>

<p>I am running in the standard environment. </p>

<p>My cloudbuild.yaml looks like this:</p>

<pre><code>steps:
- name: ""gcr.io/cloud-builders/gcloud""
  args: [""app"", ""deploy"", ""app.yaml"", ""dispatch.yaml"", ""service_1.yaml"", ""service_2.yaml"", ""service_3.yaml"", ""index.yaml""]
timeout: ""1600s""
</code></pre>

<p>The .yaml file for the service in which I am experiencing the error looks like this:</p>

<pre><code>runtime: python27
api_version: 1
threadsafe: true
service: notifications

libraries:
- name: ssl
  version: latest

handlers:
- url: /.*
  script: notifications.app
</code></pre>","<p>For the (1st gen) standard environment the deployment process does not use a <code>requirements.txt</code> file, the libraries should be installed inside your app. From <a href=""https://cloud.google.com/appengine/docs/standard/python/tools/using-libraries-python-27#copying_a_third-party_library"" rel=""nofollow noreferrer"">Copying a third-party library</a>:</p>

<blockquote>
  <p>Use <a href=""https://pypi.python.org/pypi/pip"" rel=""nofollow noreferrer"">pip</a> (version 6 or later) with the <code>-t &lt;directory&gt;</code> flag to
  copy the libraries into the folder you created in the previous step.
  For example:</p>

<pre><code>pip install -t lib/ &lt;library_name&gt;
</code></pre>
</blockquote>

<p>Yes, if you want you can use a <code>requirements.txt</code> file to do the installation inside your app, but that file itself is not used by the deployment process.</p>

<p>Since you're not deploying directly from the environment in which you had the libraries installed you'd have to add the installation directory to the git repo as well.</p>"
"Why is it recommended to use a cloud builder for yarn?<p>Looking at the source code for the yarn builder for Google Cloud Build I was wondering why it is recommended to use the builder rather than specifying the entrypoint.</p>

<p><a href=""https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/yarn"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/yarn</a></p>

<p>Basically</p>

<pre><code>steps
  - name: 'gcr.io/cloud-builders/yarn'
    args:
      - install
</code></pre>

<p>vs </p>

<pre><code>steps:
  - name: node:10
    entrypoint: yarn
    args:
      - install
</code></pre>

<p>Is it because the cloud builder is registered with the Google Cloud Container Registry which is faster to read from within Google Cloud build?</p>","<p>Yes, you are corrected. Indeed, it's recommended because the read from the Container Registry will be faster to be done, using the builder. </p>

<p>As per the code indicates, you referencing directly the Container from yarn, which will make the access faster than using an entrypoint.</p>

<p>Let me know if the information helped you!</p>"
"How to pass date arg to my cloudbuild yaml<p>My <code>cloudbuild.yaml</code> exists of the following:</p>

<pre><code>- name: 'gcr.io/cloud-builders/gsutil'
  args: ['-m', 'cp', '-r', '/workspace/api-testing/target/cucumber-html-reports', 'gs://testing-reports/$BUILD_ID']

- name: 'gcr.io/cloud-builders/gsutil'
   args: ['-m', 'rm', '-r', 'gs://studio360-testing-reports/latest']

- name: 'gcr.io/cloud-builders/gsutil'
  args: ['-m', 'cp', '-r', '/workspace/api-testing/target/cucumber-html-reports', 'gs://testing-reports/latest']
</code></pre>

<p>This way I always have my latest report seperated from the older ones. But can I pass a {date} arg or something into my first line so I can have a visual order of all the older reports? </p>

<p>(Because there is no way to rank the files by last modified in the gcp storage/bucket)</p>

<p>Thanks</p>","<p>Change the first block to this:</p>

<p><code>- name: 'gcr.io/cloud-builders/gsutil'
  args: ['-m', 'cp', '-r', '/workspace/api-testing/target/cucumber-html-reports', 'gs://testing-reports/${_DATE}_$BUILD_ID']
</code></p>

<p>Then run this:</p>

<p><code>gcloud builds submit . --substitutions _DATE=$(date +%F_%H:%M:%S)</code></p>

<p>Then you would have something like this in the bucket:</p>

<p><code>gs://testing-reports/2020-02-13_14:01:40_8a6a7ed0-62e0-43ed-8f97-aa6eca9c2834</code></p>

<p>Explanation <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values#using_user-defined_substitutions"" rel=""nofollow noreferrer"">here</a> and <a href=""https://cloud.google.com/sdk/gcloud/reference/builds/submit#--substitutions"" rel=""nofollow noreferrer"">here</a>.</p>

<hr>

<p><strong>EDIT:</strong></p>

<hr>

<p>For automatic builds started by Cloud Build triggers, use this <code>cloudbuild.yaml</code>:</p>

<pre><code>steps:
- name: 'gcr.io/cloud-builders/gsutil'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    gsutil -m cp -r $FILENAME gs://$BUCKET/$FILENAME-$(date +%F_%H:%M:%S)-$BUILD_ID
</code></pre>

<p>This allows the builder to use <code>bash</code> to execute <code>gsutil</code>, so the <code>bash</code> command ""<code>date</code>"" can be used inside the command.</p>

<p>Good explanation of the syntax by Googler <a href=""https://medium.com/@davidstanke/mastering-google-cloud-build-config-syntax-8c3024607daf"" rel=""nofollow noreferrer"">here</a>, and info about entrypoint <a href=""https://cloud.google.com/cloud-build/docs/build-config#entrypoint"" rel=""nofollow noreferrer"">here</a>.</p>

<hr>"
"Google Cloud Build - How to Cache Bazel?<p>I recently started using Cloud Build with Bazel.
So I have a basic <code>cloudbuild.yaml</code></p>

<pre><code>steps:
  - id: 'run unit tests'
    name: gcr.io/cloud-builders/bazel
    args: ['test', '//...']
</code></pre>

<p>which runs all tests of my Bazel project.</p>

<p>But as you can see from this screenshot, every build takes around 4 minutes, although I haven't touched any code which would affect my tests.</p>

<p><a href=""https://i.stack.imgur.com/Qi4P7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qi4P7.png"" alt=""build duration""></a></p>

<p>Locally running the tests for the first time takes about 1 minute. But running the tests a second time, with the help of Bazels cache, it takes only a few seconds.</p>

<p><strong>So my goal is to use the Bazel cache with Google Cloud Build</strong></p>

<h1>Update</h1>

<p>As suggested by <a href=""https://stackoverflow.com/users/4786620/thierry-falvo"">Thierry Falvo</a> I'v looked into those <a href=""https://cloud.google.com/cloud-build/docs/speeding-up-builds?hl=en"" rel=""nofollow noreferrer"">recommendations</a>. An thus I tried to the add the following to my <code>cloudbuild.yaml</code>:</p>

<pre><code>steps:
  - name: gcr.io/cloud-builders/gsutil
    args: ['cp', 'gs://cents-ideas-build-cache/bazel-bin', 'bazel-bin']

  - id: 'run unit tests'
    name: gcr.io/cloud-builders/bazel
    args: ['test', '//...']

  - name: gcr.io/cloud-builders/gsutil
    args: ['cp', 'bazel-bin', 'gs://cents-ideas-build-cache/bazel-bin']
</code></pre>

<p>Although I created the bucket and folder, I get this error:</p>

<pre><code>CommandException: No URLs matched
</code></pre>

<p><a href=""https://i.stack.imgur.com/2N5j2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2N5j2.png"" alt=""cloud build error""></a></p>","<p>I think that rather than cache discrete results (artifacts), you want to use GCS (cloud storage) as a <a href=""https://github.com/bazelbuild/bazel/blob/master/site/docs/remote-caching.md#user-content-google-cloud-storage"" rel=""noreferrer"">bazel remote cache</a>.</p>

<pre><code>- name: gcr.io/cloud-builders/bazel
  args: ['test', '--remote_cache=https://storage.googleapis.com/&lt;bucketname&gt;', '--google_default_credentials', '--test_output=errors', '//...']
</code></pre>"
"Cloud Build kubectl - How to Apply Output of Previous Step to Kubernetes Cluster<p>I have a simple <code>cloudbuild.yaml</code> file which runs a Bazel command. This command returns a Kubernetes configuration in form as a log output.</p>

<p>My goal is to take the output of the first step and apply it to my Kubernetes cluster.</p>

<pre><code>steps:
  - name: gcr.io/cloud-builders/bazel
    args: [""run"", ""//:kubernetes""]

  - name: ""gcr.io/cloud-builders/kubectl""
    args: [""apply"", ""&lt;log output of previous step&gt;""]
    env:
      - ""CLOUDSDK_COMPUTE_ZONE=europe-west3-a""
      - ""CLOUDSDK_CONTAINER_CLUSTER=cents-ideas""
</code></pre>

<h1>Update</h1>

<p>I've tried the following:</p>

<pre><code>- name: gcr.io/cloud-builders/bazel
  entrypoint: /bin/bash
  args:
    [
      ""bazel"",
      ""run"",
      ""//:kubernetes"",
      "" &gt; kubernetes.yaml"",
    ]

- name: ""gcr.io/cloud-builders/kubectl""
  args: [""apply"", ""-f"", ""kubernetes.yaml""]
  env:
    - ""CLOUDSDK_COMPUTE_ZONE=europe-west3-a""
    - ""CLOUDSDK_CONTAINER_CLUSTER=cents-ideas""
</code></pre>

<p>But then I get this error:</p>

<pre><code>Running: kubectl apply -f kubernetes.yaml
error: the path ""kubernetes.yaml"" does not exist
</code></pre>","<p>As everyone already suggested here use <a href=""https://cloud.google.com/cloud-build/docs/build-config#volumes"" rel=""nofollow noreferrer"">volumes</a>.</p>

<p>Tweak your <code>cloudbuild.yaml</code> file like this:</p>

<pre><code>- name: gcr.io/cloud-builders/bazel
  entrypoint: /bin/bash
  args:
    [
      ""bazel"",
      ""run"",
      ""//:kubernetes"",
      "" &gt; /workspace/kubernetes.yaml"",
    ]

- name: ""gcr.io/cloud-builders/kubectl""
  args: [""apply"", ""-f"", ""/workspace/kubernetes.yaml""]
  env:
    - ""CLOUDSDK_COMPUTE_ZONE=europe-west3-a""
    - ""CLOUDSDK_CONTAINER_CLUSTER=cents-ideas""
</code></pre>"
"Issue connecting to Database during Google Cloud Build process<p>Similar to <a href=""https://stackoverflow.com/questions/52352103/run-node-js-database-migrations-on-google-cloud-sql-during-google-cloud-build"">this issue.</a> (I will describe the link in more detail below)</p>

<p>Problem:
When running my Google Cloud Build I get an error stating:</p>

<pre><code>django.db.utils.OperationalError: could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket ""/cloudsql/sample-kubernetes-268320:us-west1:cloud-build-staging/.s.PGSQL.3306""
</code></pre>

<p>I am following the solution <a href=""https://github.com/GoogleCloudPlatform/cloudsql-proxy/issues/317#issuecomment-543392502"" rel=""nofollow noreferrer"">posted here</a> by collaborator. They provide a sample cloudbuild.yaml that I followed closely without any luck.</p>

<p>Working cloudbuild.yaml</p>

<pre><code>steps:
  - id: proxy-install
    name: alpine:3.10
    entrypoint: sh
    args:
      - -c
      - 'wget -O /workspace/cloud_sql_proxy https://storage.googleapis.com/cloudsql-proxy/v1.16/cloud_sql_proxy.linux.386 &amp;&amp;  chmod +x /workspace/cloud_sql_proxy'
    waitFor: ['-']
  - id: execute-with-proxy
    name: python:3.7
    timeout: 100s
    entrypoint: sh
    args:
      - -c
      - '(/workspace/cloud_sql_proxy -dir=/workspace -instances=[INSTANCE_CONNECTION_NAME] &amp; sleep 2) &amp;&amp; (pip install -r requirements.txt &amp;&amp; python test_sql.py)'
    waitFor: ['proxy-install']
</code></pre>

<p>My cloudbuild.yaml</p>

<pre><code>steps:
  - id: proxy-install
    name: alpine:3.10
    entrypoint: sh
    args:
      - -c
      - 'wget -O /workspace/cloud_sql_proxy https://storage.googleapis.com/cloudsql-proxy/v1.16/cloud_sql_proxy.linux.386 &amp;&amp;  chmod +x /workspace/cloud_sql_proxy'
    waitFor: ['-']
  - id: Test
    name: 'python:3.7.6-buster'
    env:
      - ""CLOUDBUILD_TEST=True""
    timeout: 100s
    entrypoint: /bin/sh
    args:
      - -c
      - '(/workspace/cloud_sql_proxy -dir=/workspace -instances=sample-kubernetes-268320:us-west1:cloud-build-staging &amp; sleep 2) &amp;&amp; (pip install -r requirements.txt &amp;&amp; cd fastestfollowup &amp;&amp; python3 manage.py test)'
    waitFor: ['proxy-install']
</code></pre>

<p>Steps I've taken to debug this:</p>

<ol>
<li>I have added the Cloud Build Service as the Cloud SQL Admin</li>
<li>I have ensured my instance name is correct through using <code>gcloud sqlinstances describe cloud-build-staging</code> and copying the ""connectionname""</li>
</ol>

<p>Edit:
I changed my cloudbuild.yaml file from</p>

<pre><code>    args:
      - -c
      - '(/workspace/cloud_sql_proxy -dir=/workspace -instances=sample-kubernetes-268320:us-west1:cloud-build-staging &amp; sleep 2) &amp;&amp; (pip install -r requirements.txt &amp;&amp; cd fastestfollowup &amp;&amp; python3 manage.py test)'
</code></pre>

<p>to:</p>

<pre><code>    args:
      - -c
      - '(/workspace/cloud_sql_proxy -dir=/cloudsql -instances=sample-kubernetes-268320:us-west1:cloud-build-staging &amp; sleep 2) &amp;&amp; (pip install -r requirements.txt &amp;&amp; cd fastestfollowup &amp;&amp; python3 manage.py test)'
</code></pre>

<p>With no effect.</p>","<p>Looks like you started the proxy with <code>-dir=/workspace</code> but then tried to connect at <code>/cloudsql</code>. You'll need either update the proxy or update the path your app uses to connect. </p>

<p>The line:</p>

<pre><code>args:
      - -c
      - '(/workspace/cloud_sql_proxy -dir=/workspace -instances=sample-kubernetes-268320:us-west1:cloud-build-staging &amp; sleep 2) &amp;&amp; (pip install -r requirements.txt &amp;&amp; cd fastestfollowup &amp;&amp; python3 manage.py test)'
</code></pre>

<p>Should Read:</p>

<pre><code>args:
      - -c
      - '(/workspace/cloud_sql_proxy -dir=/cloudsql -instances=sample-kubernetes-268320:us-west1:cloud-build-staging &amp; sleep 2) &amp;&amp; (pip install -r requirements.txt &amp;&amp; cd fastestfollowup &amp;&amp; python3 manage.py test)'
</code></pre>

<p>Other mistakes include: </p>

<ol>
<li><p>Listening at the wrong port. In your logs you will see the port that your database listens on. The default for postgres is 5432, yours is 3306.</p></li>
<li><p>Verify your Cloud Build service has the correct permissions. Add [projectnumber]@cloudbuild.gserviceaccount.com as a ""Cloud SQL Admin"" through the IAM dashboard.</p></li>
</ol>"
"Why can't I override the timeout on my Google Cloud Build?<p>I am attempting to setup a CI Pipeline using Google Cloud Build. </p>

<p>I am attempting to deploy a MeteorJS app which has a lengthy build time - the default build timeout for GCB is 10 minutes and it was recommended <a href=""https://forums.meteor.com/t/self-contained-dockerfile-for-deploying-meteor-apps/47949/3"" rel=""nofollow noreferrer"">here</a> that I increase the timeout. </p>

<p>I have setup my <code>cloudbuild.yaml</code> file with the <code>timeout</code> option increased to 20 minutes: </p>

<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['app', 'deploy']
timeout: 1200s
</code></pre>

<p>I have a Trigger setup in GCB connected to a Bitbucket Repo and when I push a change and the Trigger fires, I get 2 new builds - one coming from Bitbucket and one whose source is <code>Google Cloud Storage</code>. </p>

<p>Once 10 minutes of build time has elapsed, the build from Cloud Storage will timeout which will cause the Bitbucket build to fail as well with <code>Error Response: [4] DEADLINE_EXCEEDED</code></p>

<p>Occasionally, for whatever reason, the Cloud Storage build will finish in under 10 minutes which will allow the Bitbucket build to finish successfully and deploy. </p>

<p>If I attempt to cancel/stop the Cloud Storage build, it will also stop the Bitbucket build. </p>

<p>The screenshot below shows 2 attempts of the exact same build with differing results. </p>

<p><a href=""https://i.stack.imgur.com/Et1Sw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Et1Sw.png"" alt=""enter image description here""></a></p>

<p>I do not understand where this second Cloud Storage Build is coming from, but it does not seem to be affected by the settings in my yaml file or my global GCP settings. </p>

<p>I have attempted to run the following commands from the <code>gcloud</code> CLI: </p>

<pre><code>gcloud config set app/cloud_build_timeout 1200
gcloud config set builds/timeout 1200
gcloud config set container/build-timeout 1200
</code></pre>

<p>I have also attempted to use a high CPU build machine to speed up the process but it did not seem to have any effect. </p>

<p>Any insight would be greatly appreciated - I feel that I have exhausted every possible combination of Google Search keywords I can think up!</p>","<p>This timeout error comes from app engine deployment itself which has 10 min timeout by default.</p>
<p>You will need to update app/cloud_build_timeout property inside container itself like this:</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args: ['-c', 'gcloud config set app/cloud_build_timeout 1200 &amp;&amp; gcloud app deploy']
timeout: 1200s
</code></pre>
<h1>Update</h1>
<p>Actually simpler <a href=""https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-appengine#configuring_the_deployment"" rel=""nofollow noreferrer"">solution</a>:</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['app', 'deploy']
  timeout: 1200s
timeout: 1200s
</code></pre>"
"How to specify secretEnv to cloudbuild.yaml via gcloud cli args or environment variables<p>If I follow the cloud build <a href=""https://cloud.google.com/cloud-build/docs/securing-builds/use-encrypted-secrets-credentials?hl=ja"" rel=""nofollow noreferrer"">document</a>, I have to specify encrypted secret on cloudbuild.yaml.</p>

<pre><code>secrets:
- kmsKeyName: projects/[PROJECT-ID]/locations/global/keyRings/[KEYRING-NAME]/cryptoKeys/[KEY-NAME]
  secretEnv:
    MY_SECRET: &lt;base64-encoded encrypted secret&gt;
</code></pre>

<p>Even if it is encrypted, I don't commit secret value at code. Please tell me another way.</p>

<p>ex. via args from gcloud builds submit command or environment variables,...etc</p>","<p>You can use <a href=""https://cloud.google.com/secret-manager"" rel=""nofollow noreferrer"">Google Secret Manager</a> instead. We're still updating the documentation, but there is an example of how you can use it with Cloud Build:</p>
<p>First, create a secret:</p>
<pre><code>$ echo -n &quot;my-secret-data&quot; | gcloud beta secrets create &quot;my-api-key&quot; \
    --replication-policy &quot;automatic&quot; \
    --data-file -
</code></pre>
<p>Grant the Cloud Build Service Account permission to access your secret:</p>
<pre><code>$ gcloud beta secrets add-iam-policy-binding &quot;my-api-key&quot; \
    --member &quot;serviceAccount:&lt;project-number&gt;@cloudbuild.gserviceaccount.com&quot; \
    --role &quot;roles/secretmanager.secretAccessor&quot;
</code></pre>
<h3>Update (February 2021)</h3>
<p>Then retrieve the secret in your build steps:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: 'my-step'
  args:
  - '--secret=$$MY_SECRET'
  secretEnv:
  - 'MY_SECRET'

availableSecrets:
  secretManager:
  - env: 'MY_SECRET'
    versionName: 'projects/my-project/secrets/my-secret/versions/latest'
</code></pre>
<h3>Old answer (pre-February 2021)</h3>
<p>Then retrieve the secret in your build steps:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: 'gcr.io/cloud-builders/gcloud@sha256:c1dfa4702cae9416b28c45c9dcb7d48102043578d80bfdca57488f6179c2211b'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
       gcloud beta secrets versions access --secret=my-api-key latest &gt; /secrets/my-api-key
  volumes:
  - name: 'secrets'
    path: '/secrets'

- name: 'my-step'
  volumes:
  - name: 'secrets'
    path: '/secrets'
  args: # ... /secrets/my-api-key contains the secret
</code></pre>"
"Bazel k8s_deploy Does not Apply Kubernetes Resources to Cluster<p>I have multiple Bazel rules for deploying Kubernetes</p>

<ol>
<li><a href=""https://github.com/flolude/cents-ideas/blob/2b47e53c6f8f2b44db09c570fc9873f27b83f9a0/services/ideas/BUILD#L79"" rel=""nofollow noreferrer"">deployments</a>:</li>
</ol>

<pre><code>load(""@k8s_deploy//:defaults.bzl"", ""k8s_deploy"")
k8s_object(
  name = ""k8s_deployment"",
  kind = ""deployment"",
  cluster = ""cents-ideas"",
  template = "":ideas.deployment.yaml"",
  images = {
    ""gcr.io/cents-ideas/ideas:latest"": "":image""
  },
)
</code></pre>

<ol start=""2"">
<li><a href=""https://github.com/flolude/cents-ideas/blob/2b47e53c6f8f2b44db09c570fc9873f27b83f9a0/services/ideas/BUILD#L88"" rel=""nofollow noreferrer"">services</a>:</li>
</ol>

<pre><code>k8s_object(
  name = ""k8s_service"",
  kind = ""service"",
  template = "":ideas.service.yaml"",
)
</code></pre>

<p>And I've put all those <code>k8s_object</code> together into a single <code>k8s_objects</code> like <a href=""https://github.com/flolude/cents-ideas/blob/2b47e53c6f8f2b44db09c570fc9873f27b83f9a0/BUILD#L21"" rel=""nofollow noreferrer"">this</a>:</p>

<pre><code>load(""@io_bazel_rules_k8s//k8s:objects.bzl"", ""k8s_objects"")
k8s_objects(
    name = ""k8s"",
    objects = [
      "":k8s_deployment"",
      "":k8s_service""
    ]
)
</code></pre>

<p>Lastly, I have a <a href=""https://github.com/flolude/cents-ideas/blob/2b47e53c6f8f2b44db09c570fc9873f27b83f9a0/cloudbuild.yaml"" rel=""nofollow noreferrer""><code>cloudbuild.yaml</code></a> file which runs the <code>k8s_objects</code> rule:</p>

<pre><code>steps:
  - id: 'push docker images and deploy changes to kubernetes'
    name: gcr.io/cloud-builders/bazel
    args: ['run', '//:kubernetes']
</code></pre>

<p>But the Kubernetes resources are not applied to the cluster, although the Cloud Build succeeds:
<a href=""https://i.stack.imgur.com/Cb2Dv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cb2Dv.png"" alt=""cloud build""></a></p>

<p><a href=""https://00e9e64bac83fdc66c45609b911427a2f5c2d6ad95f35fb827-apidata.googleusercontent.com/download/storage/v1/b/64811142611.cloudbuild-logs.googleusercontent.com/o/log-5a859279-021b-4b1d-a589-06bce962f6e5.txt?qk=AD5uMEtumX9E_XLDwKbPXDw5MV98lirAnwjMyZgC3cplR0JTjrUOcZwD8kJc8Q5QVuwbVJHLyks3PRMh0uxlHq-TWpkJ5ss25PkEOBAXj5PCtp6amo0Pbxo8KIWyKKlHGec49l_jlHEUGz5Hw8ccWThpFsHzvqjd5JMAStWB60d0eUWHhiAjypyhrnetKHOI1Ibve-PhUuPmcQggp4tRTKmWBb8vjOAdMNaC9Jv-HA0dOXTUqHTaS88iCM56l-51hfzp-aW_ISaltRXrtLQU_gWD6hA9QaC_EGKQ8d9B9BqZoCERO8PwHgWEsM3fF87GTKGWZ2bTiFMVPEiCKBwDhZeX2jtcGeLdM-6aQzKYOAFSUpQ81enMhUJyFxdF-sIpojC4NnyhwDxRPe0MNr0xLWQg9pF0rrKhvBojnAX0fsbxQjTrIaRJS6jBfzwcN5NZJHr6tKK8MXkYvopNfkWaBhkVsvDr5hIr_iKeZ--xgUg9EX98bEPMaOEqRXbIZZDfIopc_qPEi-V1oacadbW5PbjNU_9JbeURqWgUTz6eglHHxcqeTSCXlJxEyanKd4BeKw8QsURzXnfIQlj99Dnq8g7Sye-21PiEVPLEApLMuETbHMBW_uoa3SEbtCNNx3_A0Ix1p5936KliCD57KFzpQkaJGjSI8MDg0eDrJHyVjo-GEdDZKJmVbLnkrez1P_NOJbI73sDWqoytwytx35LN0ypVBtr8KLF_1WQFfmu1F7B4X2sp2m2RGCHReVTvUqys4X76IKAwbWXKBl5-psitqWeAiQLGpZUDKjQGBoj8IwHYZyTNHq2MAAvhY9vByQjSqq1rg7IW5OQGEjftgsj3Oracp1cERLE9xGwHO6X6oM6NfYVJQxDUeRkxeH4f5bFoqMR7mQsaA6Fy"" rel=""nofollow noreferrer"">Here</a> is the full build log.</p>

<p>So basically the images are pushed to the container registry and then the modified <code>.yaml</code> files are logged, but <strong>not</strong> applied to my Google Kubernetes Engine cluster.</p>","<p>I think you need to suffix the target with "".create"" or "".replace"" see <a href=""https://github.com/bazelbuild/rules_k8s#create"" rel=""nofollow noreferrer"">docs</a></p>

<p>So you would want something like:</p>

<pre><code>bazel run //:k8s.create
</code></pre>

<p>where k8s is your target name.</p>"
"How to run python unit test for Google Cloud Functions using Cloud Builds?<p>I am trying to build a CI/CD pipeline for my google cloud functions.
What i have right know is, i have local developement environment with gcloud and git. 
i write my code in local environment and have cloudbuilds.yaml file. 
After writing the code i push it to Google Source Repository where i have Build Trigger. 
It  builds the function and deploy it.</p>

<p>Now i would like to have some test files with it too.That means whenever i push it to source Repository it should also run the tests and build my main.py file and then deploy it.
The cloudbuild.yaml file i have is</p>

<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions 
  - deploy
  - FunctionName
  - --runtime=python37
  - --source=.
  - --entry-point=function
  - --trigger-topic=topic_name
  - --region=europe-west3 
</code></pre>","<p>You can add a step in you Cloud Build. I don't know how your run your test, but here an example for running your script in a python3.7 context</p>

<pre><code>- name: 'python:3.7'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
       run the python script that you want
       pip install and others.
</code></pre>

<p><strong>Update</strong></p>

<p>Add this step before your deployment function. If the step fail (exit code different than 0), the cloud build process stopped and the deployment is not performed.</p>

<p><strong>Update 2</strong></p>

<p>The concept of Cloud Build is quite simple. You load a container (represented in the <code>name</code>). In the container, only the volume <code>/workspace</code> is attached and kept from one step to the next one.</p>

<p>This concept is very important. If you set environment variable or other in one step, the step after will loose this context. Only the file of <code>/workspace</code> are kept. The next step is call only if the current one finish correctly (exit code = 0).</p>

<p>When a container is loaded, a command is trigger. If you use <a href=""https://github.com/GoogleCloudPlatform/cloud-builders"" rel=""noreferrer"">cloud builder</a>, a default entry point is called by default (for example, the gcloud Cloud Builder launch automatically the gcloud command). Then you only have to add the args array to submit to this entry point. Example</p>

<pre><code>- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions 
  - list
</code></pre>

<p>This command represent the <code>gcloud functions list</code> with <code>gcloud</code> as entrypoint and <code>functions</code> and  <code>list</code> as args.</p>

<p>If your container don't have entrypoint (like python container) or if you want to override your entrypoint, you can specify it with <code>entrypoint</code> key word. In my first code example, few linux concept are required. The entrypoint is bash. the arg is <code>-c</code> for executing a command. The pipe <code>|</code> if for allowing a multi command (multi line) command entry.</p>

<p>If you have only one python command to launch, you can do like this:</p>

<pre><code>- name: 'python:3.7'
  entrypoint: 'python3'
  args:
    - 'test_main.py'
    - '.'
</code></pre>

<p>But, the steps that you wrote won't work. Why? go back to the beginning of my explanation: only the file of the <code>/workspace</code> are kept. If you perform a <code>pip3 install</code> the files aren't written in the <code>/workspace</code> directory, but elsewhere in the system. When you switch of step, you loose this system context.</p>

<p>That's why, a multi-line command is useful</p>

<pre><code>- name: 'python:3.7'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
       pip3 install -r requirements.txt
       python3 test_main.py .
</code></pre>

<p>Hope this help!</p>"
"Cloud Build Bazel Error: ""kubectl toolchain was not properly configured so apply cannot be executed""<p>I am trying to use <a href=""https://github.com/bazelbuild/rules_k8s"" rel=""nofollow noreferrer"">rules_k8s</a> for Bazel to deploy to my Kubernetes cluster.</p>

<p>Thus I have this <code>cloudbuild.yaml</code> file, which is executed by Google Cloud Build:</p>

<pre><code>steps:
  - name: gcr.io/cloud-builders/bazel
    args: ['run', '//:kubernetes.apply']
</code></pre>

<p>(<code>//:kubernetes</code> is just a <a href=""https://github.com/bazelbuild/rules_k8s#multi-object-actions"" rel=""nofollow noreferrer""><code>k8s_objects</code></a>)</p>

<hr>

<p>On my local machine running <code>bazel run //:kubernetes.apply</code> works fine, but
although the Google Cloud Build succeeds, it logs those errors. So the configuration is not applied to my Kubernetes cluster:</p>

<pre><code>Target //:kubernetes.apply up-to-date:
  bazel-bin/kubernetes.apply
INFO: Elapsed time: 29.863s, Critical Path: 0.14s
INFO: 0 processes.
INFO: Build completed successfully, 1 total action
INFO: Running command line: bazel-bin/kubernetes.apply
INFO: Build Event Protocol files produced successfully.
INFO: Build completed successfully, 1 total action
kubectl toolchain was not properly configured so k8s_deployment.apply cannot be executed.
kubectl toolchain was not properly configured so k8s_service.apply cannot be executed.
kubectl toolchain was not properly configured so projection_database_k8s_deployment.apply cannot be executed.
kubectl toolchain was not properly configured so projection_database_k8s_service.apply cannot be executed.
kubectl toolchain was not properly configured so k8s_deployment.apply cannot be executed.
kubectl toolchain was not properly configured so k8s_service.apply cannot be executed.
kubectl toolchain was not properly configured so k8s_deployment.apply cannot be executed.
kubectl toolchain was not properly configured so k8s_service.apply cannot be executed.
kubectl toolchain was not properly configured so event_store_k8s_deployment.apply cannot be executed.
kubectl toolchain was not properly configured so event_store_k8s_service.apply cannot be executed.
kubectl toolchain was not properly configured so k8s_deployment.apply cannot be executed.
kubectl toolchain was not properly configured so k8s_service.apply cannot be executed.
kubectl toolchain was not properly configured so event_store_k8s_deployment.apply cannot be executed.
kubectl toolchain was not properly configured so event_store_k8s_service.apply cannot be executed.
kubectl toolchain was not properly configured so k8s_deployment.apply cannot be executed.
kubectl toolchain was not properly configured so k8s_service.apply cannot be executed.
kubectl toolchain was not properly configured so event_store_k8s_deployment.apply cannot be executed.
kubectl toolchain was not properly configured so event_store_k8s_service.apply cannot be executed.
</code></pre>

<p>I also get a warning from the bazel cache:</p>

<pre><code>DEBUG: /builder/home/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/io_bazel_rules_k8s/toolchains/kubectl/kubectl_toolchain.bzl:28:9: No kubectl tool was found or built, executing run for rules_k8s targets might not work.
</code></pre>

<p>P.S.: I get the same errors when using <code>//:kubernetes.create</code></p>

<h1>My setup</h1>

<p><strong>Deployments</strong></p>

<pre><code>load(""@io_bazel_rules_k8s//k8s:object.bzl"", ""k8s_object"")
k8s_object(
  name = ""k8s_deployment"",
  kind = ""deployment"",
  cluster = ""gke_cents-ideas_europe-west3-a_cents-ideas"",
  template = "":ideas.deployment.yaml"",
  images = {
    ""gcr.io/cents-ideas/ideas:latest"": "":image""
  },
)
</code></pre>

<p><strong>Services</strong></p>

<pre><code>k8s_object(
  name = ""k8s_service"",
  kind = ""service"",
  cluster = ""gke_cents-ideas_europe-west3-a_cents-ideas"",
  template = "":ideas.service.yaml"",
)
</code></pre>

<p><strong>Aggregations</strong></p>

<pre><code>load(""@io_bazel_rules_k8s//k8s:objects.bzl"", ""k8s_objects"")
k8s_objects(
    name = ""k8s"",
    objects = [
      "":k8s_deployment"",
      "":k8s_service"",
    ]
)
</code></pre>

<p><strong>Final composition</strong></p>

<pre><code>k8s_objects(
    name = ""kubernetes"",
    objects = [
        ""//services/ideas:k8s"",
        # ...
    ]
)
</code></pre>

<h1>Update</h1>

<p>I've now tried to make my own docker image with Bazel and kubectl:</p>

<pre><code>FROM gcr.io/cloud-builders/bazel

RUN curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
RUN chmod +x ./kubectl
RUN mv ./kubectl /usr/local/bin/kubectl
</code></pre>

<p>I pushed it to GCR and changed my <code>cloudbuild.yaml</code> to:</p>

<pre><code>steps:
  - name: eu.gcr.io/cents-ideas/bazel-kubectl
    args: [""run"", ""//:kubernetes.apply""]
</code></pre>

<p>I firstly noticed, that the step took way longer than before. However at the end it throws an error:</p>

<pre><code>$ /usr/local/bin/kubectl --kubeconfig= --cluster=gke_cents-ideas_europe-west3-a_cents-ideas --context= --user= apply -f -
error: cluster ""gke_cents-ideas_europe-west3-a_cents-ideas"" does not exist
</code></pre>

<p><a href=""https://00e9e64bac389a379917c629a22bd56b248c258ede42cf265c-apidata.googleusercontent.com/download/storage/v1/b/64811142611.cloudbuild-logs.googleusercontent.com/o/log-bb2f4b6f-84c2-4f56-af89-15a305e536e0.txt?qk=AD5uMEu_YRlHAfSU5xwyiKmKh8lsG_Gb6O2Q114junmxLrr_VyNhbo_udKtW0yniyjEeWqRQDe-o2NohuW6dXWXmXvyvInr7nimY35qmp-KsvBdWmbCPs-eAcqz6TTk65lcEz-3CwYr7LgKFx0QCOHvZQEzMC6cDquYJNOV6oTGurHmt67gsKkf5DHLiZ7cCdGPsoucsZoByVL6Bf95bfCOch0TByIeQdnkNSVJCn-uoa7JAe3n9ge0TBh01frBLXraYxW_3T9EYsAkNA5vDkdA-UIckqXy86rloR4bruDv_bBwYdqgUkkJ1sl83BDpfT5rbTim11lfeF9a1lbvNPWXB3dO7lfjiiqPIcsDaPpM4V5dRy0InSHO7AYhoJom6chfj-MXLZfcDeuUZdqW2845BGOijkFp0d5vB_YVHp47dMAxI8TYJztiY5ktgQ7n3DzG4aynzLKTXgn6BLGpZqkz70JFW0ZMTE0Gr2afsoIXN1rZeI-BkDyZqs4MXd3m6HfSnO0o9z08IlWgNwzNUf68zTO2gk2BFA1QVvbOFFdfTl9z0wDIMdBRT7vHqgTGXF1bhjK1fWhROAxsJdOQp3qrFftLPFE2POwiFXG-wAqTYULN6usLlAcPLyJcU0VzGmNaDovZdXXzYtcdSzmCgIC6fk-_cOJiq6RsicK_co3sevGqqjL3Hehba-VxyLw930GyuwtzC6zVst3p9-RbKg9xw3tzznikg8q7hIOu9leLfAG7LzmejLkB7dKSrDKUBVDLDtxNzRzK2pNU9D-ErBZcvb2o8qRNWoMrbiZaU0l3cRDRLlrRJYSFt90PbsfHBmyZXcYHSnXyg6rQB08sWq_MM0awUySouQ3IYJZQhNfXFmFUUoAFtafwRC4po1gbzxBzjhtpw6WKj"" rel=""nofollow noreferrer"">Here</a> is the full log.</p>","<p>As for the updated question, now you need to authenticate somehow to GKE inside the container.</p>

<p>First thing, I recommend installing gcloud tool to your container.
Btw, as for the huge container size 1.2 GB, that's because <code>cloud-builders/bazel</code> is huge :)</p>

<p>Have a look at our example on slim bazel container version:
<a href=""https://github.com/aspect-development/bazel-k8s-example/blob/master/tools/Dockerfile.dazel"" rel=""nofollow noreferrer"">https://github.com/aspect-development/bazel-k8s-example/blob/master/tools/Dockerfile.dazel</a></p>

<p>And here is Dockerfile for installing gcloud and kubectl, so you can grab needed parts from both files:
<a href=""https://github.com/GoogleCloudPlatform/cloud-builders/blob/master/gcloud/Dockerfile"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-builders/blob/master/gcloud/Dockerfile</a></p>

<p>The second thing is authenticating, after gcloud is installed it should be easy.
Overall cloudbuild step should look similar to this:</p>

<pre><code>- name: &lt;link to your container&gt;
  entrypoint: /bin/sh
  args:
  - -c
  - |
    gcloud container clusters get-credentials cents-ideas --zone europe-west3-a --project cents-ideas
    bazel run //:kubernetes.apply
</code></pre>"
"Cloud Build Trigger returns 14 UNAVAILABLE: 502:Bad Gateway<p>Using the JavaScript library, <a href=""https://github.com/googleapis/nodejs-cloudbuild"" rel=""nofollow noreferrer"">NodeJS-CloudBuild</a> we've been able to write a Cloud Function that triggers a new Cloud Build whenever we need. </p>

<p>It's been stable for a few months, but starting monday, we've been getting an inconsistent error whenever we call the function:
Error: 14 UNAVAILABLE: 502:Bad Gateway</p>

<p>The code sample is pretty simple, taken straight from the repository but with our variables. </p>

<pre><code>  // Creates a client
  const cb = new CloudBuildClient();

  // Starts a build against the branch provided.
  const [resp] = await cb.runBuildTrigger({
    projectId,
    triggerId,
    source: {
      projectId,
      dir: './',
      branchName,
    },
  });
  console.info(`triggered build for ${triggerId}`);
</code></pre>

<p>I'm not sure what changed recently to cause these return errors. It works fine when doing a ""manual"" trigger from the Cloud build dashboard, too. </p>","<p>It's been solved <a href=""https://issuetracker.google.com/issues/158130392"" rel=""nofollow noreferrer"">upstream</a>. It wasn't the library's fault. </p>"
"How to substitute a global variable for multiple triggers on cloud build?<p>Cloud build allows user-defined substitutions for the single trigger, But I need to assign global user-defined substitutions variable for multiple triggers.</p>

<p>Thanks in Advance.</p>","<p>If I have understood your question well, you are asking about sharing user-defined substituting variable values across different Cloud Build builds.</p>

<p>This is not currently possible since the <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values#using_user-defined_substitutions"" rel=""nofollow noreferrer"">user-defined substitutions</a> are related to an specific build either by being specified in the <code>cloudbuild.yaml</code> config file or in the <a href=""https://cloud.google.com/sdk/gcloud/reference/builds/submit#--substitutions"" rel=""nofollow noreferrer""><code>--substitutions argument</code></a> flag for the <code>gcloud</code> command.</p>"
"env step parameter in cloudbuild.yaml file not settings environment variable<p>My <code>cloudbuild.yaml</code> file looks like</p>

<pre><code>steps:
  # build the container image
  - name: ""gcr.io/cloud-builders/docker""
    args: [""build"", ""-t"", ""gcr.io/$PROJECT_ID/backend:$COMMIT_SHA"", "".""]
    env:
      - ""APP_ENV=production""
  # push the container image to Container Registry
  - name: ""gcr.io/cloud-builders/docker""
    args: [""push"", ""gcr.io/$PROJECT_ID/backend:$COMMIT_SHA""]
  # Deploy container image to Cloud Run
  - name: ""gcr.io/cloud-builders/gcloud""
    args:
      - ""run""
      - ""deploy""
      - ""backend""
      - ""--image""
      - ""gcr.io/$PROJECT_ID/backend:$COMMIT_SHA""
      - ""--region""
      - ""us-central1""
      - ""--platform""
      - ""managed""
images:
  - ""gcr.io/$PROJECT_ID/backend:$COMMIT_SHA""

</code></pre>

<p>and it builds and deploys a new container to Cloud Run, however it doesn't set the APP_ENV environment variable to <code>""production""</code>. Why is that and how do I get it to?</p>

<p>I am following <a href=""https://cloud.google.com/cloud-build/docs/build-config"" rel=""nofollow noreferrer"">this guide</a>.</p>","<pre><code>steps:
- env: [...]
</code></pre>

<p>approach sets environment variables for the Cloud Build container that runs the <code>docker build -t</code> command, so in this case only <code>docker build</code> it executes gets <code>APP_ENV</code> variable (and probably doesn't do anything with it).</p>

<p>You should not expect this to set environment variable for Cloud Run. For that to work, you need to specify <code>--set-env-vars</code> or <code>--update-env-vars</code> to Cloud Run in the <code>gcloud run deploy</code> step by specifying additional <code>args</code> above like:</p>

<pre><code>  - name: ""gcr.io/cloud-builders/gcloud""
    args:
      - ""run""
      - ""deploy""
      ...
      - ""--set-env-vars=KEY1=VALUE1""
      - ""--set-env-vars=KEY2=VALUE2""
      ...
</code></pre>

<p>See <a href=""https://cloud.google.com/run/docs/configuring/environment-variables#command-line"" rel=""nofollow noreferrer"">https://cloud.google.com/run/docs/configuring/environment-variables#command-line</a> to learn more or read <a href=""https://ahmet.im/blog/mastering-cloud-run-environment-variables/"" rel=""nofollow noreferrer"">this article</a> about alternative ways of specifying environment variables for Cloud Run applications.</p>"
"ERROR: (gcloud.builds.submit) HTTPError 403: Insufficient Permission using a build config file<p>Running <code>gcloud builds submit --config cloudbuild.yaml</code> 
results into </p>

<pre><code>ERROR: (gcloud.builds.submit) HTTPError 403: Insufficient Permission
</code></pre>

<p>Running <code>gcloud</code> info displays that I am using gcloud as the service account, which has EDITOR permissions on the project IAM.
<code>&lt;project-id&gt;@cloudbuild.gserviceaccount.com</code> edited it in the <strong>IAM page</strong> and gave it the App Engine Admin role.
Expected output should be similar to this :</p>

<pre><code>DONE
-------------------------------------------------------------------------------------------------------------------------
ID                                    CREATE_TIME                DURATION SOURCE                                   STATUS
$BUILD_ID                             2020-05-28T13:46:29+00:00  8S    gs://[PROJECT_ID]_cloudbuild/source/1508158566.55-725755714baa4b7e9e99984c422ec4e2.gz  gcr.io/[PROJECT-ID]/quickstart-image (+1 more)       SUCCESS
</code></pre>","<p>Managed to locate the problem, i had to Change the service account and access scopes for the instance i was using the Compute Engine Default Service Account and granted it ""Allow full access to all Cloud APIs"" access scope. 
Check out <a href=""https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances"" rel=""nofollow noreferrer"">https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances</a></p>"
"redirect output of gcloud command to a file in cloud build<p>I am using <code>cloudbuild.yml file</code>.</p>

<p>I am trying to grab the build output from inside the cloud build and push it to a file. This is how my step looks like:</p>

<pre><code>- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:slim'
  args: ['gcloud', 'builds', 'log', '$BUILD_ID', '&gt;buildlog.log']
  id: 'fetch-build-log'
</code></pre>

<p>This throws me an error saying <code>ERROR: (gcloud.builds.log) unrecognized arguments: &gt;buildlog.log</code> </p>

<p>If I execute that command in <code>cloud shell</code>, it works fine: <code>gcloud builds log xxxxx-xxxx-xxxx-xxxx-xxxxxxx &gt;guildlog.log</code></p>

<p>I am not sure why <code>cloud build</code> considers <code>&gt;buildlog.log</code> an argument when it is to redirect the output to the file.</p>

<p>Am I missing something here or is there another way of doing it?</p>","<p>In Cloud Build each builder has a default entrypoint, which typically correlates to that builder’s purpose. </p>

<p>In your example, you are using <code>cloud-sdk</code> default entrypoint and the positional args syntax, so each index should be a single argument.</p>

<p>That's why you receive the error: <code>ERROR: (gcloud.builds.log) unrecognized arguments: &gt;buildlog.log</code></p>

<p>I put together a working example changing the entrypoint to <code>/bin/bash</code>:</p>

<pre><code>steps:
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:slim'
  entrypoint: '/bin/bash'
  args: ['-c',
         'gcloud builds log $BUILD_ID &gt; buildlog.log']
  id: 'fetch-build-log'
- name: 'alpine'
  id: 'OUTPUT_LOG'
  args: ['sh',
         '-c',
         'cat /workspace/buildlog.log']
</code></pre>

<p>In that example I'm using the <code>-c</code> command, in case you want to understand why:</p>

<p>Quoting from <code>man bash</code>:</p>

<pre><code>   -c string If the -c option is present,  then  commands  are  read  from
             string.   If  there  are arguments after the string, they are
             assigned to the positional parameters, starting with $0.
</code></pre>

<p>Let me know if it works for you.</p>"
"Restart Services on GCE Instance Using Cloud Build<p>I have a GCE instance that I push code to using Cloud Build. I need to restart services on that VM once the code has been pushed. Here's what my Cloud Build YAML looks like:</p>

<pre><code>steps:

- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args: 
  - '-c'
  -  |
      for d in *; do
              gcloud compute scp $d user@instance-1:/path/to/directory --zone=asia-south1-a --recurse

      done

- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args:
  - '-c'
  -  |
      gcloud compute ssh user@instance-1 --zone=asia-south1-a --command=""service uwsgi restart""

timeout: 1200s
</code></pre>

<p>While the first step works fine, while trying to restart the service, I receive the following error:</p>

<pre><code>Already have image (with digest): gcr.io/cloud-builders/gcloud
Sent message type=method_call sender=n/a destination=org.freedesktop.DBus object=/org/freedesktop/DBus interface=org.freedesktop.DBus member=Hello cookie=1 reply_cookie=0 error=n/a
Got message type=method_return sender=org.freedesktop.DBus destination=:1.3477 object=n/a interface=n/a member=n/a cookie=1 reply_cookie=1 error=n/a
Sent message type=method_call sender=n/a destination=org.freedesktop.DBus object=/org/freedesktop/DBus interface=org.freedesktop.DBus member=AddMatch cookie=2 reply_cookie=0 error=n/a
Sent message type=method_call sender=n/a destination=org.freedesktop.DBus object=/org/freedesktop/DBus interface=org.freedesktop.DBus member=AddMatch cookie=3 reply_cookie=0 error=n/a
Calling manager for ReloadUnit on uwsgi.service, replace
Sent message type=method_call sender=n/a destination=org.freedesktop.systemd1 object=/org/freedesktop/systemd1 interface=org.freedesktop.systemd1.Manager member=ReloadUnit cookie=4 reply_cookie=0 error=n/a
Failed to reload uwsgi.service: Interactive authentication required.
See system logs and 'systemctl status uwsgi.service' for details.
Sent message type=method_call sender=n/a destination=org.freedesktop.DBus object=/org/freedesktop/DBus interface=org.freedesktop.DBus member=RemoveMatch cookie=5 reply_cookie=0 error=n/a
Sent message type=method_call sender=n/a destination=org.freedesktop.DBus object=/org/freedesktop/DBus interface=org.freedesktop.DBus member=RemoveMatch cookie=6 reply_cookie=0 error=n/a
Sent message type=method_call sender=n/a destination=org.freedesktop.systemd1 object=/org/freedesktop/systemd1 interface=org.freedesktop.systemd1.Manager member=ReloadUnit cookie=4 reply_cookie=0 error=n/a
Failed to reload uwsgi.service: Interactive authentication required.
See system logs and 'systemctl status uwsgi.service' for details.
</code></pre>

<p>I've tried disabling the interactive authentication mechanism as mentioned <a href=""https://serverfault.com/questions/864717/unable-to-restart-a-systemctl-service-remotely-as-a-non-root-user"">here</a> and <a href=""https://unix.stackexchange.com/questions/79960/how-to-disable-requiretty-for-a-single-command-in-sudoers"">here</a> but that hasn't worked. Please help.</p>","<p>After investigating this issue, I suggest a workaround, which is to make a listener run by root, that checks if a file exists. This file is created by the user from Cloud Build when there is a build (with <code>touch</code>, for example). If the file exists, the listener deletes it and restarts the service. The code:</p>

<pre><code>#!/bin/bash

# Listener to be run by root. When user from
# Cloud Build touches $FILE, root removes it
# and restarts the service from within the VM.

FILE=""/home/user/file""

function listen()
{
        if [ -f ""$FILE"" ]; then
            rm $FILE
            service uwsgi restart
            sleep 60
        else
            sleep 10
        fi
}

while true
do
    listen
done
</code></pre>"
"Migrating google cloud functions from one project to another in GCP?<p>I have some cloud functions. I deploy these cloud function through source repository using cloudbuild.yaml and Cloud Triggers
The cloudbuild.yaml is..</p>

<pre><code>steps:

- name: 'python:3.7'
  entrypoint: 'bash'
  args: 
    - '-c'
    - |
        pip3 install -r requirements.txt
        pytest -rP

- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions
  - deploy
  - Function_Name
  - --runtime=python37
  - --source=https://source.developers.google.com/projects/{}/repos/{}/moveable-aliases/master/paths/{}
  - --entry-point=main
  - --trigger-topic=TOPIC_NAME
  - --region=REGION
</code></pre>

<p>Now i would like to move this cloud function from this project to another project( Project A to Project B)</p>

<p>Now as i am not defining my project_id here. From where it is getting the project id?From service account?</p>

<p>How can i efficiently move this cloud function from Repository A to Repository B? as well as deploy it at Project B.</p>","<p>When you run your Cloud Build, <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values"" rel=""nofollow noreferrer"">some variable are set automatically</a>, like the current project. This current projet is used by default in the environment for deploying the function.</p>

<p>For keeping the current behavior and adding the capability to extend it to the next projet you can do this</p>

<ul>
<li>Define a substitution variable, for example <code>_TARGET_PROJECT_ID</code></li>
<li>Assign by default the current project</li>
</ul>

<pre><code>
...
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions
  - deploy
  - Function_Name
  - --runtime=python37
  - --source=https://source.developers.google.com/projects/{}/repos/{}/moveable-aliases/master/paths/{}
  - --entry-point=main
  - --trigger-topic=TOPIC_NAME
  - --region=REGION
  - --project=$_TARGET_PROJECT_ID
substitions:
  _TARGET_PROJECT_ID: $PROJECT_ID

</code></pre>

<p>Now, in your trigger (a new one?) or when you run your Cloud Build manually, specify the new project_ID if you want. If no, the current behavior (deployment in the current project) will continue.</p>"
"Google cloud build permission error while deploying rasa to App engine<p>I am trying to deploy a rasa project described like <a href=""https://medium.com/@muhammadhassaanrafique/deploying-rasa-powered-chatbot-to-google-cloud-platform-f4ee2eee5be4"" rel=""nofollow noreferrer"">here</a> to google app engine, But the build failed due to some permission issues  </p>

<p>The scripts try to create a folder inside container, </p>

<pre><code>FROM rasa/rasa
ENV BOT_ENV=production

COPY . /var/www
WORKDIR /var/www

RUN rasa train

ENTRYPOINT [ ""rasa"", ""run"", ""-p"", ""8080""]
</code></pre>

<p>I have added following permissions to google cloud build.</p>

<p><a href=""https://i.stack.imgur.com/8XP01.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8XP01.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/5euYS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5euYS.png"" alt=""enter image description here""></a></p>

<p>Error log frome cloud build.</p>

<pre><code>Epochs: 100%|██████████| 100/100 [00:16&lt;00:00,  6.04it/s, t_loss=1.485, i_loss=0.104, i_acc=1.000]
2020-05-31 18:05:25 INFO     rasa.utils.tensorflow.models  - Finished training.
2020-05-31 18:05:25 INFO     rasa.nlu.model  - Finished training component.
2020-05-31 18:05:25 INFO     rasa.nlu.model  - Starting to train component EntitySynonymMapper
2020-05-31 18:05:25 INFO     rasa.nlu.model  - Finished training component.
2020-05-31 18:05:25 INFO     rasa.nlu.model  - Starting to train component ResponseSelector
2020-05-31 18:05:25 INFO     rasa.nlu.selectors.response_selector  - Retrieval intent parameter was left to its default value. This response selector will be trained on training examples combining all retrieval intents.
2020-05-31 18:05:25 INFO     rasa.nlu.model  - Finished training component.
2020-05-31 18:05:25 INFO     rasa.nlu.model  - Successfully saved model into '/tmp/tmpha6sd3hw/nlu'
Training Core model...
Core model training completed.
Training NLU model...
NLU model training completed.
Traceback (most recent call last):
  File ""/opt/venv/bin/rasa"", line 8, in &lt;module&gt;
    sys.exit(main())
  File ""/opt/venv/lib/python3.7/site-packages/rasa/__main__.py"", line 108, in main
    cmdline_arguments.func(cmdline_arguments)
  File ""/opt/venv/lib/python3.7/site-packages/rasa/cli/train.py"", line 77, in train
    nlu_additional_arguments=extract_nlu_additional_arguments(args),
  File ""/opt/venv/lib/python3.7/site-packages/rasa/train.py"", line 52, in train
    nlu_additional_arguments=nlu_additional_arguments,
  File ""uvloop/loop.pyx"", line 1456, in uvloop.loop.Loop.run_until_complete
  File ""/opt/venv/lib/python3.7/site-packages/rasa/train.py"", line 107, in train_async
    nlu_additional_arguments=nlu_additional_arguments,
  File ""/opt/venv/lib/python3.7/site-packages/rasa/train.py"", line 206, in _train_async_internal
    fixed_model_name=fixed_model_name,
  File ""/opt/venv/lib/python3.7/site-packages/rasa/model.py"", line 464, in package_model
    create_package_rasa(train_path, output_directory, fingerprint)
  File ""/opt/venv/lib/python3.7/site-packages/rasa/model.py"", line 258, in create_package_rasa
    os.makedirs(output_directory)
  File ""/usr/local/lib/python3.7/os.py"", line 223, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: 'models'
The command '/bin/bash -o pipefail -c rasa train' returned a non-zero code: 1
ERROR
ERROR: build step 0 ""gcr.io/cloud-builders/docker"" failed: step exited with non-zero status: 1
</code></pre>

<p>also facing the same issue when doing  <code>gcloud app deploy</code> from cloud shell </p>

<p><a href=""https://github.com/PRAJINPRAKASH/covid19-chatbot"" rel=""nofollow noreferrer"">my repo</a> of the source code,
 <em>need to remove cloudbuild.yaml</em> before running <code>gcloud app deploy</code></p>","<p>It was due to permission issue of the docker, fixed by adding the <code>USER root</code></p>

<pre><code>FROM rasa/rasa
USER root
ENV BOT_ENV=production

COPY . /var/www
WORKDIR /var/www

RUN rasa train

ENTRYPOINT [ ""rasa"", ""run"", ""-p"", ""8080""]
</code></pre>"
"Google Cloud Build: How to increase RAM memory?<p>How do I increase RAM memory on Google Cloud Build?</p>

<p>I'm getting this error:</p>

<p><code>Step #1: FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory</code></p>

<p>I'm using the REST API.</p>

<p>I'm trying to find the RAM memory config, but I only found a property called <code>diskSizeInGb</code>. 
The default for <code>diskSizeInGb</code>is 100GB, and it's just a React app I'm compiling, so I don't think that's the case.</p>

<p><a href=""https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.builds#buildoptions"" rel=""nofollow noreferrer"">https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.builds#buildoptions</a></p>","<p>The RAM of the instance is dependant on the <a href=""https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.builds#Build.MachineType"" rel=""noreferrer"">machine type</a> you are using, if you need more RAM in your build, you will need to use a different value for <code>machineType</code>.</p>

<p>By default, Cloud Build uses a ""n1-standard-1"" instance to run the build which has 3.75 GB of memory, however, you can change it to a ""n1-highcpu-8"" which has double that. You can find the information regarding the instance types over <a href=""https://cloud.google.com/compute/docs/machine-types#n1_machine_type"" rel=""noreferrer"">here</a>.</p>

<p>Keep in mind that Cloud  Build only accepts ""n1-standard-1"", ""n1-highcpu-8"" and ""n1-highcpu-32"" machines as mentioned in the documentation, and that each has a <a href=""https://cloud.google.com/cloud-build/pricing"" rel=""noreferrer"">different billing</a>.</p>

<p>Hope you find this useful!</p>"
"Laravel deployment to the App engine broken after a composer command<p>I am programming a PHP API using Laravel 7. My dev process is like that:</p>

<p>Local (Homestead@Virtualbox: Ubuntu) => Github => Google Cloud build (deployment triggered by a master commit) => Google App Engine &lt;> Google Cloud SQL</p>

<p>For now I had no problem deploying my app this way but yesterday I tried to add a composer package (mpociot/laravel-apidoc-generator). After some local tests where everything worked just fine I committed my master branch and my GC Build trigger tried to deploy the app on my App engine instance.</p>

<p>The process terminated with a error I have never seen so far. I then reverted my latest commit, going back to the state where I never ran the composer command and committed my master branch again: the deployment succeeded as usual.</p>

<p>So as there was something happening with the composer command, I investigated further.</p>

<p>Here is the log I get in my GC Build failed deployment:</p>

<pre><code>FETCHSOURCE
Fetching storage object: gs://701891080190.cloudbuild-source.googleusercontent.com/7541db97159423ca9ec0996eb740a404aef3c4b9-ec3f59e5-8eb5-46f9-98ab-6848be6dd6f8.tar.gz#1589712757789375
Copying gs://701891080190.cloudbuild-source.googleusercontent.com/7541db97159423ca9ec0996eb740a404aef3c4b9-ec3f59e5-8eb5-46f9-98ab-6848be6dd6f8.tar.gz#1589712757789375...
/ [0 files][    0.0 B/  9.4 MiB]                                                
/ [1 files][  9.4 MiB/  9.4 MiB]                                                
Operation completed over 1 objects/9.4 MiB.                                      
BUILD
Already have image (with digest): gcr.io/cloud-builders/gcloud
Services to deploy:

descriptor:      [/workspace/app.yaml]
source:          [/workspace]
target project:  [#####]
target service:  [#####]
target version:  [20200517t105250]
target url:      [https://#####.ew.r.appspot.com]


Do you want to continue (Y/n)?  
Beginning deployment of service [heavnt-test]...
#============================================================#
#= Uploading 2 files to Google Cloud Storage                =#
#============================================================#
File upload done.
Updating service [heavnt-test]...
.............................................................................................................................................................................................................failed.
ERROR: (gcloud.app.deploy) Error Response: [9] Cloud build 0dd458db-cd18-463a-b0de-034668d9802a status: FAILURE.
Build error details: {""error"":{""errorType"":""BuildError"",""canonicalCode"":""INVALID_ARGUMENT"",""errorId"":""39857F2F"",""errorMessage"":""INFO     FTL version php-v0.17.0\nINFO     Beginning FTL build for php\nINFO     FTL arg passed: exposed_ports None\nINFO     FTL arg passed: cache_repository eu.gcr.io/hotpot-63d32/app-engine-tmp/build-cache/ttl-7d\nINFO     FTL arg passed: tar_base_image_path None\nINFO     FTL arg passed: export_cache_stats False\nINFO     FTL arg passed: builder_output_path \""\""\nINFO     FTL arg passed: name eu.gcr.io/hotpot-63d32/app-engine-tmp/app/ttl-2h:a30cd452-e54a-46f4-a041-9b975258909a\nINFO     FTL arg passed: ttl 168\nINFO     FTL arg passed: global_cache False\nINFO     FTL arg passed: cache True\nINFO     FTL arg passed: upload True\nINFO     FTL arg passed: sh_c_prefix False\nINFO     FTL arg passed: fail_on_error True\nINFO     FTL arg passed: base eu.gcr.io/gae-runtimes/php72:php72_20200412_7_2_29_RC00\nINFO     FTL arg passed: output_path None\nINFO     FTL arg passed: cache_key_version v0.17.0\nINFO     FTL arg passed: cache_salt \nINFO     FTL arg passed: directory /workspace\nINFO     FTL arg passed: entrypoint None\nINFO     FTL arg passed: additional_directory /.googleconfig\nINFO     FTL arg passed: destination_path /srv\nINFO     FTL arg passed: verbosity NOTSET\nINFO     starting: full build\nINFO     starting: builder initialization\nINFO     Loading Docker credentials for repository 'eu.gcr.io/gae-runtimes/php72:php72_20200412_7_2_29_RC00'\nINFO     Loading Docker credentials for repository 'eu.gcr.io/hotpot-63d32/app-engine-tmp/app/ttl-2h:a30cd452-e54a-46f4-a041-9b975258909a'\nINFO     builder initialization took 0 seconds\nINFO     starting: build process for FTL image\nINFO     starting: rm_vendor_dir\nINFO     rm_vendor_dir rm -rf /workspace/vendor\nINFO     `rm_vendor_dir` stdout:\n\nINFO     rm_vendor_dir took 0 seconds\nINFO     descriptor_contents:\n{\n    \""_readme\"": [\n        \""This file locks the dependencies of your project to a known state\"",\n        \""Read more about it at https://getcomposer.org/doc/01-basic-usage.md#installing-dependencies\"",\n        \""This file is @generated automatically\""\n    ],\n    \""content-hash\"": \""f8027c7d3e5d36c84e1215645bb96efa\"",\n    \""packages\"": [\n        {\n            \""name\"": \""asm89/stack-cors\"",\n            \""version\"": \""1.3.0\"",\n            \""source\"": {\n                \""type\"": \""git\"",\n                \""url\"": \""https://github.com/asm89/stack-cors.git\"",\n                \""reference\"": \""b9c31def6a83f84b4d4a40d35996d375755f0e08\""\n            },\n            \""dist\"": {\n                \""type\"": \""zip\"",\n                \""url\"": \""https://api.github.com/repos/asm89/stack-cors/zipball/b9c31def6a83f84b4d4a40d35996d375755f0e08\"",\n                \""reference\"": \""b9c31def6a83f84b4d4a40d35996d375755f0e08\"",\n                \""shasum\"": \""\""\n            },\n            \""require\"": {\n                \""php\"": \""\u003e=5.5.9\"",\n                \""symfony/http-foundation\"": \""~2.7|~3.0|~4.0|~5.0\"",\n                \""symfony/http-kernel\"": \""~2.7|~3.0|~4.0|~5.0\""\n            },\n            \""require-dev\"": {\n                \""phpunit/phpunit\"": \""^5.0 || ^4.8.10\"",\n                \""squizlabs/php_codesniffer\"": \""^2.3\""\n            },\n            \""type\"": \""library\"",\n            \""extra\"": {\n                \""branch-alias\"": {\n                    \""dev-master\"": \""1.2-dev\""\n                }\n            },\n            \""autoload\"": {\n                \""psr-4\"": {\n                    \""Asm89\\\\Stack\\\\\"": \""src/Asm89/Stack/\""\n                }\n            },\n            \""notification-url\"": \""https://packagist.org/downloads/\"",\n            \""license\"": [\n                \""MIT\""\n            ],\n            \""authors\"": [\n                {\n                    \""name\"": \""Alexander\"",\n                    \""email\"": \""iam.asm89@gmail.com\""\n                }\n            ],\n            \""description\"": \""Cross-origin resource sharing library and stack middleware\"",\n            \""homepage\"": \""https://github.com/asm8.
Check the build log for errors: ####SEE BELOW####
ERROR
ERROR: build step 0 ""gcr.io/cloud-builders/gcloud"" failed: step exited with non-zero status: 1
</code></pre>

<p>Le full content of the log is too big (16k+ lines) for this site but it ends like this:</p>

<pre><code>Step #1 - ""builder"": Generating optimized autoload files
Step #1 - ""builder"": &gt; Illuminate\Foundation\ComposerScripts::postAutoloadDump
Step #1 - ""builder"": &gt; @php artisan package:discover --ansi
Step #1 - ""builder"": Script @php artisan package:discover --ansi handling the post-autoload-dump event returned with error code 1
Step #1 - ""builder"": 
Step #1 - ""builder"": error: `composer_install` returned code: 1
Step #1 - ""builder"": Traceback (most recent call last):
Step #1 - ""builder"":   File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
Step #1 - ""builder"":     ""__main__"", fname, loader, pkg_name)
Step #1 - ""builder"":   File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
Step #1 - ""builder"":     exec code in run_globals
Step #1 - ""builder"":   File ""/usr/local/bin/ftl.par/__main__.py"", line 65, in &lt;module&gt;
Step #1 - ""builder"":   File ""/usr/local/bin/ftl.par/__main__.py"", line 57, in main
Step #1 - ""builder"":   File ""/usr/local/bin/ftl.par/__main__/ftl/common/ftl_error.py"", line 58, in UserErrorHandler
Step #1 - ""builder"": IOError: [Errno 2] No such file or directory: '""""/output'
Finished Step #1 - ""builder""
ERROR
ERROR: build step 1 ""gcr.io/gae-runtimes/php72_app_builder:php72_20200412_7_2_29_RC00"" failed: step exited with non-zero status: 1
</code></pre>

<p>My App.yaml file:</p>

<pre><code>service: heavnt-test
runtime: php72

env_variables:
    ## Put production environment variables here.
    APP_KEY: base64:TRoTxS+x5PeiAOlwV/F1AwyfgV6y8mZloqGastMYylQ=
    APP_STORAGE: /tmp
    VIEW_COMPILED_PATH: /tmp
    SESSION_DRIVER: cookie  
    APP_SERVICES_CACHE: /tmp/services.php
    APP_PACKAGES_CACHE: /tmp/packages.php
    APP_CONFIG_CACHE: /tmp/config.php
    APP_ROUTES_CACHE: /tmp/routes.php

handlers:
- url: /(.*\.(gif|png|jpg|css|js))$
  static_files: public/\1
  upload: public/.*\.(gif|png|jpg|css|js)$

- url: /.*
  secure: always
  redirect_http_response_code: 301
  script: auto

entrypoint: serve /public/index.php
</code></pre>

<p>My Cloudbuild.yaml file:</p>

<pre><code>steps:
- name: ""gcr.io/cloud-builders/gcloud""
  args: [""app"", ""deploy""]
timeout: ""1600s""
</code></pre>

<p>I tried to check, using Github Desktop, the changes happening on my files when adding my composer package but there are so many files added and changed that I had no clue</p>

<p>EDIT 18/05/2020:</p>

<p>In the 16k+ lines log, I found this issue:</p>

<pre><code>Step #1 - ""builder"": INFO     composer_install composer install --no-dev --no-progress --no-suggest --no-interaction
Step #1 - ""builder"": INFO     `composer_install` stdout:
Step #1 - ""builder"": 
Step #1 - ""builder"": In PackageManifest.php line 177:
Step #1 - ""builder"":                                                                           
Step #1 - ""builder"":   The /workspace/bootstrap/cache directory must be present and writable.  
</code></pre>

<p>I am not sure how to figure out if this is the problem I am experiencing as everything works fine in local and used to work fine online. </p>","<p>To investigate further I tried to deploy my app directly from my local setup (cloud app deploy), so without using my Github>GCloud Build bridge.</p>

<p>It deployed like a charm so I tried accessing the docs package I initially wanted to add and as the static files in public/docs/ weren't served I added those lines to my app.yaml: </p>

<pre><code>- url: /docs
  static_dir: public/docs
</code></pre>

<p><strong>So, as far as I'm aware, this is the only lines I changed.</strong></p>

<p>To try, I committed this code (docs package+app.yaml lines) via my original process and it deployed perfectly (triggered multiple times to be sure).</p>

<p>So to conclude, I have absolutely no idea how and why this happened and I truly hope it doesn't appear again.</p>"
"substitution variable $BRANCH_NAME gives nothing while building<p>I'm building docker images using cloud builder trigger, previously $BRNACH_NAME was working but now its giving null.</p>

<p>Thanks in advance.</p>","<p>I will post my comment as an answer as it is too long for comment section.</p>
<p>According to <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values#using_default_substitutions"" rel=""nofollow noreferrer"">this documentation</a>, you should have the possibility to use <em><strong>$BRANCH_NAME</strong></em> default substitution for builds invoked by triggers.</p>
<p>In the same documentation it is stated that:</p>
<blockquote>
<p>If a default substitution is not available (such as with sourceless
builds, or with builds that use storage source), then occurrences of
the missing variable are replaced with an empty string.</p>
</blockquote>
<p>I assume this might be the reason you are receiving <em><strong>NULL</strong></em>.</p>
<p>Have you performed any changes? Could you please provide some further information, such as your .yaml/.json file, your trigger configuration and the error you are receiving?</p>"
"Cloud build avoid billing by changing eu.artifacts.<project>.appspot.com bucket to single-region<p>Using app engine standard environment for python 3.7.</p>

<p>When running the <code>app deploy</code> command are container images uploaded to google storage in the bucket <code>eu.artifacts.&lt;project&gt;.appspot.com</code>.</p>

<p>This message is printed during <code>app deploy</code></p>

<pre><code> Beginning deployment of service [default]...
#============================================================#
#= Uploading 827 files to Google Cloud Storage              =#
#============================================================#
File upload done.
Updating service [default]...
</code></pre>

<p>The files are uploaded to a multi-region (eu), how do I change this to upload to a single region?</p>

<p>Guessing that it's a configuration file that should be added to the repository to instruct app engine, cloud build or cloud storage that the files should be uploaded to a single region.</p>

<p>Is the <code>eu.artifacts.&lt;project&gt;.appspot.com</code> bucket required, or could all files be ignore using the <code>.gcloudignore</code> file?</p>

<p>The issue is similar to this issue <a href=""https://stackoverflow.com/questions/60982068/how-can-i-specify-a-region-for-the-cloud-storage-buckets-used-by-cloud-build-for"">How can I specify a region for the Cloud Storage buckets used by Cloud Build for a Cloud Run deployment?</a>, but for app engine.</p>

<p>I'm triggering the cloud build using a service account.</p>

<p>Tried to implement the changes in the solution in the link above, but aren't able to get rid of the multi region bucket.</p>

<pre><code>substitutions:
  _BUCKET: unused
steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['app', 'deploy', '--promote', '--stop-previous-version']
artifacts:
  objects:
    location: 'gs://${_BUCKET}/artifacts'
    paths: ['*']
</code></pre>

<p>Command <code>gcloud builds submit --gcs-log-dir=""gs://$BUCKET/logs"" --gcs-source-staging-dir=""gs://$BUCKET/source"" --substitutions=_BUCKET=""$BUCKET""</code></p>","<p>After investigation a little bit more, I want to mention that this
kind of bucket is created by the “container registry” product when you deploy a new container( <strong>when you deploy your App Engine Application</strong>)-&gt; When you push an image to a registry with a new hostname, Container Registry creates a storage bucket in the specified multi-regional location.This bucket is the underlying storage for the registry. Within a project, all registries with the same hostname share one storage <a href=""https://cloud.google.com/container-registry/docs/overview#registry_name"" rel=""nofollow noreferrer"">bucket</a>.</p>
<p>Based on <a href=""https://cloud.google.com/cloud-build/docs/securing-builds/set-service-account-permissions#pull_private_images_from_others"" rel=""nofollow noreferrer"">this</a>, it is not accessible by default and itself contains container images which are written when you deploy a new container. It's not recommended to modify it because the artifacts bucket is meant to contain deployment images, which may influence your app.</p>
<p>Finally, something curious that I found is when you create a default bucket (as is the case of the aforementioned bucket), you also get a staging bucket with the same name except that staging. You can use this staging bucket for temporary files used for staging and test purposes; it also has a 5 GB limit, but it is automatically emptied on a weekly <a href=""https://cloud.google.com/appengine/docs/standard/python/googlecloudstorageclient/setting-up-cloud-storage#activating_a_cloud_storage_bucket"" rel=""nofollow noreferrer"">basis</a></p>"
"Continuous deployment from git using Cloud Build<p>I am trying to make a build trigger for Cloud Run using <a href=""https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build#fully-managed_1"" rel=""nofollow noreferrer"">this tutorial</a>,
but I get the following error message:</p>

<pre><code>Starting Step #0
Step #0: Already have image (with digest): gcr.io/cloud-builders/docker
Step #0: unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /workspace/Dockerfile: no such file or directory
Finished Step #0
ERROR
ERROR: build step 0 ""gcr.io/cloud-builders/docker"" failed: step exited with non-zero status: 1
</code></pre>

<p>Does anyone know why?</p>

<p>EDIT: My project repo is split into frontend and backend folders. I am just trying to deploy my backend folder which contains a go api.</p>","<p>I have followed the tutorial you provided and I encountered the same error message.</p>

<p>It seems like the steps specified inside the <code>cloudbuild.yaml</code> file are requiring a <code>Dockerfile</code> to be created on the repositories root folder. Precisely, the following instruction is building the image on your <code>.</code> folder.</p>

<pre><code>- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/$PROJECT_ID/[SERVICE-NAME]:$COMMIT_SHA', '.']
</code></pre>

<p>There are two solutions to your problem. If you need build a docker image, simply creating the <code>Dockerfile</code> will solve your issue. Another solution would be to not use a custom image. I have used the following <code>cloudbuild.yaml</code> file in order to deploy successfully:</p>

<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - 'run'
  - 'deploy'
  - '[SERVICE-NAME]'
  - '--image'
  - 'gcr.io/cloudrun/hello'
  - '--region'
  - '[REGION]'
  - '--platform'
  - 'managed'
</code></pre>

<p>Notice how I'm still using a container image (<code>gcr.io/cloudrun/hello</code>).</p>

<p>-- edit</p>

<p>As explained by @guillaume-blaquiere, the tutorial takes for granted that your repository is already working on Cloud Run. You should check a <a href=""https://cloud.google.com/run/docs/quickstarts/build-and-deploy"" rel=""nofollow noreferrer"">Cloud Run tutorial</a> before this one.</p>

<p>-- edit 2</p>

<p>A third solution that worked for OP is to specify the path of the <code>Dockerfile</code> in the build instruction. That is done by changing the <code>.</code> directory for the relative directory that contains the <code>Dockerfile</code>.</p>"
"Google Cloud Build - build.yaml & app.yaml in a sub directory not building correct source<p>I'm having trouble setting up my yaml files for google app engine. The configuration works correctly when my app.yaml file is in the root of the project but if it is within a subdirectory it does not build the correct source. I suspect I need to set the <code>dir:</code> option in the build config, but I have tried multiple variations and I can't get it to work.</p>

<p>Working file structure, deployed app is ~3mb in size.</p>

<pre><code>src
deployment
└── staging
    └── build.yaml
app.staging.yaml


# build.yaml
steps:
- name: node:12
  entrypoint: yarn
- name: node:12
  entrypoint: yarn
  args: ['build']
- name: ""gcr.io/cloud-builders/gcloud""
  args: [""app"", ""deploy"", ""app.staging.yaml""]
timeout: ""1800s""
</code></pre>

<p>Not working file structure, deployed app is ~1kb in size.</p>

<pre><code>src
deployment
└── staging
    └── build.yaml
    └── app.yaml


# build.yaml
steps:
- name: node:12
  entrypoint: yarn
- name: node:12
  entrypoint: yarn
  args: ['build']
- name: ""gcr.io/cloud-builders/gcloud""
  args: [""app"", ""deploy"", ""deployment/staging/app.yaml""]
timeout: ""1800s""
</code></pre>

<p>In both scenarios I am kicking off the deployment with:
<code>gcloud builds submit --config deployment/staging/build.yaml</code></p>

<p>What should my <code>dir:</code> be set to in the build.yaml steps so that the build step knows to build from root? Is there any way to debug this locally without having to upload the source every time?</p>

<p>Thanks!
A</p>","<p>you cannot have the app.yaml and the cloudbuild.yaml in the same directory if you are deploying in a non-custom runtime. Please see <a href=""https://stackoverflow.com/questions/52555496/does-appengine-cloudbuild-yaml-requires-a-custom-runtime#answer-52558814"">this comment</a></p>"
"Run DB migrations on cloud build connecting to cloud sql using private IP<p>I am trying to setup db migrations for a Nodejs app on cloud build connecting to cloud sql with a private IP via cloud sql proxy.
Cloud SQL connection always fail from cloud build.</p>

<p>Currently I am running migration manually from a compute engine.</p>

<p>I followed this SO to setup the build steps.
<a href=""https://stackoverflow.com/questions/52352103/run-node-js-database-migrations-on-google-cloud-sql-during-google-cloud-build/52366671#52366671"">Run node.js database migrations on Google Cloud SQL during Google Cloud Build</a></p>

<p>cloudbuild.yaml</p>

<pre><code>steps:
  - name: node:12-slim
    args: [""npm"", ""install""]
    env:
      - ""NODE_ENV=${_NODE_ENV}""
  - name: alpine:3.10
    entrypoint: sh
    args:
      - -c
      - ""wget -O /workspace/cloud_sql_proxy https://storage.googleapis.com/cloudsql-proxy/v1.16/cloud_sql_proxy.linux.386 &amp;&amp;  chmod +x /workspace/cloud_sql_proxy""
  - name: node:12
    timeout: 100s
    entrypoint: sh
    args:
      - -c
      - ""(/workspace/cloud_sql_proxy -dir=/workspace -instances=my-project-id:asia-south1:postgres-master=tcp:5432 &amp; sleep 3) &amp;&amp; npm run migrate""
    env:
      - ""NODE_ENV=${_NODE_ENV}""
      - ""DB_NAME=${_DB_NAME}""
      - ""DB_PASS=${_DB_PASS}""
      - ""DB_USER=${_DB_USER}""
      - ""DB_HOST=${_DB_HOST}""
      - ""DB_PORT=${_DB_PORT}""
  - name: ""gcr.io/cloud-builders/gcloud""
    entrypoint: ""bash""
    args:
      [
        ""-c"",
        ""gcloud secrets versions access latest --secret=backend-api-env &gt; credentials.yaml"",
      ]
  - name: ""gcr.io/cloud-builders/gcloud""
    args: [""app"", ""deploy"", ""--stop-previous-version"", ""-v"", ""$SHORT_SHA""]
timeout: ""600s""
</code></pre>

<p>Error:</p>

<pre><code>KnexTimeoutError: Knex: Timeout acquiring a connection. The pool is probably full. Are you missing a .transacting(trx) call?
Step #2:     at Client_PG.acquireConnection (/workspace/node_modules/knex/lib/client.js:349:26)
</code></pre>

<p>Cloud build roles:</p>

<pre><code>Cloud Build Service Account
Cloud SQL Admin
Compute Network User
Service Account User
Secret Manager Secret Accessor
Serverless VPC Access Admin
</code></pre>

<p>CLOUD SQL ADMIN API is enabled too.</p>

<p>Versions:</p>

<pre><code>NPM libs:
  ""pg"": ""8.0.3""
  ""knex"": ""0.21.1""
</code></pre>","<p>The Cloud SQL Private IP feature uses internal IP addresses hosted in a <a href=""https://cloud.google.com/vpc/docs/vpc"" rel=""nofollow noreferrer"">VPC network</a>, which are only accessible from other resources within the same VPC network. </p>

<p>Since Cloud Build does not support VPC Networks, it is not possible to connect from Cloud Build to the private IP of a Cloud SQL instance. </p>

<p>You might want to take a look at the <a href=""https://cloud.google.com/sql/docs/mysql/configure-private-ip#connecting_to_an_instance_using_its_private_ip"" rel=""nofollow noreferrer"">official Cloud SQL documentation</a> regarding this topic to choose another alternative that suits your use case.  </p>"
"quick start in google cloud build<p>I ran the quick start</p>

<pre><code>https://cloud.google.com/cloud-build/docs/quickstart-build
</code></pre>

<p>and in the section ""View the build details"", I don't see the output of the quickstart.sh file anywhere.  Where is the logs from actually running the quickstart.sh file?</p>

<p>Without any output from quickstart.sh, I am unsure how to log what is going on in docker so I can fix broken builds that build in docker.</p>","<p>In this official tutorial, a docker container is built via Cloud Build, with only one executable bash script which is displaying the current date.</p>

<pre class=""lang-sh prettyprint-override""><code>#!/bin/sh
echo ""Hello, world! The time is $(date).""
</code></pre>

<p>Here is the Dockerfile :</p>

<pre><code>FROM alpine
COPY quickstart.sh /
CMD [""/quickstart.sh""]
</code></pre>

<p>It means <code>quickstart.sh</code> is never executed during build phase but only at the execution step of container.</p>

<p>To see the output of script, you should run container (either locally on your computer, or via Cloud Shell) : </p>

<pre><code>$ docker run gcr.io/[PROJECT-ID]/quickstart-image:latest
Hello, world! The time is Sat Jun 13 05:10:41 UTC 2020.
</code></pre>

<p>If you want to execute a script during build phase of container, you should use <code>RUN</code> command. </p>

<p>For example, let's create a second executable script called <code>build.sh</code> in the same directory:</p>

<pre class=""lang-sh prettyprint-override""><code>#!/bin/sh
echo ""Hello, build at $(date).""
</code></pre>

<p>Then, add it on <code>Dockerfile</code> file description : </p>

<pre><code>FROM alpine
COPY quickstart.sh /
COPY build.sh /
RUN /build.sh
CMD [""/quickstart.sh""]
</code></pre>

<p>Now, we can build a new version of container image : </p>

<pre><code>gcloud builds submit --tag gcr.io/[PROJECT-ID]/quickstart-image
</code></pre>

<p>This time, output of <code>build.sh</code> could be seen in the details output log in Cloud Build console:</p>

<p><a href=""https://i.stack.imgur.com/tcACI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tcACI.png"" alt=""Cloud build details log""></a></p>

<p>Of course, here it's just a simple example to give you a quick answer. You may check all other possible options to write a correct and clean <code>Dockerfile</code>. But it's not really linked with Cloud Build.</p>"
"How to use `gcloud config` to build an image based on `gcr.io/google.com/cloudsdktool/cloud-sdk`<p>I am trying to build an image based on <code>gcr.io/google.com/cloudsdktool/cloud-sdk:alpine</code>, with some configuration I already know on build time. After some testing with <code>gcloud config set</code> on my local system, I transposed these commands into my Dockerfile. Although the sequence of commands yields the expected <em>final</em> gcloud configuration on my computer, the behaviour I found on the docker image was different and no change was persisted after the <code>RUN</code> commands.</p>

<p>I didn't expect this behaviour. What should explain this?</p>

<p>I used the following Dockerfile to assess the issue during troubleshoot:</p>

<pre><code>FROM gcr.io/google.com/cloudsdktool/cloud-sdk:alpine AS runner
  RUN [""gcloud"", ""config"", ""list""]
  RUN [""gcloud"", ""config"", ""set"", ""compute/region"", ""europe-west1""]
  RUN [""gcloud"", ""config"", ""list""]
  ENTRYPOINT [ ""bash"" ]
</code></pre>

<p>One can see that step 2/5 and step 4/5 yield the same output, even though the step 3/4 should change the <em>current</em> gcloud configuration for <code>compute/region</code>.</p>

<pre class=""lang-sh prettyprint-override""><code>$ docker build --no-cache .
Sending build context to Docker daemon  2.048kB
Step 1/5 : FROM gcr.io/google.com/cloudsdktool/cloud-sdk:alpine AS runner
 ---&gt; e3ce7ea1a190
Step 2/5 : RUN [""gcloud"", ""config"", ""list""]
 ---&gt; Running in b8fb806b04aa
[component_manager]
disable_update_check = true
[core]
disable_usage_reporting = true
[metrics]
environment = github_docker_image

Your active configuration is: [default]
Removing intermediate container b8fb806b04aa
 ---&gt; bc8321e71e60
Step 3/5 : RUN [""gcloud"", ""config"", ""set"", ""compute/region"", ""europe-west1""]
 ---&gt; Running in cfbcb80b1a31
Updated property [compute/region].
Removing intermediate container cfbcb80b1a31
 ---&gt; db76ee7c5e60
Step 4/5 : RUN [""gcloud"", ""config"", ""list""]
 ---&gt; Running in d90c64fcf0b0
[component_manager]
disable_update_check = true
[core]
disable_usage_reporting = true
[metrics]
environment = github_docker_image

Your active configuration is: [default]
Removing intermediate container d90c64fcf0b0
 ---&gt; a593aa61055e
Step 5/5 : ENTRYPOINT [ ""bash"" ]
 ---&gt; Running in acb4d360754c
Removing intermediate container acb4d360754c
 ---&gt; edab55257026
Successfully built edab55257026
</code></pre>

<p>cloud-builders
<a href=""https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/gcloud"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/gcloud</a></p>","<p><code>gcloud</code> config is persisted in <code>/root/.config/gcloud/configurations/config_default</code></p>

<p>BUT <code>/root/.config</code> and <code>/root/.kube</code> are <code>VOLUME</code>s and can't be changed:</p>

<pre><code>VOLUME [""/root/.config"", ""/root/.kube""]
</code></pre>

<p><a href=""https://github.com/GoogleCloudPlatform/cloud-sdk-docker/blob/8764ec3c23860b35e9a7883c74ca2b4a6b45d866/Dockerfile#L42"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-sdk-docker/blob/8764ec3c23860b35e9a7883c74ca2b4a6b45d866/Dockerfile#L42</a></p>

<p>""If any build steps change the data within the volume after it has been declared, those changes will be discarded.""</p>

<p>see: <a href=""https://docs.docker.com/engine/reference/builder/#volume"" rel=""nofollow noreferrer"">https://docs.docker.com/engine/reference/builder/#volume</a></p>"
"Install google chrome for python selenium in a google cloud run app<p>I'm trying to use selenium in a cloud run app that I'm working on. The app works correctly on my local machine but it's not working after I deploy it on google cloud run. The error I'm getting is related to not having <code>chromedriver.exe</code> installed on google cloud.</p>

<p>This is the traceback error from the logs:</p>

<pre><code>Traceback (most recent call last): File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 2447, in wsgi_app response = self.full_dispatch_request() File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1952, in full_dispatch_request rv = self.handle_user_exception(e) File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1821, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/usr/local/lib/python3.6/site-packages/flask/_compat.py"", line 39, in reraise raise value File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1950, in full_dispatch_request rv = self.dispatch_request() File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1936, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/app/main.py"", line 22, in daily_login login.create_access_token() File ""/app/scripts/loginFlow/login.py"", line 30, in create_access_token request_token = generate_request_token() File ""/app/scripts/loginFlow/login.py"", line 61, in generate_request_token driver = webdriver.Chrome() File ""/usr/local/lib/python3.6/site-packages/selenium/webdriver/chrome/webdriver.py"", line 73, in __init__ self.service.start() File ""/usr/local/lib/python3.6/site-packages/selenium/webdriver/common/service.py"", line 83, in start os.path.basename(self.path), self.start_error_message) selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home
</code></pre>

<p>The relevant part of that traceback seems to be <code>Message: 'chromedriver' executable needs to be in PATH</code>. How do I get the chromedriver executable in PATH for this google cloud run app?</p>

<hr>

<p>Now, what have I tried to do about it?</p>

<p>First, I found this post: <a href=""https://stackoverflow.com/questions/58780601/unable-to-run-selenium-chrome-driver-on-google-cloud-run"">unable to run selenium chrome-driver on google-cloud-run</a> and followed the instructions left in the comments. I looked at this <a href=""https://dev.to/googlecloud/using-headless-chrome-with-cloud-run-3fdp"" rel=""nofollow noreferrer"">post on dev.to</a> and modeled my <code>Dockerfile</code> similar to what was suggested. The only difference is that I'm using Python3.6 instead of Python3.7:</p>

<pre><code># Use the official lightweight Python image.
# https://hub.docker.com/_/python
FROM python:3.6-slim

# Install manually all the missing libraries
RUN apt-get update
RUN apt-get install -y gconf-service libasound2 libatk1.0-0 libcairo2 libcups2 libfontconfig1 libgdk-pixbuf2.0-0 libgtk-3-0 libnspr4 libpango-1.0-0 libxss1 fonts-liberation libappindicator1 libnss3 lsb-release xdg-utils

# Install Chrome
RUN wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
RUN dpkg -i google-chrome-stable_current_amd64.deb; apt-get -fy install


# Install Python dependencies.
COPY requirements.txt requirements.txt
RUN pip install -r requirements.txt

# Copy local code to the container image.
ENV APP_HOME /app
WORKDIR $APP_HOME
COPY . ./

# Run the web service on container startup. Here we use the gunicorn
# webserver, with one worker process and 8 threads.
# For environments with multiple CPU cores, increase the number of workers
# to be equal to the cores available.
CMD exec gunicorn --bind :$PORT --workers 1 --threads 8 main:app
</code></pre>

<p>I also added <code>chromedriver-binary==77.0.3865.40.0</code> to my <code>requirements.txt</code> file. </p>

<p>When I try to deploy it using <code>gcloud builds submit</code>, I get this error:</p>

<p><code>/bin/sh: 1: wget: not found
The command '/bin/sh -c wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb' returned a non-zero code: 127
ERROR
ERROR: build step 0 ""gcr.io/cloud-builders/docker"" failed: step exited with non-zero status: 127</code></p>

<p>I'm been trying to find a solution for this error online but I'm lost. Your help would be greatly appreciated!</p>","<p>Add a command to install <code>wget</code>.</p>

<pre><code># Install manually all the missing libraries
RUN apt-get update
RUN apt-get install wget
</code></pre>"
"CloudBuild fastlane task failing with JAVA_HOME is not set and no 'java' command could be found in your PATH<p>I am attempting to run a fastlane build inside <code>Google Cloud Build</code> however I'm getting the following error:</p>

<blockquote>
  <p>ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.</p>
</blockquote>

<p>My first step is to install the Android SDK, which works fine, and then run the fastlane command, however each and every time, no matter what I do in the <code>name</code> before the <code>id:fastlane</code> I get the same Java error. I've downloaded both the Android and Fastlane images from the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders"" rel=""nofollow noreferrer"">Cloud Builder</a> and <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community"" rel=""nofollow noreferrer"">Community Cloud Builders</a> github repo and placed them in our GCP project repository, so each are accessible in the <code>name</code> tasks. </p>

<pre><code>steps:
  # Android SDK
  - name: 'gcr.io/$PROJECT_ID/android:29'
    id: android
    args: [""./gradlew"", ""assembleDebug""]

  # run fastlane
  - name: 'gcr.io/$PROJECT_ID/fastlane'
    id: fastlane
    args: ['distribute_staging', 'signingPassword:${_PASSWORD}', 'firebaseToken:${_TOKEN}']
</code></pre>","<p>Keep in mind that each step are containers. They are loaded one by one, the tasks performed and unloaded. Only the <code>/workspace</code> directory is kept from one step to another one.</p>

<p>Therefore, in your first step you load an android builder and you can build Java stuff because Java is installed in the container. Then unloaded</p>

<p>The second step is Fastlane. Look at the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/blob/master/fastlane/Dockerfile"" rel=""nofollow noreferrer"">Dockerfile</a>, it's a ruby image. No Java inside, thus, your process can't work. You have to build a custom worker</p>

<ul>
<li>Either from the fastlane base image and install Java on it</li>
<li>Or from a Java image (Android?) and install Ruby and fastlane on it.</li>
</ul>"
"Cloudbuild.yaml with enviroment variables?<p>I have cloud build file something like this</p>

<pre><code>steps:

- name: 'python:3.7'
  entrypoint: 'bash'
  args: 
    - '-c'
    - |
        pip3 install -r requirements.txt
        pytest -rP

#- name: 'gcr.io/cloud-builders/gcloud'
#  args:
#  - functions
#  - deploy
#  - Test_Function
#  - --runtime=python37
#  - --source=https://source.developers.google.com/projects/proj_name/repos/repo_name/moveable-aliases/master/paths/function
#  - --entry-point=main
#  - --trigger-topic=Test_Topic
#  - --region=region
</code></pre>

<p>-now what i would like to do is i would like to define soemthing like a variable (Test_name) or in enviroment variables and put that variable in different lines in my yaml like this below.</p>

<pre><code>Test_name: ""my_name""
steps:

- name: 'python:3.7'
  entrypoint: 'bash'
  args: 
    - '-c'
    - |
        pip3 install -r requirements.txt
        pytest -rP

#- name: 'gcr.io/cloud-builders/gcloud'
#  args:
#  - functions
#  - deploy
#  - **{Test_name}**_Function
#  - --runtime=python37
#  - --source=https://source.developers.google.com/projects/proj_name/repos/repo_name/moveable-aliases/master/paths/function
#  - --entry-point=main
#  - --trigger-topic=**{Test_name}**_Topic
#  - --region=region
</code></pre>

<p>how can i do that?</p>","<p>You can make use of Cloud Build's substitutions to achieve that, as stated in the official <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values#using_user-defined_substitutions"" rel=""nofollow noreferrer"">docs</a>.</p>

<p>Following the samples provided in the aforementioned docs, your <code>cloudbuild.yaml</code> will look like:</p>

<pre><code>Test_name: ""my_name""
steps:

- name: 'python:3.7'
  entrypoint: 'bash'
  args: 
    - '-c'
    - |
        pip3 install -r requirements.txt
        pytest -rP

#- name: 'gcr.io/cloud-builders/gcloud'
#  args:
#  - functions
#  - deploy
#  - ${_TEST_NAME}_Function
#  - --runtime=python37
#  - --source=https://source.developers.google.com/projects/proj_name/repos/repo_name/moveable-aliases/master/paths/function
#  - --entry-point=main
#  - --trigger-topic=${_TEST_NAME}_Topic
#  - --region=region

substitutions:
    TEST_NAME: TEST # default value
</code></pre>"
"Deployment failing using Google Cloud Build on to GCS<p>I have a reactjs static app running on Google Cloud Storage. Using Cloud Build, I'm trying to automate the build and deployment process.</p>

<p>Following is the cloudbuild.yaml file:</p>

<pre><code>steps:
  # Install
  - name: 'gcr.io/cloud-builders/npm'
    entrypoint: 'npm'
    args: ['install']

  # Test
  #- name: 'gcr.io/cloud-builders/npm'
    #entrypoint: 'npm'
    #args: ['test', 'test:ci']

  # Build
  - name: 'gcr.io/cloud-builders/npm'
    entrypoint: 'npm'
    args: ['build']

  # Syncing to Static Hosting on GCS  
  - name: 'gcr.io/cloud-builders/gsutil'
    args: [""-m"", ""rsync"", ""-r"", ""-c"", ""-d"", ""./build"", ""gs://mybucket.com""]
</code></pre>

<p>The last step which is Syncing to GCS is failing. I'm getting below error:</p>

<pre><code>Starting Step #0
Step #0: Already have image (with digest): gcr.io/cloud-builders/npm
Step #0: npm WARN saveError ENOENT: no such file or directory, open '/workspace/package.json'
Step #0: npm notice created a lockfile as package-lock.json. You should commit this file.
Step #0: npm WARN enoent ENOENT: no such file or directory, open '/workspace/package.json'
Step #0: npm WARN workspace No description
Step #0: npm WARN workspace No repository field.
Step #0: npm WARN workspace No README data
Step #0: npm WARN workspace No license field.
Step #0: 
Step #0: up to date in 0.711s
Step #0: found 0 vulnerabilities
Step #0: 
Finished Step #0
Starting Step #1
Step #1: Already have image (with digest): gcr.io/cloud-builders/npm
Finished Step #1
Starting Step #2
Step #2: Already have image (with digest): gcr.io/cloud-builders/gsutil
Step #2: CommandException: arg (./build) does not name a directory, bucket, or bucket subdir.
Finished Step #2
ERROR
ERROR: build step 2 ""gcr.io/cloud-builders/gsutil"" failed: step exited with non-zero status: 1
</code></pre>

<p>My project structure:</p>

<pre><code>RootProject
  -&gt; front-end-webapp
           -&gt; src/
           -&gt; cloudbuild.yaml
           -&gt; build/
           -&gt; public/
           -&gt; package.json
  -&gt; backend-service1
  -&gt; backend-service2
</code></pre>

<p>The trigger which I have written points to cloudbuild.yaml file inside the front-end-webapp sub-folder.
When I manually execute </p>

<pre><code>npm run build or yarn build
</code></pre>

<p>then I get production ready files inside build folder under front-end-webapp which I want to put on Google Cloud Storage. Only files inside the build folder and not the build folder itself containing files.</p>

<p>I tried a lot but not able to get it. If I do . instead of './build' in the args like args: [""-m"", ""rsync"", ""-r"", ""-c"", ""-d"", ""./build"", ""gs://mybucket.com""] then cloud build executes successfully but it also copies entires files and sub-folders present in the root project.</p>","<p>The build job is triggered by a file committed into the <code>front-end-webapp</code> and you run the <code>cloudbuild.yaml</code> file which is into <code>front-end-webapp</code>.</p>

<p><strong>BUT</strong>, the root of the repo, is <code>/</code> and not <code>front-end-webapp</code>. Thereby, add the <a href=""https://cloud.google.com/cloud-build/docs/build-config"" rel=""nofollow noreferrer""><code>dir: 'front-end-webapp'</code> attribute on each step</a>. That force the steps to start into the correct folder!</p>

<p><strong>EDIT</strong></p>

<p>When I'm stuck and lost in the directory management, I introduce this step for having a view of what exist in my workspace.</p>

<pre><code>- name: gcr.io/cloud-builders/gcloud:latest
  entrypoint: ""ls""
  args: [""-lah"",""/workspace""]
</code></pre>"
"Adding a “Are you sure …” dialog in GCP cloudbuild.yaml<p>Basically, I want to add a <code>""Are you sure you want to run unit tests ?""</code> dialog before <strong>step 2(run_test_test-coverage)</strong> in my <code>cloudbuild.yaml</code>. How can I do that ? This can be done in jenkins but don't know how to do the same in GCP cloudbuild.</p>

<p>cloudbuild.yaml</p>

<pre><code>steps:
- name: 'node:10.10.0'
  id: installing_npm
  args: ['npm', 'install']
  dir: 'API/system_performance'
- name: 'node:10.10.0'
  id: run_test_test-coverage
  args: ['npm', 'run', 'coverage']
  dir: 'API/system_performance'
</code></pre>

<p>Edit:
Below is my updated cloudbuild.yaml file:</p>

<pre><code>- name: 'node:10.10.0'
  id: installing_npm
  args: ['npm', 'install']
  dir: 'API/groups'
- name: 'gcr.io/cloud-builders/gcloud'
  id: deploy
  dir: '/workspace/API/groups'
  entrypoint: bash
  args: 
   - '-c'
   - |
      if [ $BRANCH_NAME != ""xoxoxoxox"" ] 
      then 
        gcloud functions deploy groups &amp;&amp;\ 
        --region=us-central1 &amp;&amp;\
        --source=. &amp;&amp;\
        --trigger-http &amp;&amp;\ 
        --runtime=nodejs8 &amp;&amp;\ 
        --entry-point=App &amp;&amp;\ 
        --allow-unauthenticated &amp;&amp;\
        --service-account=xoxoxxooxox@appspot.gserviceaccount.com
      fi
</code></pre>

<p>Here, I get the build as successful, but when the condition becomes false, although build gets deployed successfully, yet I get the below output and the the build gets failed. Why so ?</p>

<pre><code>Finished Step #0 - ""installing_npm""
Starting Step #1 - ""deploy""
Step #1 - ""deploy"": Already have image (with digest): gcr.io/cloud-builders/gcloud
Step #1 - ""deploy"": Created .gcloudignore file. See `gcloud topic gcloudignore` for details.
Step #1 - ""deploy"": Deploying function (may take a while - up to 2 minutes)...
Step #1 - ""deploy"": .....................done.
Step #1 - ""deploy"": availableMemoryMb: 256
Step #1 - ""deploy"": entryPoint: App
Step #1 - ""deploy"": httpsTrigger:
Step #1 - ""deploy"":   url: https://xoxoxoxo.cloudfunctions.net/groups
Step #1 - ""deploy"": ingressSettings: ALLOW_ALL
Step #1 - ""deploy"": labels:
Step #1 - ""deploy"":   deployment-tool: cli-gcloud
Step #1 - ""deploy"": name: projects/xoxoxoxo/locations/us-central1/functions/groups
Step #1 - ""deploy"": runtime: nodejs8
Step #1 - ""deploy"": serviceAccountEmail: xoxoxoxo@appspot.gserviceaccount.com
Step #1 - ""deploy"": sourceUploadUrl: https://storage.googleapis.com/xoxoxo
Step #1 - ""deploy"": status: ACTIVE
Step #1 - ""deploy"": timeout: 60s
Step #1 - ""deploy"": updateTime: '2020-05-25T19:18:26.099Z'
Step #1 - ""deploy"": versionId: '12'
Step #1 - ""deploy"": bash: line 2:  : command not found
Step #1 - ""deploy"": bash: line 3: --region=us-central1: command not found
Step #1 - ""deploy"": bash: line 6: --runtime=nodejs8: command not found
Step #1 - ""deploy"": bash: line 7: --entry-point=App: command not found
Step #1 - ""deploy"": bash: line 8: --allow-unauthenticated: command not found
Finished Step #1 - ""deploy""
ERROR
ERROR: build step 1 ""gcr.io/cloud-builders/gcloud"" failed: step exited with non-zero status: 127
</code></pre>","<p>You can't interact with Cloud Build. In fact, you send the file to a CI, and you wait for the result, no more.</p>

<p>But, you can customize the steps. I answered <a href=""https://stackoverflow.com/questions/58235945/google-cloud-build-conditional-step/58239148#58239148"">this question</a> about the conditional step. Use a <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values"" rel=""nofollow noreferrer"">substitution variable</a> for running your build with, or without the tests.</p>

<p>The choice won't be interactive, but at the build submission, you can have the choice.</p>

<p>-> I mean that when you submit the build, for example manually</p>

<pre><code>gcloud build submit --substitions=_SKIPTEST=true
</code></pre>

<p>You choose to skip the tests at the submission, not during the build. </p>

<p><strong>EDIT</strong></p>

<p>When you perform a <code>bash -c |</code> imagine that you are on your linux terminal and enter line by line your commands in sequence. For multiline, add backslash <code>\</code>. Here the <code>&amp;&amp;</code> are useless. Use it for chaining command, but here it's useless because you execute command in sequence.</p>

<p>So here the correct steap</p>

<pre><code>- name: 'gcr.io/cloud-builders/gcloud'
  id: deploy
  dir: '/workspace/API/groups'
  entrypoint: bash
  args: 
   - '-c'
   - |
      if [ $BRANCH_NAME != ""xoxoxoxox"" ] 
      then 
        gcloud functions deploy groups \ 
        --region=us-central1 \
        --source=. \
        --trigger-http \ 
        --runtime=nodejs8 \ 
        --entry-point=App \ 
        --allow-unauthenticated \
        --service-account=xoxoxxooxox@appspot.gserviceaccount.com
      fi
</code></pre>"
"Unity - Distribution build crash - EXC_BAD_ACCESS - KERN_INVALID_ADDRESS<p>I have developed a game on Unity (2019.2.16f1) and I have a <strong>big problem</strong> :
The build submitted to the App Store crash after 1 second (on startup)</p>
<p>Some more informations :</p>
<ul>
<li>I coded some c# script, but most of the game is running with Bolt (Ludiq / Visual scripting)</li>
<li>I use the Unity Cloud Building service (For development and distribution build)</li>
<li>The development build dont crash. Only the distribution build. (The one submitted to the app store connect)</li>
<li>I distribute the ipa file with the mac &quot;Transporter&quot; application</li>
<li>I use UnityAds in this game</li>
</ul>
<p>Here is the main log :</p>
<hr />
<pre><code>Exception Type:  EXC_BAD_ACCESS (SIGSEGV)
Exception Subtype: KERN_INVALID_ADDRESS at 0x000000000000003a
VM Region Info: 0x3a is not in any region.  Bytes before following region: 4367777734
      REGION TYPE                      START - END             [ VSIZE] PRT/MAX SHRMOD  REGION DETAIL
      UNUSED SPACE AT START
---&gt;  
      __TEXT                 0000000104570000-0000000104574000 [   16K] r-x/r-x SM=COW  ....app/z4league
</code></pre>
<hr />
<p>(Full Log : <a href=""https://docs.google.com/document/d/1wjLG5oKqP_isgPce8ND5Nrz7wYtoiC2OIXc1cN9sPy0/edit?usp=sharing"" rel=""nofollow noreferrer"">https://docs.google.com/document/d/1wjLG5oKqP_isgPce8ND5Nrz7wYtoiC2OIXc1cN9sPy0/edit?usp=sharing</a>)</p>
<p>Did someone had this (quite obscur) problem ?
I did some research on google, but nothing seems to be appropriate to my case.</p>","<p>I did not find the <strong>exact</strong> solution of this problem.</p>
<p>What I did is remove &quot;Bolt&quot; from the project and re-code everything in c#.
(By the way, incredible exercise for learning to code c# in Unity)</p>
<p>My game is now validated by Apple and ready to go live</p>"
"Google Cloud Build + Google Secret Manager Substitution Problems<p>we have a repository that needs to <code>go get</code> a private repo. To do this, we are using an SSH key to access the private repo/module.</p>
<p>We are storing this SSH key using <a href=""https://cloud.google.com/secret-manager"" rel=""nofollow noreferrer"">Google Secret Manager</a> and passing it to Docker using the <code>build-arg</code> flag. Now, when we do this locally, the Dockerfile builds and runs as intended. This is the command we use for a local build:</p>
<pre><code>export SSH_PRIVATE_KEY=&quot;$(gcloud secrets versions access latest --secret=secret-data)&quot; &amp;&amp; \
docker build --build-arg SSH_PRIVATE_KEY -t my-image .
</code></pre>
<p>However, when we try to move this setup to Google Cloud Build, we run into 403 forbidden errors from Bitbucket, which leads me to believe that the SSH key is either not being read or formatted correctly.</p>
<p>The full 403 error is:</p>
<pre><code>https://api.bitbucket.org/2.0/repositories/my-repo?fields=scm: 403 Forbidden
Step #0 - &quot;Build&quot;:  server response: Access denied. You must have write or admin access.
</code></pre>
<p>What is even stranger is that when I run the Cloud Build local emulator, it works fine using this command: <code>cloud-build-local --config=builder/cloudbuild-prod.yaml --dryrun=false .</code></p>
<p>I've tried <strong>many</strong> different formats and methods, so out of desperation I am asking the community for help. What could be the problem?</p>
<p>Here is our cloudbuild.yaml:</p>
<pre><code>steps:
# Get secret
  - id: 'Get Secret'
    name: gcr.io/cloud-builders/gcloud
    entrypoint: 'bash'
    args:
      - '-c'
      - |
          gcloud secrets versions access latest --secret=secret-data &gt; /workspace/SSH_PRIVATE_KEY.txt

# Build
  - id: 'Build'
    name: 'gcr.io/cloud-builders/docker'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
          export SSH_PRIVATE_KEY=$(cat /workspace/SSH_PRIVATE_KEY.txt) &amp;&amp;
          docker build --build-arg SSH_PRIVATE_KEY -t my-image .
</code></pre>","<p>Thanks for all the help! This one was pretty weird. Turns out it's not an issue with Cloud Build or Secret Manager but the Dockerfile I was using.</p>
<p>Instead of setting <code>GOPRIVATE</code> with the command in the Dockerfile below, I was using a statement like <code>RUN export GOPRIVATE=&quot;bitbucket.org/odds&quot;</code>.</p>
<p>In case anyone runs into something like this again, here's the full Dockerfile that works.</p>
<pre><code>FROM golang:1.15.1

WORKDIR $GOPATH/src/bitbucket.org/gml/my-srv

ENTRYPOINT [&quot;./my-srv&quot;]

ARG CREDENTIALS

RUN git config \
    --system \
    url.&quot;https://${CREDENTIALS}@bitbucket.org/&quot;.insteadOf \
    &quot;https://bitbucket.org/&quot;

RUN go env -w GOPRIVATE=&quot;bitbucket.org/my-team&quot;

COPY . .

RUN make build
</code></pre>"
"Google cloud build - provide assets to docker build step<p>I'm trying to combine two examples from the Google Cloud Build documentation</p>
<p><a href=""https://cloud.google.com/cloud-build/docs/quickstart-build#build_using_a_build_config_file"" rel=""nofollow noreferrer"">This example where a container is built using a yaml file</a></p>
<p><a href=""https://cloud.google.com/cloud-build/docs/build-config#volumes"" rel=""nofollow noreferrer"">And this example where a persistent volume is used</a></p>
<p>My scenario is simple, the first step in my build produces some assets I'd like to bundle into a Docker image built in a subsequent step. However, since the <a href=""https://docs.docker.com/engine/reference/builder/#copy"" rel=""nofollow noreferrer"">COPY</a> command is <a href=""https://stackoverflow.com/questions/47012495/docker-copy-from-ubuntu-absolute-path/47012599#47012599"">relative to the build directory of the image</a>, I'm unable to determine how to reference the assets from the first step inside the Dockerfile used for the second step.</p>","<p>There are potentially multiple ways to solve this problem, but the easiest way I've found is to use the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/docker"" rel=""nofollow noreferrer""><code>docker</code> cloud builder</a> and run an <code>sh</code> script (since Bash is not included in the docker image).</p>
<p>In this example, we build on the examples from the question, producing an asset <code>file</code> in the first build step, then using it inside the <code>Dockerfile</code> of the second step.</p>
<h2>cloudbuild.yaml</h2>
<pre><code>steps:
- name: 'ubuntu'
  volumes:
  - name: 'vol1'
    path: '/persistent_volume'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
        echo &quot;Hello, world!&quot; &gt; /persistent_volume/file
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'sh'
  volumes:
  - name: 'vol1'
    path: '/persistent_volume'
  args: [ 'docker-build.sh' ]
  env:
    - 'PROJECT=$PROJECT_ID'
images:
- 'gcr.io/$PROJECT_ID/quickstart-image'
</code></pre>
<h2>docker-build.sh</h2>
<pre><code>#/bin/sh
cp -R /persistent_volume .
docker build -t gcr.io/$PROJECT/quickstart-image .
</code></pre>"
"Use different data for production and develop firebase sites<p>I have a CI/DC pipeline with google cloud build triggers that deploy my code to different sites depending on which branch I push to. The develop site is a live test - the final check before I merge to master, which triggers a deploy of master to the production site.</p>
<p>Currently, both sites use the same firebase Firestore db, and any document changed on the develop site will also be changed on the production site.</p>
<p>What I want to avoid is creating another firebase project to push the develop code to with a different database, because that means I need a separate set of credentials and would copy the same functions over to the new project every time I change them. That's not maintainable and is a lot of work.</p>
<p>What I would like is some way for the develop site to only have access to part of the firestore database, and the production site to have access to another part.</p>
<p>How do people do this? Is it even possible? Is there a better way? One alternative I can think of is using authentication and creating separate accounts for testing with different access permissions, but this seems a work-around and not the ideal solution.</p>","<p>What you're trying to do sounds like a lot more hassle than <a href=""https://firebase.google.com/docs/projects/multiprojects"" rel=""nofollow noreferrer"">using multiple projects</a>, which is the documented and strongly preferred solution.  Putting everything in one project is a huge anti-pattern in Firebase and Google Cloud, and it will cause you more problems in the long run, in addition to increasing the risk of catastrophic failure if you manage to misconfigure something in that one project.</p>
<p>It's perfectly maintainable to have multiple projects like this, if you apply some scripting to automate the work.  This is very common, and I strongly suggest thinking through how this would work for you.</p>
<p>You CI/CD pipeline could definitely check out your updates from source control and deploy them to whatever other project environments you have set up.  It's very common to manage different credentials and configurations for use in CI/CD.</p>"
"Unexpected Error on Google Cloud Build Deploy to Firebase Hosting<p>I have previously managed to deploy small projects to firebase hosting with google cloud build. My current project includes functions, but my deploy script is --only:hosting</p>
<p>I understand this is similar to these posts: <a href=""https://stackoverflow.com/questions/62607465/cannot-deploy-from-gcp-cloud-build-to-firebase-hosting"">stackoeverflow1</a> <a href=""https://stackoverflow.com/questions/55754674/google-cloud-built-not-substituting-environment-variable-for-firebase-token/55762821#55762821"">stackoverflow2</a> and <a href=""https://stackoverflow.com/questions/60552244/google-cloud-build-firebase-deploy-an-unexpected-error-has-occurred"">stackoverflow3</a> - but I am using functions and kms not secrets, and I have logged in to firebase ci to get my token. Nothing I've seen points the way clearly enough so far!</p>
<p>I am using google kms cryptography and all my permissions are set up correctly. Correct accounts have firebase admin permissions. Trigger variables are defined correctly.</p>
<p>My .env file contains a parameter FIREBASE_TOKEN that I got by doing firebase login:ci</p>
<p>Here is my cloudbuild.yaml</p>
<pre><code>steps:
    # Install
    - name: 'gcr.io/cloud-builders/npm'
      args: ['install']
    # Build
    - name: 'gcr.io/cloud-builders/npm'
      args: ['run', 'build']
    #Decrypt
    - name: 'gcr.io/cloud-builders/gcloud'
      args: ['kms', 'decrypt', '--ciphertext-file=.env.enc', '--plaintext-file=.env', '--location=global', '--keyring=$_KEY_RING', '--key=$_KEY']
    # Deploy
    - name: 'gcr.io/$_PROJECT_ENV/firebase'
      args: ['deploy', '--debug', '--token=$FIREBASE_TOKEN', '--only=hosting:$_DEPLOY_TO', '--project', '$_PROJECT_ENV']
</code></pre>
<p>This runs fine until right at the end, when it gives these messages:</p>
<pre><code>Step #3: === Deploying to 'dev-xxxxx'...
Step #3: 
Step #3: i  deploying hosting 
Step #3: 
Step #3: ✔  Deploy complete! 
Step #3: 
Step #3: Project Console: https://console.firebase.google.com/project/dev-xxxxx/overview
Step #3: [2020-09-27T15:51:57.341Z] TypeError: Cannot read property 'deploys' of undefined
Step #3:     at /usr/local/lib/node_modules/firebase-tools/lib/deploy/index.js:85:36
Step #3:     at processTicksAndRejections (internal/process/task_queues.js:93:5)
Step #3: 
Step #3: Error: An unexpected error has occurred.
</code></pre>
<p>Tearing my hair out trying to find what could possibly be wrong? Not getting any permissions errors and firebase image is the latest and builds fine? Any suggestions?</p>","<p>This error message might be produced for deployments that don't have the <code>target</code> key and value defined in the <code>firebase.json</code> file, and for deployments that have incorrect <code>target</code> values.</p>
<p>According to the <a href=""https://firebase.google.com/docs/cli/targets#configure_your_firebasejson_file_to_use_deploy_targets"" rel=""nofollow noreferrer"">Firebase documentation</a>:</p>
<blockquote>
<p>In your <code>firebase.json</code> file, reference the associated target name when
you're configuring the settings for each resource or group of
resources.</p>
</blockquote>"
"Is it possible to start PubSub Emulator from Cloud Build step<p>As the title mentions, I would like to know if, from a Cloud Build step, I can start and use the pubsub emulator?</p>
<pre><code>options:
  env:
    - GO111MODULE=on
    - GOPROXY=https://proxy.golang.org
    - PUBSUB_EMULATOR_HOST=localhost:8085
  volumes:
    - name: &quot;go-modules&quot;
      path: &quot;/go&quot;

steps:
  - name: &quot;golang:1.14&quot;
    args: [&quot;go&quot;, &quot;build&quot;, &quot;.&quot;]

  # Starts the cloud pubsub emulator
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args: [
      '-c',
      'gcloud beta emulators pubsub start --host-port 0.0.0.0:8085 &amp;'
    ]

  - name: &quot;golang:1.14&quot;
    args: [&quot;go&quot;, &quot;test&quot;, &quot;./...&quot;]
</code></pre>
<p>For a test I need it, it works locally and instead of using a dedicated pubsub from cloud build, I want to use an emulator.</p>
<p>Thanks</p>","<p>As I found a workaround and an <a href=""https://github.com/Philmod/gcb-docker-compose"" rel=""nofollow noreferrer"">interesting git repository</a>, I wanted to share with you the solution.</p>
<p>As required, you need a <code>cloud-build.yaml</code> and you want to add a step where the emulator will get launched:</p>
<pre><code>options:
  env:
    - GO111MODULE=on
    - GOPROXY=https://proxy.golang.org
    - PUBSUB_EMULATOR_HOST=localhost:8085
  volumes:
    - name: &quot;go-modules&quot;
      path: &quot;/go&quot;

steps:
  - name: &quot;golang:1.14&quot;
    args: [&quot;go&quot;, &quot;build&quot;, &quot;.&quot;]

  - name: 'docker/compose'
    args: [
        '-f',
        'docker-compose.cloud-build.yml',
        'up',
        '--build',
        '-d'
    ]
    id: 'pubsub-emulator-docker-compose'

  - name: &quot;golang:1.14&quot;
    args: [&quot;go&quot;, &quot;test&quot;, &quot;./...&quot;]
</code></pre>
<p>As you can see, I run a docker-compose command which will actually start the emulator.</p>
<pre><code>version: &quot;3.7&quot;

services:
  pubsub:
    # Required for cloudbuild network access (when external access is required)
    container_name: pubsub
    image: google/cloud-sdk
    ports:
      - '8085:8085'
    command: [&quot;gcloud&quot;, &quot;beta&quot;, &quot;emulators&quot;, &quot;pubsub&quot;, &quot;start&quot;, &quot;--host-port&quot;, &quot;0.0.0.0:8085&quot;]
    network_mode: cloudbuild

networks:
  default:
    external:
      name: cloudbuild
</code></pre>
<p>It is important to set the container name as well as the network, otherwise you won't be able to access the pubsub emulator from another cloud build step.</p>"
"google cloud build syntax<p>I'm working on my first cloudbuild.yaml file and running into this error:</p>
<blockquote>
<p>Your build failed to run: failed unmarshalling build config cloudbuild.yaml: yaml: line 8: did not find expected key</p>
</blockquote>
<p>Here are the contents of my file (comments omitted), I have a few questions afterwards:</p>
<pre><code>steps:
- name: 'node:12-alpine'
    entrypoint: 'bash'
    args:
        - 'build.sh'
- name: 'docker'
    args:
    - 'build'
    - '-t'
    - 'gcr.io/$PROJECT_ID/my-project:$(git describe --tags `git rev-list --tags --max-count=1`)'
images: ['gcr.io/$PROJECT_ID/my-project']
</code></pre>
<p>Questions:</p>
<ul>
<li>The line with <code>- name: 'node:12-alpine'</code> seems to be where it's blowing up. However, the documentation states, <a href=""https://cloud.google.com/cloud-build/docs/cloud-builders#publicly_available_images"" rel=""nofollow noreferrer"">&quot;Cloud Build enables you to use any publicly available image to execute your tasks.&quot;</a>. The <a href=""https://github.com/nodejs/docker-node/blob/fdd2b251827817ac7bb4f0b2b082483abff8ea77/12/alpine3.11/Dockerfile"" rel=""nofollow noreferrer"">node:12-alpine imgage</a> is publicly available so what am I doing wrong?</li>
<li>Secondly, I'm trying to execute a file with a bunch of BASH commands in the first step. That should work, provided the commands are all supported by the Alpine image I'm using, right?</li>
<li>Lastly, I'm trying to create a docker image with a version number based on the version of the latest git tag. Is syntax like this supported, or how is versioning normally handled with google cloud build (I saw nothing on this topic looking around)</li>
</ul>","<p>When you run a container into Cloud Build, the entry point defined into the container is automatically called, and the <code>args</code> are passed in parameter of this entry point.</p>
<p>What you have to know</p>
<ul>
<li>You can override the entrypoint, as you did in the node:12 image</li>
<li>If the container doesn't contain an entrypoint, the build failed (Your error, you use a generic docker image). You can
<ul>
<li>Either define the correct entrypoint (here <code>entrypoint: &quot;docker&quot;</code>)</li>
<li>Or use a <a href=""https://github.com/GoogleCloudPlatform/cloud-builders"" rel=""nofollow noreferrer"">Cloud Builder</a>, for docker, this one <code>- name: 'gcr.io/cloud-builders/docker'</code></li>
</ul>
</li>
<li>The steps' args are forwarded as-is, without any interpretation (except variable replacement like $MyVariable). Your command interpretation <code>$(my command)</code> isn't evaluated. Except is you do this</li>
</ul>
<pre><code>- name: 'gcr.io/cloud-builders/docker' #you can also use the raw docker image here
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      first bash command line
      second bash command line
      docker build -t gcr.io/$PROJECT_ID/my-project:$(git describe --tags `git rev-list --tags --max-count=1`)
</code></pre>
<p>But you can get the tag smarter. If you look at the <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values"" rel=""nofollow noreferrer"">default environment variable of Cloud Build</a>, you can use <code>$TAG_NAME</code>.</p>
<pre><code>docker build -t gcr.io/$PROJECT_ID/my-project:$TAG_NAME
</code></pre>
<p>Be careful, it's true only if you trigger it from your repository. If you run a manual build it doesn't work. So, there is a workaround. Look at this</p>
<pre><code>- name: 'gcr.io/cloud-builders/docker' 
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      TAG=${TAG_NAME}
      if [ -z $${TAG} ]; then TAG=$(git describe --tags `git rev-list --tags --max-count=1`); fi      
      docker build -t gcr.io/$PROJECT_ID/my-project:$${TAG}
</code></pre>
<p>However, I don't recommend you to override your images. You will lost the history, and if you override a good image with a wrong version, you loose it!</p>
<p><em>If you didn't catch some part, like why the double $$ and so on, don't hesitate to comment</em></p>"
"How to run tests for a node app deployed on Cloud Run with Cloud Build<p>I have a nodejs  API running on Google cloud Run and am using Cloud Build for continuous Deployment.</p>
<p>I would need to run tests so that in case of errors found, cloud Build does not deploy the image to cloud Run. How can I implement this either in Dockerfile or using anyother way.</p>","<p>You have 2 solutions</p>
<ul>
<li>Either you start your API in background and run tests on it.</li>
<li>Or, if you want to test the container, deploy the container on Cloud Run, in a staging instance and test it. If the tests are ok, continue and deploy the container on the correct service.</li>
</ul>
<p>You can't run a container inside Cloud Build (Docker in Docker aren'r permitted. In fact, you can run the container, but the port forwarding is forbidden, so you can test your API with the container started!)</p>"
"how to access content across successive build steps (including in docker steps) in google cloudbuild<p>Within Google Cloud Platform, in a multistage cloudbuild process it is possible to use the shared <code>/workspace</code> to access content from one build step to the next.
For instance the following cloudbuild.yaml would print &quot;hello world&quot; where the text hello is accessed from the first build step.</p>
<pre><code>steps:
  - name: gcr.io/cloud-builders/gcloud
    entrypoint: 'bash'
    args: [ '-c', &quot; echo world &gt; sample.txt&quot; ]
  - name: gcr.io/cloud-builders/gcloud
    entrypoint: 'bash'
    args: [ '-c', &quot;echo hello $( &lt; sample.txt)&quot; ]
</code></pre>
<p>However, it seems this method does not work when it comes to accessing this content in the docker build step. There does not appear to be a way to access files within the docker build step.</p>
<p>The following step, when executed as the third step subsequent to the above steps executes a Dockerfile but doesn't appear to make the sample.txt file available to commands within it.</p>
<pre><code>  # Build the container image
  - name: gcr.io/cloud-builders/docker
    args: ['build',
           '-t', 'repositoryname:latest',
           '.']
</code></pre>
<p>I've tried the following:</p>
<ul>
<li>looking for the default directory named workspace within the container, to no avail.</li>
<li>creating a custom volume shared across all steps (the custom volume does not appear to be accessible within the Dockerfile scope.</li>
</ul>
<p>Is it possible in cloudbuild to progressively build content in a series of steps including docker steps? If so, how could this be done?</p>
<h1>Update</h1>
<p>Actually it appears the files do remain in a workspace environment. My problem was in not being able to use the content of the file in the build command as follows.</p>
<pre><code>  # Build the container image
  - name: gcr.io/cloud-builders/docker
    args: ['build',
           '--build-arg', 'GITHUB_AUTH_TOKEN=$( &lt; sample.txt)',
           '-t', '$_REPO_PATH/$_PROJECT_ID/$_REPO_NAME:$_TAG_NAME',
           '.']
</code></pre>
<p>In the first step I am actually using gcloud to get a secret from the secret manager, I then wanted to pass this as an argument to the docker build process.
I could not get the above build step to work, and then I read that command substition does not work on the docker argument?</p>","<p>The problem comes from the <code>args</code> interpretation. Written like this, there isn't any arg evaluation. Use script mode like this</p>
<pre><code>  # Build the container image
  - name: gcr.io/cloud-builders/docker
    entrypoint: &quot;bash&quot;
    args: 
      - &quot;-c&quot;
      - |
          docker build --build-arg GITHUB_AUTH_TOKEN=$( &lt; sample.txt) -t $_REPO_PATH/$_PROJECT_ID/$_REPO_NAME:$_TAG_NAME .

</code></pre>"
"How to make a string compare work in CloudBuild?<p>I have a simple string test in my GCP <em>CloudBuild</em> step, but it never works. The step looks like this</p>
<pre><code>steps:

- id: 'branch name'
  name: 'alpine'
  entrypoint: 'sh'  
  args: 
  - '-c'
  - | 
      export ENV=$BRANCH_NAME
      if [ $ENV = &quot;master&quot; ]; then
         export ENV=&quot;test-dev&quot;
      fi
      echo &quot;***********************&quot;
      echo &quot;$BRANCH_NAME&quot;
      echo &quot;$ENV&quot;
      echo &quot;***********************&quot;     
</code></pre>
<p><em>CloudBuild</em> always reports this as <code>sh: master: unknown operand</code>. It's a literal, obviously.
I put the same code into a little <code>sh</code> script and it ran fine as long as I set a value for <code>BRANCH_NAME</code>. <em>CloudBuild</em> definitely supplies a value for <code>BRANCH_NAME</code> and it shows up in the <code>echo &quot;$BRANCH_NAME&quot;</code> while the <code>echo &quot;$ENV&quot;</code> is always empty.</p>
<p>Is there a way to make this string compare work?</p>","<p>When you use linux <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values#using_user-defined_substitutions"" rel=""nofollow noreferrer"">env var and not substitution variables (or predefined variables), you have to escape the <code>$</code> with another one</a></p>
<pre><code>steps:

- id: 'branch name'
  name: 'alpine'
  entrypoint: 'sh'  
  args: 
  - '-c'
  - | 
      export ENV=$BRANCH_NAME
      if [ $$ENV = &quot;master&quot; ]; then
         export ENV=&quot;test-dev&quot;
      fi
      echo &quot;***********************&quot;
      echo &quot;$BRANCH_NAME&quot;
      echo &quot;$$ENV&quot;
      echo &quot;***********************&quot;     
</code></pre>"
"Your build failed to run: Couldn't read commit xxxxxxxx<p>I'm trying to build and deploy an image on GCP with cloud build.
I have a rails api repo on bitbucket running on docker, the repo is synced with google cloud repository</p>
<p>I configured a trigger on cloud build when a commit is made on my master branch.</p>
<p>Trigger Configuration:</p>
<p><a href=""https://i.stack.imgur.com/XKPTe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XKPTe.png"" alt=""Trigger Configuration"" /></a></p>
<p>Service account permissions:</p>
<p><a href=""https://i.stack.imgur.com/gooYM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gooYM.png"" alt=""Service account permissions"" /></a></p>
<p>But when the master branch gets a commit, the triggers returns the following error:
<code>our build failed to run: &lt;br&gt;Couldn't read commit xxxxxxxx</code></p>
<p>Build Error:</p>
<p><a href=""https://i.stack.imgur.com/irmbk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/irmbk.png"" alt=""Build Error"" /></a></p>
<p>I tried checked the GCP doc and I can't find anything. I think the issue might be on the IAM level, maybe the service account needs more credentials.</p>","<p>I actually edited the IAM setting for this service account to add:
Project &gt; Editor
Source Repository &gt; Reader</p>
<p>Now it works</p>"
"How can I make sure files in a Storage Bucket are being served gzipped on my website?<p>I have a setup like this in Google Cloud Platform for my website, a Gatsbyjs project:</p>
<ol>
<li>Push to repository</li>
<li>Trigger CloudBuild that builds my Gatsby website into public folder</li>
<li>Copy the files within the &quot;public&quot; folder to a Storage Bucket, using rsync</li>
</ol>
<p>However, when I visit my site (LoadBalancer connected to the Bucket), the .js,.css, .html files are not being served as gzip.
I know there seems to be a flag on <code>cp</code> command for gzip, but how does this work for <code>rsync</code>?</p>
<p>Thanks</p>","<p>If you want to &quot;compress on the fly&quot; your files with rsync this is not possible.</p>
<p>However, if you just want to apply to gzip transport encoding to certain files you can use the -j &lt;...&gt; option of <a href=""https://cloud.google.com/storage/docs/gsutil/commands/rsync"" rel=""nofollow noreferrer"">rsync</a>. This will saves network bandwidth but is going to leave the data uncompressed in the Storage bucket.</p>
<p>However, if you want to take uncompressed files from the public folder and send them to a bucket and keep them compressed you will need to do a <code>gsutil cp -z</code> <a href=""https://cloud.google.com/storage/docs/gsutil/commands/cp"" rel=""nofollow noreferrer"">command</a>. This will compress your file (if they are not compressed in your public folder) and store them compressed in the bucket</p>"
"Standard Go Project Layout with Docker causes build context problem<p>I'm following two guides which appear to conflict</p>
<ol>
<li><a href=""https://github.com/golang-standards/project-layout"" rel=""nofollow noreferrer"">Standard Go Project Layout</a></li>
<li><a href=""https://github.com/GoogleCloudPlatform/golang-samples/tree/master/endpoints/getting-started-grpc"" rel=""nofollow noreferrer"">Google Cloud Endpoints Sample for Go using gRPC</a></li>
</ol>
<p>Go's <a href=""https://github.com/golang-standards/project-layout"" rel=""nofollow noreferrer"">Standard Go Project Layout</a> recommends a <code>/build</code> directory, the use of which is <a href=""https://github.com/golang-standards/project-layout/tree/master/build"" rel=""nofollow noreferrer"">described</a> as</p>
<blockquote>
<p><em>Packaging and Continuous Integration.</em></p>
<p><em>Put your cloud (AMI), container (Docker), OS (deb, rpm, pkg) package</em>
<em>configurations and scripts in the /build/package directory.</em></p>
</blockquote>
<p>Following <a href=""https://github.com/GoogleCloudPlatform/golang-samples/tree/master/endpoints/getting-started-grpc"" rel=""nofollow noreferrer"">Google Cloud Endpoints Sample for Go using gRPC</a>, I have a Dockerfile which copies application source code and installs any dependencies with <code>go get</code>.</p>
<pre class=""lang-sh prettyprint-override""><code># /build/Dockerfile

FROM golang:alpine

# Alpine Linux package management
RUN apk update
RUN apk add git

COPY ../ /go/src/github.com/username/repository

# Downloads the packages named by the import paths, along with their
# dependencies. It then installs the named packages, like 'go install'.
# ****Don't do this in production! Use vendoring instead.****
RUN go get -v github.com/username/repository/foo-module

# Compiles and installs the packages named by the import paths.
RUN go install github.com/username/repository/foo-module

ENTRYPOINT [&quot;/go/bin/foo-module&quot;]
</code></pre>
<p>Following <a href=""https://github.com/golang-standards/project-layout"" rel=""nofollow noreferrer"">Standard Go Project Layout</a> I place the aforementioned Dockerfile in <code>/build</code>.</p>
<p>Since the Dockerfile is now in the <code>/build</code> directory, I modified the <code>COPY</code> command to copy the <strong>parent</strong> directory to find the application source code (<code>COPY ../ /go/src/github.com/username/repository</code>).</p>
<p>The Docker <code>build</code> command is <strong>not run directly</strong>, rather the build is started with <a href=""https://cloud.google.com/cloud-build/docs/overview"" rel=""nofollow noreferrer"">Google Cloud Build</a></p>
<pre class=""lang-sh prettyprint-override""><code>cloud-build-local --config=build/cloudbuild.yaml .
</code></pre>
<p>The <code>cloudbuild.yaml</code> file is <em>really simple</em> and just kicks off the Docker build. The <code>--file</code> flag points to <code>build/Dockerfile</code> since the Cloud Build command is started from the project root and not from <code>/build</code>)</p>
<pre class=""lang-yaml prettyprint-override""><code># /build/cloudbuild.yaml

steps:
    - id: Build
      name: &quot;gcr.io/cloud-builders/docker&quot;

      dir: ${_SOURCE}
      args:
        [
            &quot;build&quot;,
            &quot;--file build/Dockerfile&quot;,
            &quot;.&quot;,
        ]
</code></pre>
<p>As expected, this fails</p>
<pre><code>COPY failed: Forbidden path outside the build context: ../ ()
</code></pre>
<p><a href=""https://stackoverflow.com/questions/27068596/how-to-include-files-outside-of-dockers-build-context"">How to include files outside of Docker's build context?</a> suggests using the <code>--file</code> flag, however, this assumes the build is started from the root context. When using Google's Cloud Build, the build is started from <code>cloudbuild.yaml</code>, which also located in <code>/build</code>.</p>
<p>I could just place the Docker file in the root of my go module, but I would like to follow best practices where possible and keep <code>cloudbuild.yaml</code> and <code>Dockerfile</code> in <code>/build</code>.</p>
<p>What is the correct way to achieve this while following the <a href=""https://github.com/golang-standards/project-layout"" rel=""nofollow noreferrer"">Standard Go Project Layout</a>?</p>","<p>That's a rather long question, so lets focus on the problem encountered:</p>
<blockquote>
<pre><code>COPY ../ /go/src/github.com/username/repository
</code></pre>
</blockquote>
<p>which resulted in</p>
<blockquote>
<pre><code>COPY failed: Forbidden path outside the build context: ../ ()
</code></pre>
</blockquote>
<p>You don't include files outside of the context, docker doesn't allow that. Instead you change your context to include the files you need to build your image, and make the paths in your COPY/ADD commands relative to that context. Make sure to reread that, <strong>these paths are not relative to the Dockerfile location</strong>.</p>
<p>So with a <code>docker build</code>, if you have <code>build/Dockerfile</code>, you would build from the parent directory with:</p>
<pre><code>docker build -f build/Dockerfile .
</code></pre>
<p>That trailing dot is the path to the build context which gets sent to the docker engine to perform the build. It doesn't access files directly on the client, which is why you can't include files from outside of the context (plus you don't want malicious Dockerfiles extracting data from the build server).</p>
<p>Then inside the Dockerfile you'd define the paths relative to that context:</p>
<pre><code>COPY . /go/src/github.com/username/repository
</code></pre>
<p>And if for some reason you cannot build from that parent directory because of the tooling, then make the context the parent folder with a relative path:</p>
<pre><code>docker build -f Dockerfile ..
</code></pre>"
How to ssh to another server from GCP<p>I need ssh to another server with SSH from Google Cloud Build. I try run this. But it's not working. <code>echo ${_KEY_STAGING} &gt;&gt; keyStaging &amp;&amp; ssh -i keyStaging phihoang@${_SERVER_STAGING} -p 2222</code></p>,"<p>Based on the error message that you are receiving:</p>
<pre><code> Permissions 0644 for '/Users/tudouya/.ssh/vm/vm_id_rsa.pub' are too open. It is required that your private key files are NOT accessible by others
</code></pre>
<p>It is necessary to run the following command</p>
<pre><code>chmod 600 /Users/tudouya/.ssh/vm/vm_id_rsa.pub
</code></pre>
<p>This command must be executed by the user <code>tudouya</code> and will be remove access for other users, fixing your issue.</p>"
"GCP Cannot enable Cloud Build on GCP Console with Roles: Owner, Service Management Administrator, Service Usage Admin<p>Not able to enable Cloud Build on the project that I am assigned as Owner. I get an error message: &quot;You are missing the required permission: billing.accounts.list&quot; as in following screen-shot</p>
<p><a href=""https://i.stack.imgur.com/nTfAO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nTfAO.png"" alt=""enter image description here"" /></a></p>","<p>I solved the issue by assigning the roles with gcloud on the terminal.
There must be some glitches on Google Console.</p>
<pre><code>gcloud projects add-iam-policy-binding $GC_PROJECT \  
--member &quot;serviceAccount:$GC_PROJECT_NUMBER@cloudbuild.gserviceaccount.com&quot; \
--role roles/run.admin

gcloud iam service-accounts add-iam-policy-binding \
$GC_PROJECT_NUMBER-compute@developer.gserviceaccount.com \
--member=&quot;serviceAccount:$GC_PROJECT_NUMBER@cloudbuild.gserviceaccount.com&quot; \
--role=&quot;roles/iam.serviceAccountUser&quot;

gcloud projects add-iam-policy-binding $GC_PROJECT \
--member &quot;serviceAccount:$GC_PROJECT_NUMBER@cloudbuild.gserviceaccount.com&quot; \
--role roles/owner
</code></pre>"
"Cloud Run service in GCP shows ""Container failed to start and then listen on the port defined by the PORT environment variable"" Codeigniter 4<p>I am trying continuous deployment of codeigniter 4 application from github repo using cloud build in GCP, but it shows failed to listen port but I used port environment variable, Help me with this error</p>
<p>My Docker File</p>
<pre><code>FROM php:fpm

RUN [&quot;apt-get&quot;, &quot;update&quot;]
RUN [&quot;apt-get&quot;, &quot;install&quot;, &quot;-y&quot;, &quot;libzip-dev&quot;]
RUN [&quot;apt-get&quot;, &quot;install&quot;, &quot;-y&quot;, &quot;zip&quot;]
RUN [&quot;apt-get&quot;, &quot;install&quot;, &quot;-y&quot;, &quot;unzip&quot;]
RUN [&quot;apt-get&quot;, &quot;install&quot;, &quot;-y&quot;, &quot;libxml2-dev&quot;]
RUN [&quot;docker-php-ext-install&quot;, &quot;soap&quot;]
RUN [&quot;docker-php-ext-configure&quot;, &quot;zip&quot;]
RUN [&quot;docker-php-ext-install&quot;, &quot;mysqli&quot;, &quot;pdo&quot;, &quot;pdo_mysql&quot;, &quot;zip&quot;]

RUN touch /usr/local/etc/php/conf.d/uploads.ini \
    &amp;&amp; echo &quot;upload_max_filesize=64M;\r\n post_max_size=128M;\r\nmemory_limit = 512M&quot; &gt;&gt; /usr/local/etc/php/conf.d/uploads.ini

RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer

COPY ./writable /var/www/html/writable

## Install codeIgniter Dependecies
COPY ./composer.json /var/www/html/composer.json
RUN cd /var/www/html/ &amp;&amp; composer update /var/www/html/ --no-dev --ignore-platform-reqs &quot;vendor/*&quot;

RUN cd /var/www/html/
#RUN [&quot;chmod&quot;, &quot;-R&quot;, &quot;777&quot;, &quot;writable/&quot;]
RUN [&quot;chown&quot;, &quot;-R&quot;, &quot;www-data:www-data&quot;, &quot;/var/www/html/&quot;]
########################################

EXPOSE ${PORT}
</code></pre>
<p>My composer.json file</p>
<pre><code>{
  &quot;name&quot;: &quot;codeigniter4/appstarter&quot;,
  &quot;type&quot;: &quot;project&quot;,
  &quot;description&quot;: &quot;CodeIgniter4 starter app&quot;,
  &quot;homepage&quot;: &quot;https://codeigniter.com&quot;,
  &quot;license&quot;: &quot;MIT&quot;,
  &quot;require&quot;: {
    &quot;php&quot;: &quot;&gt;=7.2&quot;,
    &quot;codeigniter4/framework&quot;: &quot;^4&quot;
  },
  &quot;require-dev&quot;: {
    &quot;mikey179/vfsstream&quot;: &quot;1.6.*&quot;,
    &quot;phpunit/phpunit&quot;: &quot;8.5.*&quot;
  },
  &quot;autoload-dev&quot;: {
    &quot;psr-4&quot;: {
      &quot;Tests\\Support\\&quot;: &quot;tests/_support&quot;
    }
  },
  &quot;scripts&quot;: {
    &quot;test&quot;: &quot;phpunit&quot;,
    &quot;post-update-cmd&quot;: [
      &quot;@composer dump-autoload&quot;
    ]
  },
  &quot;support&quot;: {
    &quot;forum&quot;: &quot;http://forum.codeigniter.com/&quot;,
    &quot;source&quot;: &quot;https://github.com/codeigniter4/CodeIgniter4&quot;,
    &quot;slack&quot;: &quot;https://codeigniterchat.slack.com&quot;
  }
}
</code></pre>","<pre><code>FROM php:7.3-apache

RUN apt-get update
RUN apt-get upgrade -y

RUN apt-get install --fix-missing -y libpq-dev
RUN apt-get install --no-install-recommends -y libpq-dev
RUN apt-get install -y libxml2-dev libbz2-dev zlib1g-dev
RUN apt-get -y install libsqlite3-dev libsqlite3-0 mariadb-client curl exif ftp
RUN docker-php-ext-install intl
RUN apt-get -y install --fix-missing zip unzip

RUN curl -sS https://getcomposer.org/installer | php
RUN mv composer.phar /usr/local/bin/composer
RUN chmod +x /usr/local/bin/composer
RUN composer self-update

RUN sed -i 's/80/${PORT}/g' /etc/apache2/sites-available/000-default.conf /etc/apache2/ports.conf

ADD conf/apache.conf /etc/apache2/sites-available/000-default.conf

RUN a2enmod rewrite

RUN printf &quot;#!/bin/bash\n/usr/sbin/apache2ctl -D FOREGROUND&quot; &gt; /startScript.sh
RUN chmod +x /startScript.sh

RUN cd /var/www/html
COPY . /var/www/html/
COPY ./.env.example /var/www/html/.env
COPY ./composer.json /var/www/html/composer.json

RUN composer update /var/www/html/ --no-dev --ignore-platform-reqs &quot;vendor/*&quot;
RUN chmod -R 0777 /var/www/html/writable

RUN apt-get clean \
    &amp;&amp; rm -r /var/lib/apt/lists/*

EXPOSE ${PORT}
VOLUME [&quot;/var/www/html&quot;, &quot;/var/log/apache2&quot;, &quot;/etc/apache2&quot;]

CMD [&quot;bash&quot;, &quot;/startScript.sh&quot;]
</code></pre>
<p>I used apache instead of fpm and it works but I still don't know why fpm is not working</p>"
"GCP Cloud Build: No longer able to build latest commits<p>We use Bitbucket for our source
We use GCP Cloud Build for building and deploying
Since last couple of days, auto-trigger is not working as in it doesn't start the build
If I manually trigger it, it pick the last commit it successfully built and deployed</p>
<p>Trigger is set to &quot;Push To Branch&quot; with filter of branch name, one for develop and other for master</p>
<p>I think it could be because we migrated our domain and Google cloud build no longer has access to repository? But then how come it keeps working on last commit?</p>","<p>I finally figured out what was happening. The user that was used to authenticate connection to external repository in google cloud repository was removed from bitbucket. To fix it, I had to disconnect the repository (I was afraid I would have to setup triggers and everything else again) and reconnect while I was logged into Bitbucket. It used my credentials and preserved all other previous configurations which was great! I didn't find any other way to change credentials.</p>"
"Google Cloud Build failed to retrieve Github repositories<p>I am trying to set up a cloud build trigger from a public github repository with the Cloud Build GitHub App. I installed the app on my repository and authorized it but when I was redirected to GCP to connect the repository to a project this error message came up:</p>
<p>Failed to retrieve GitHub repositories.
The caller does not have permission</p>
<p><a href=""https://i.stack.imgur.com/M6yuG.png"" rel=""nofollow noreferrer"">error</a></p>
<p>I suspect it may have something to do with having two factor authentication enabled on my github account, which I need for an organization.</p>
<p>I was able to mirror the same github repository from cloud source repositories without any issues though. I am the owner of the repository and gcp project.</p>
<p>*edit<br />
Looks like the issue is due to having 2 factor authentication enabled on my github account. I disabled it and cloud build was able to connect with my repository. However I will need to have 2 factor enabled as my github organization requires it.</p>
<p>*edit<br />
I hadn't mentioned the github organization i was part of had an ip whitelist configured on top of requiring 2 factor auth. I left the organization and reenabled 2 factor auth and cloud build was able to connect to my repo. Not sure why I would get the original issue if the repo is not in the github organization.</p>","<p>After looking more into this problem you either need to add GCE IP address ranges to the github organization IP whitelist <a href=""https://cloud.google.com/compute/docs/faq#find_ip_range"" rel=""nofollow noreferrer"">https://cloud.google.com/compute/docs/faq#find_ip_range</a> or just disable the whitelist if able to.</p>"
"Steps for Cloud Function + Cloud Build API + Node 10?<p>As I received email from Firbase to upgrade Firebase Cloud Function with Node 10 and Cloud Build API.</p>
<p><a href=""https://i.stack.imgur.com/ZZKKq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZZKKq.png"" alt=""enter image description here"" /></a></p>
<p>I have done all the steps as below:</p>
<ul>
<li>Updated to the &quot;Blaze&quot; pay-as-you-go Plan</li>
<li>Updated Node.js 10 runtime</li>
<li>Enabled Cloud Build API</li>
</ul>
<p>Now what?</p>
<p>I am just sending notification when some data get updated in <strong>Cloud Firestore</strong> in my old cloud function.</p>
<p>Will the same function will work which I have used before? or Do I need to do something with Cloud Build API?</p>
<p>Can anyone please suggest me?</p>","<blockquote>
<p>Now what?</p>
</blockquote>
<p>If you did everything requested, and everything works as you expect, then you're done.</p>
<blockquote>
<p>Will the same function will work which I have used before?</p>
</blockquote>
<p>It should.  Test it and see if it works.  If it doesn't, redeploy it and try again.</p>
<blockquote>
<p>or Do I need to do something with Cloud Build API?</p>
</blockquote>
<p>There's nothing you need to do.  The Cloud Build process is managed by the Firebase CLI.</p>"
"Helm Fail In Cloud Build<p>I'm using <code>alpine/helm:3.0.0</code> in the following Google Cloud Build step</p>

<pre><code>- id: 'update helm app'
  name: 'alpine/helm:3.0.0'
  args: ['upgrade', 'staging', './iprocure-chart/']
  env:
  - CLOUDSDK_COMPUTE_ZONE=us-central1-a
  - CLOUDSDK_CONTAINER_CLUSTER=iprocure-cluster
</code></pre>

<p>The problem is when i run this using <code>cloud-build-local</code> i get the following error and the pipeline ends with a fail</p>

<pre><code>Starting Step #4 - ""update helm app""
Step #4 - ""update helm app"": Already have image (with digest): alpine/helm:3.0.0
Step #4 - ""update helm app"": Error: UPGRADE FAILED: query: failed to query with labels: Get http://localhost:8080/api/v1/namespaces/default/secrets?labelSelector=name%3Dstaging%2Cowner%3Dhelm%2Cstatus%3Ddeployed: dial tcp 127.0.0.1:8080: connect: connection refused
</code></pre>","<p>This is because the configuration has not been set or passed.
To configure checkout = <a href=""https://cloud.google.com/cloud-build/docs/build-debug-locally#before_you_begin"" rel=""nofollow noreferrer"">https://cloud.google.com/cloud-build/docs/build-debug-locally#before_you_begin</a></p>
<p>and in your build step add a evn like this :</p>
<ul>
<li>id: 'update helm app'
name: 'alpine/helm:3.0.0'
args: ['upgrade', 'staging', './iprocure-chart/']
env:
<ul>
<li>CLOUDSDK_COMPUTE_ZONE=us-central1-a</li>
<li>CLOUDSDK_CONTAINER_CLUSTER=iprocure-cluster</li>
<li>KUBECONFIG=/workspace/.kube/config</li>
</ul>
</li>
</ul>
<p>If this does't work try passing the config with --kubeconfig flag in your helm command.Like this :
--kubeconfig=/workspace/.kube/config..</p>"
"Cloud Build fails to build App Engine Python 3.8 app (due to pip bug?)<p>I have a number of Python 3.7 apps on Google App Engine standard, all building and deploying fine. I'm trying to upgrade some of them to the <a href=""https://cloud.google.com/blog/products/application-development/updating-app-engine-with-more-new-runtimes"" rel=""nofollow noreferrer"">new Python 3.8 runtime</a>, but when I try to deploy, they fail in Cloud Build.</p>
<p>It looks like they're hitting <a href=""https://github.com/pypa/pip/issues/4390#issuecomment-431874500"" rel=""nofollow noreferrer"">this open pip bug</a> (<a href=""https://github.com/UnitedIncome/serverless-python-requirements/issues/240"" rel=""nofollow noreferrer"">more background</a>). Odd that only the Python 3.8 runtime triggers this bug, though, and 3.7 builds fine.</p>
<p>Full log below. (Note that it's happening in Cloud Build, not my local machine, so I can't upgrade pip or otherwise change any of the commands or environment.) Anyone know how I can fix or work around this?</p>
<pre><code>File upload done.
Updating service [default]...failed.
ERROR: (gcloud.beta.app.deploy) Error Response: [9] Cloud build 83e346a0-7e88-43dd-b89c-a4820526e4a1 status: FAILURE
Error ID: f8df99ad
Error type: INTERNAL
Error message: ... (setup.py): started
  Building wheel for webapp2 (setup.py): finished with status 'done'
  Created wheel for webapp2: filename=webapp2-3.0.0b1-py3-none-any.whl size=68362 sha256=9dd9f3ab6a55404492a88eb9a6bacb00faa37efafbc41f21a24d21cfba0eaea3
  Stored in directory: /layers/google.python.pip/pipcache/wheels/55/e9/4d/76b030f418cac0bef4a3dcc15ca95c9671f1e826731ce2bc0f
  Building wheel for tlslite-ng (setup.py): started
  Building wheel for tlslite-ng (setup.py): finished with status 'done'
  Created wheel for tlslite-ng: filename=tlslite_ng-0.7.5-py3-none-any.whl size=199869 sha256=b9ead00f0832041fba1e9d3883e57847995c2d6f83ecb7ea87d09cf82c730e8b
  Stored in directory: /layers/google.python.pip/pipcache/wheels/a6/e1/a6/09610854c3405202d0b71d8f869811781e40cd26ffb85eacf8
Successfully built gdata humanize mf2py mf2util python-tumblpy ujson webapp2 tlslite-ng
Installing collected packages: six, ecdsa, tlslite-ng, lxml, gdata, certifi, urllib3, chardet, idna, requests, setuptools, protobuf, googleapis-common-protos, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, pytz, grpcio, google-api-core, google-cloud-core, google-cloud-logging, gunicorn, pbr, extras, linecache2, traceback2, python-mimeparse, argparse, unittest2, testtools, fixtures, mox3, soupsieve, beautifulsoup4, gdata-python3, redis, google-cloud-datastore, google-cloud-ndb, humanize, MarkupSafe, jinja2, webencodings, html5lib, mf2py, mf2util, oauthlib, prawcore, websocket-client, update-checker, praw, requests-oauthlib, python-tumblpy, tweepy, ujson, webob, webapp2, oauth-dropins
  Running setup.py develop for oauth-dropins
    ERROR: Command errored out with exit status 1:
     command: /opt/python3.8/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/workspace/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/workspace/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' develop --no-deps --home /tmp/pip-target-zp53suvg
         cwd: /workspace/
    Complete output (6 lines):
    usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]
       or: setup.py --help [cmd1 cmd2 ...]
       or: setup.py --help-commands
       or: setup.py cmd --help
    
    error: option --home not recognized
    ----------------------------------------
ERROR: Command errored out with exit status 1: /opt/python3.8/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/workspace/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/workspace/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' develop --no-deps --home /tmp/pip-target-zp53suvg Check the logs for full command output.
WARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.
You should consider upgrading via the '/opt/python3.8/bin/python3 -m pip install --upgrade pip' command.
Full build logs: https://console.cloud.google.com/cloud-build/builds/83e346a0-7e88-43dd-b89c-a4820526e4a1?project=216076569502
</code></pre>
<p>Here's my <code>requirements.txt</code> file. I suspect the <code>-e .</code> might be the problem...but it works with Python 3.7, so if so, that's disappointing.</p>
<pre><code>git+https://github.com/dvska/gdata-python3.git#egg=gdata
google-cloud-logging~=1.14
gunicorn~=20.0
mox3~=0.28

# this includes everything in setup.py's install_requires.
# https://caremad.io/posts/2013/07/setup-vs-requirement/#developing-reusable-things-or-how-not-to-repeat-yourself
-e .
</code></pre>","<p>I checked <a href=""https://pypi.org/project/oauth-dropins/2.2/"" rel=""nofollow noreferrer"">pypi page of oauth-dropins</a> (at which it is failing) and they're mentioning there exactly this issue being caused by <code>-e</code></p>"
"Can I make cloud build run all triggered builds seriallly?<p>We are using cloud build but unlike circle CI, we have not found a way to make sure if trigger is called twice to make sure one build is run AFTER the other so they are not all run at the same time.  Is there a way to do this in google cloud build?</p>
<p>I don't see this config in cloudbuild.yaml or in the GUI...</p>
<p><a href=""https://cloud.google.com/cloud-build/docs/build-config"" rel=""nofollow noreferrer"">https://cloud.google.com/cloud-build/docs/build-config</a></p>","<p><a href=""https://cloud.google.com/cloud-build/quotas"" rel=""nofollow noreferrer"">Official documentation:</a>
Each Google Cloud project is granted quota to run ten builds at a time. When this quota is filled, requests for additional builds are queued and processed serially after the running build completes. An infinite number of builds can be queued.</p>
<p>Simply decrease the quota to 1 and the builds will be executed one at a time, while the other builds will wait in the queue.</p>
<p><a href=""https://console.cloud.google.com/iam-admin/quotas"" rel=""nofollow noreferrer"">https://console.cloud.google.com/iam-admin/quotas</a></p>
<p><a href=""https://i.stack.imgur.com/FZJ2A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FZJ2A.png"" alt=""enter image description here"" /></a></p>
<p>This is a fast <em>workaround</em>. Here is an <a href=""https://stackoverflow.com/a/59787296/13736525"">answer</a> with some references on custom builders usage. You may need to create your own queuing logic if you have for example 3 different builds and you don't want them to block each other, while each one of them has no more than one ongoing instance.</p>"
"Using two CloudBuild images in one step<p>I have run into a problem where I need to run both Java+Android and NodeJS in the same CloudBuild &quot;step&quot;.</p>
<p>My current situation is that I'm trying to build a <code>react-native</code> project within Google CloudBuild. The problem with this is that while bundling for Android with <code>.gradlew</code> a node script is called.</p>
<p>I tried using a CloudBuild step config like this:</p>
<pre><code>{
    &quot;name&quot;: &quot;gcr.io/$PROJECT_ID/android:29&quot;,
    &quot;args&quot;: [&quot;./gradlew&quot;, &quot;bundleProductionRelease&quot;]
}
</code></pre>
<p>But this resulted in this error:</p>
<blockquote>
<p><code>Cannot run program &quot;node&quot;: error=2, No such file or directory</code></p>
</blockquote>
<p>Of course, this makes sense, as there would be no reason for NodeJS to be installed by this container.</p>
<p>My question is how can I run this script using both the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/yarn"" rel=""nofollow noreferrer"">NodeJS</a> and <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/android"" rel=""nofollow noreferrer"">Android</a> container images?</p>","<p>Firstly, on Cloud Build, you can only run 1 container at each step. Here your problem isn't to run 2 containers, you want to have 2 applications embedded in the same container.</p>
<p>For this 2 solutions:</p>
<ol>
<li>You find a container with all what you want installed on it</li>
<li>You use a base container and you install by yourselves the missing app on it</li>
</ol>
<ul>
<li>Either, just after the installation you use it as-is</li>
<li>Or you can create a build pipeline to shape your own container <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/use-community-and-custom-builders#creating_a_custom_builder"" rel=""nofollow noreferrer"">(custom-builder)</a> from the base with the installations of missing part and store it in GCR. Then, for the build of your application, you can use directly this custom container</li>
</ul>
<p>To install the missing part, you can do this</p>
<pre><code>- name: &quot;gcr.io/$PROJECT_ID/android:29&quot;
  entrypoint: &quot;bash&quot;
  args: 
    - &quot;-c&quot;
    - |
         curl -sL https://deb.nodesource.com/setup_12.x | bash -
         apt-get install -y nodejs

         node -v #optional, test the installed version 

        ./gradlew bundleProductionRelease
}
</code></pre>
<p><em>Note: you can find the correct installation for your base image <a href=""https://nodejs.org/en/download/package-manager"" rel=""nofollow noreferrer"">here</a>. I use the standard Linux Ubuntu installation in my example</em></p>"
"Failure: (ID: 838926df) did not find any jar files with a Main-Class manifest entry<p>when running <code>gcloud app deploy</code> on my spring boot app, this error happens in Cloud Build.</p>","<p>I deleted my maven plugin by accident, so don't delete it.</p>
<pre class=""lang-xml prettyprint-override""><code>      &lt;plugin&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
      &lt;/plugin&gt;
</code></pre>"
"How to use PNPM with Google Cloud Build?<p>I'd like to migrate to PNPM, however, I can't find a way to use its lockfile on Google Cloud. My current <code>cloudbuild</code> config is the following:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: &quot;gcr.io/google.com/cloudsdktool/cloud-sdk:latest&quot;
  entrypoint: 'gcloud'
  args: [&quot;app&quot;, &quot;deploy&quot;]
timeout: &quot;1600s&quot;
</code></pre>
<p>Afaik these official images only support Yarn and NPM. Is there an easy way to replace Yarn with PNPM here?</p>
<p>I looked on the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders"" rel=""noreferrer"">Cloud Builders GitHub repo</a>, but there's no PNPM there either.</p>","<p>IIUC the App Engine standard Node runtime(s) require that you use npm or yarn. PNPM is thus <strong>not</strong> user-definable when using standard.</p>
<p><a href=""https://cloud.google.com/appengine/docs/standard/nodejs/specifying-dependencies"" rel=""nofollow noreferrer"">https://cloud.google.com/appengine/docs/standard/nodejs/specifying-dependencies</a></p>
<p>If you want to use App Engine with a different package manager you could use flex and define a custom runtime. This essentially allows you to define a container image to deploy to App Engine and this may be anything that exposes an httpd on <code>:8080</code>.</p>"
"How often do Cloud Build Node.js versions update?<p>I couldn't stomach purchasing the $150 for GCP's support service for this one question. I'm just looking to understand the schedule for Cloud Build Node.js versions. It's still stuck on Node.js v10.10 and my projects are starting to require higher versions to build. According to Cloud Build's changelog, I don't believe the Node.js version has updated in years. Any ideas?</p>","<p>As per the official <a href=""https://github.com/googleapis/nodejs-cloudbuild"" rel=""nofollow noreferrer"">Github repository</a>:</p>
<blockquote>
<p>Our client libraries follow the <a href=""https://nodejs.org/en/about/releases/"" rel=""nofollow noreferrer"">Node.js release schedule</a>. Libraries are compatible with all current active and maintenance versions of Node.js.</p>
</blockquote>
<p>So, this means it should work with Node.js 12 and the updates should be more constant. In addition to that, in <a href=""https://cloud.google.com/cloud-build/docs/building/build-nodejs#building_with_npm_or_yarn"" rel=""nofollow noreferrer"">here</a>, it says that if you are using a <a href=""https://cloud.google.com/cloud-build/docs/build-config"" rel=""nofollow noreferrer"">Cloud Build config file</a>, you can use Node.js 12, so the Node.js' latest version should be compatible with Cloud Build.</p>
<p>To summarize, by the repository, it should follow Node.js schedule. However, in case you think this is not occurring, I would recommend you to raise a bug on the <a href=""https://issuetracker.google.com/issues/new?component=190802&amp;template=0"" rel=""nofollow noreferrer"">Google's Issue Tracker</a> - it's free, by the way - so they can assess this.</p>"
"Error when using Firebase CLI in Cloud Build<p>I get the following error when I run &quot;cloud-build-local --dryrun=false .&quot; on my local machine:</p>
<blockquote>
<p>Firebase Management API has not been used in project 32555940559
before or it is disabled. Enable it by visiting
<a href=""https://console.developers.google.com/apis/api/firebase.googleapis.com/overview?project=32555940559"" rel=""nofollow noreferrer"">https://console.developers.google.com/apis/api/firebase.googleapis.com/overview?project=32555940559</a>
then retry. If you enabled this API recently, wait a few minutes for
the action to propagate to our systems and retry.</p>
</blockquote>
<p>The project number listed is not the number of any of my projects. I Googled this number and it seems like more users get errors with this project number in it. I already checked the following:</p>
<ul>
<li>The account that I am signed in with has owner permissions for the project</li>
<li>&quot;gcloud config list&quot; show that the correct project and account are active</li>
<li>The project is listed in my .firebase.rc file</li>
</ul>
<p>Any help on this is appreciated!</p>","<p>cloud-build-local is the command used to <a href=""https://cloud.google.com/cloud-build/docs/build-debug-locally"" rel=""nofollow noreferrer"">build a image locally</a>.</p>
<p>The Cloud Build local environment, as well as all other GCP local environments, are outside of your Google Cloud environment, hence the project-id you are seeing here references the gcloud project number. This means that gcloud will unable to use the login credentials for your respective project and will not automatically authenticate with the Cloud Build service account needed for the Cloud Build gcloud commands.</p>
<p>In this case you would need to utilize <a href=""https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login"" rel=""nofollow noreferrer""><code>gcloud auth application-default login</code></a>. This will provide new user credentials for use with <a href=""https://cloud.google.com/docs/authentication/production"" rel=""nofollow noreferrer"">Application Default Credentials</a> so as to authorize you to utilize the needed service accounts for local Cloud Build development.</p>"
"Next.js - ERROR Build directory is not writeable on Google Cloud Build<p>I was trying to automate the deployment process of my Next.JS application to App Engine using Cloud Build but at the <code>build</code> phase it keeps on failing with:</p>
<pre><code>Error: &gt; Build directory is not writeable. https://err.sh/vercel/next.js/build-dir-not-writeable
</code></pre>
<p>I cant seem to figure out what to fix for this.</p>
<p>My current build file is and it keeps failing on step 2:</p>
<pre><code>steps:
  # install dependencies
- name: 'gcr.io/cloud-builders/npm'
  args: ['install']
  # build the container image
- name: 'gcr.io/cloud-builders/npm'
  args: ['run', 'build']
  # deploy to app engine
- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  args: [&quot;app&quot;, &quot;deploy&quot;]
  env:
  - 'PORT=8080'
  - 'NODE_ENV=production'
timeout: &quot;1600s&quot;
</code></pre>
<p>app.yaml:</p>
<pre><code>runtime: nodejs12

handlers:
- url: /.*
  secure: always
  script: auto

env_variables:
  PORT: 8080
  NODE_ENV: 'production'
</code></pre>
<p>any help would be appreciated</p>","<p>Can reproduce the same behavior after upgrading to next version 9.3.3.</p>
<h2>Cause</h2>
<p>The issue is related to the npm dependency which is managed by google if you use <code>gcr.io/cloud-builders/npm</code> seems they are running your build inside of Google Cloud Build on an old node version.</p>
<p>Here you can find the currently supported version
<a href=""https://console.cloud.google.com/gcr/images/cloud-builders/GLOBAL/npm?gcrImageListsize=30"" rel=""nofollow noreferrer"">https://console.cloud.google.com/gcr/images/cloud-builders/GLOBAL/npm?gcrImageListsize=30</a></p>
<p>As you can see Googles latest node version is 10.10. The newest next.js version requires at least node 10.13</p>
<h2>Solution</h2>
<p>Change <code>gcr.io/cloud-builders/npm</code> to</p>
<pre><code>- name: node
  entrypoint: npm
</code></pre>
<p>in order to use the official docker npm package which runs on node12.
After those changes your build will be successful again.</p>
<h1>Sidenote</h1>
<p>Switching to the official npm will increase the build duration (at least in my case). It takes around 2 minutes longer then the gcr npm.</p>"
"Firebase Hosting deploy with Google Cloud Build throwing ""No such file or directory"" error<p>I built a simple ReactJs app and tried using <strong>Google Cloud Build</strong> to deploy it as soon as I push a new commit to the master branch on Github.
For that I added an &quot;On push to master&quot; trigger and followed every step from <a href=""https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-firebase"" rel=""nofollow noreferrer"">here</a> to configure my Google Cloud and Local Project. In the end I created a <strong>cloudbuild.yaml</strong> file with the following content:</p>
<p><a href=""https://i.stack.imgur.com/L6FgS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L6FgS.png"" alt=""cloudbuild.yaml content"" /></a></p>
<p>After a new commit and push the cloud build logs a correct <code>npm install</code> and <code>npm run build</code>, but the firebase stops at the following step.</p>
<p><a href=""https://i.stack.imgur.com/P6oor.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P6oor.png"" alt=""Cloud Build Error Log"" /></a></p>
<p>I didn't understand which file/directory is missing, nor how to fix it. Could anyone help me a little?
Thanks in advance.</p>","<p>So after a whole day of work I found out the problem. Apparently when you generate the firebase image mentioned in the tutorial all its files need to use End of line LF, but in windows it sets all of them to CRLF. I solved the problem using the WSL terminal in windows.</p>"
"Upload build folder in Google Cloud Build<p>I'm trying to upload my React app to App Engine using Cloud Build but it upload all source files. Is it possible to only deploy the build folder using Cloud Build pipeline?</p>
<p><strong>Current pipeline:</strong></p>
<pre><code>steps:
# Install
- name: 'gcr.io/cloud-builders/npm'
  args: ['install']
# Build
- name: 'gcr.io/cloud-builders/npm'
  args: ['run', 'build']
# Deploy
- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  args: [&quot;app&quot;, &quot;deploy&quot;]
  timeout: &quot;1600s&quot;
</code></pre>","<p>I assume that you want to only deploy the Build folder and not to upload the others file on App Engine. I think you only have statics file and your app.yaml must only describe how to serve these statics resources.</p>
<p>If so, you can do like this</p>
<pre><code># Deploy
- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  entrypoint: bash
  args: 
    - &quot;-c&quot;
    - |
        cp app.yaml ./build
        cd build
        gcloud app deploy
  timeout: &quot;1600s&quot;
</code></pre>
<p>It's one solution; others exist. And you should have to update the app.yaml file because the build directory no longer exists in the deployment.</p>"
"Is it possible to access a service account credentials from a container built and started by a GCB step?<p>To run integration tests we build and start a new container in one GCB's step:</p>
<pre><code>- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args: ['container', 'clusters', 'get-credentials', '$_GC_CLUSTER', '--zone', '$_GC_ZONE']
- name: 'nixery.dev/bash/...'
  entrypoint: 'make' # test creates and runs a new container
  args: ['test', '-C', '$_SERVICE']
</code></pre>
<p>Of course, this new container doesn't have access to the default account service of GCB, so it cannot access other resources like Google Cloud Storage.</p>
<p>We could include another service account credentials in the base image, but it would be better if we could rely on the credentials provided by GCB. I tried copying the Kubernetes configuration and the Google Cloud SDK configuration from the parent container, but it doesn't work so I guess I'm on the wrong path.</p>
<p><a href=""https://stackoverflow.com/a/59326643/259517"">This answer</a> mentions using <code>--impersonate-service-account</code> with <code>gcloud auth configure-docker</code>, but it doesn't give more details. I'm running the previous command in the container I want to grant access to just before running the tests:</p>
<pre><code>gcloud auth configure-docker --impersonate-service-account project-number-compute@developer.gserviceaccount.com

python3 -m pytest tests
</code></pre>
<p>But I still get an authentication error. If it is possible, what am I missing?</p>","<p>The container needs to be in the cloudbuild network. When you build it, specify <code>--network=cloudbuild</code></p>
<p>See <a href=""https://cloud.google.com/cloud-build/docs/build-config#network"" rel=""noreferrer"">Google Cloud Build network</a>.</p>"
"Cloud Build passing args to an entrypoint with exec<p>I have a Dockerfile with the following <code>ENTRYPOINT</code> command that uses exec.</p>
<pre><code>FROM node:10-alpine
RUN apk add ca-certificates
RUN npm install -g firebase-tools

COPY --from=gcr.io/berglas/berglas:latest /bin/berglas /bin/berglas

ENV FIREBASE_TOKEN &quot;&quot;

ENTRYPOINT exec /bin/berglas exec -- /usr/local/bin/firebase
</code></pre>
<p>In <code>cloudbuild.yaml</code> I have the following step but the arguments passed in <code>args: []</code> are not being respected.</p>
<pre class=""lang-yaml prettyprint-override""><code>  - name: firebase-tools
    dir: '/workspace/functions'
    args: ['deploy', '-P', '${_FIREBASE_PROJECT_NAME}']
    env:
      - 'FIREBASE_TOKEN=sm://$PROJECT_ID/firebase-ci-token'
</code></pre>
<p>Because <code>exec</code> changes the shell it ignores the original <code>args: []</code>. Is there a means of applying these <code>args: []</code> for the <code>/usr/local/bin/firebase deploy</code> command?</p>","<p>Your actual problem is that you're using the string from of <code>ENTRYPOINT</code>.  This wraps the command string in <code>sh -c '...'</code>, which will ignore any additional arguments passed as the command part.  <strong><code>ENTRYPOINT</code> must use JSON-array syntax to take additional parameters in <code>CMD</code>.</strong></p>
<pre class=""lang-sh prettyprint-override""><code>ENTRYPOINT [&quot;/bin/berglas&quot;, &quot;exec&quot;, &quot;--&quot;, &quot;/usr/local/bin/firebase&quot;]
</code></pre>
<p>Since this form doesn't have a shell wrapper (it just runs the command directly) you don't need the outer <code>exec</code> here.</p>
<p>(I'd consider rearranging this so that you have <code>CMD [&quot;firebase&quot;]</code>, and the <code>ENTRYPOINT</code> line ends in the <code>--</code> delimiter.  Then <code>CMD</code> is a complete command and you can easily do things like run debugging shells inside the environment wrapper.)</p>"
"GCP error while building a container image<p>I have to test a functionality where I need to ping google.com from Google Cloud Run. For this, I created a simple python script to ping google.com.
I added Dockerfile and .dockerignore to the directory as mentioned in Google <a href=""https://cloud.google.com/run/docs/quickstarts/build-and-deploy/python"" rel=""nofollow noreferrer"">Documentation</a> . When I try to build a container image, I get the following error:</p>
<pre><code>gcloud builds submit --tag gcr.io/project_id/ping-google
ERROR: (gcloud.builds.submit) The User is forbiden from accessing the bucket [project_id_cloudbuild]. Please check your organization's policy.
</code></pre>
<p>I am a storage admin and a Cloudbuild editor, what access I may be missing?</p>","<p>There was some organization policy set because of which I was not able to perform this using gcloud. But I was able to create the container from cloud shell editor using steps mentioned <a href=""https://shell.cloud.google.com/?walkthrough_tutorial_id=cloud_run_cloud_code_create_service&amp;show=ide&amp;environment_deployment=ide"" rel=""nofollow noreferrer"">here</a></p>
<p>This tutorial downloads the helo world code. I edited it to add my code of pinging google, and my job was done</p>"
"Spring batch worker pods are unable to pick custom service account for spring cloud deployer kubernetes<p>I am trying to run a spring batch with remote partitioning on K8s cluster using spring-cloud-deployer-kubernetes. Eventhough I have configured a service account and mentioned in my application properties the below way</p>
<pre><code>spring.cloud.deployer.kubernetes.deployment-service-account-name=scdf-sa
</code></pre>
<p>Still the master task is unable to spawn worker pods and it seems it does not pick the property while launching task from spring cloud dataflow UI and throws this error in master pod:</p>
<pre><code>io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: GET at: https://&lt;IP&gt;/api/v1/namespaces/test/pods/batchsampleappworker-aeghj644g. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods &quot;batchsampleappworker-j3ljqq3de9&quot; is forbidden: User &quot;system:serviceaccount:test:default&quot; cannot get resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;test&quot;.

</code></pre>
<p>PS: I am using spring-cloud-deployer-kubernetes version of 2.5.0</p>
<p>Please some hints on how to correctly configure service account?
Thanks in advance!</p>","<p>As per the official documentation of spring cloud dataflow <a href=""https://docs.spring.io/spring-cloud-dataflow/docs/current/reference/htmlsingle/#_deployment_service_account"" rel=""nofollow noreferrer"">here</a>, adding the below to SCDF server config map solved the issue for me.</p>
<pre><code>data:
  application.yaml: |-
    spring:
      cloud:
        dataflow:
          task:
            platform:
              kubernetes:
                accounts:
                  default:
                    deploymentServiceAccountName: myserviceaccountname
</code></pre>"
"Cloud Build Trigger for FTP or SSH deployment<p>How can I deploy a directory to a FTP or SSH server, with a <a href=""https://console.cloud.google.com/cloud-build/triggers"" rel=""nofollow noreferrer"">trigger</a> and  <code>cloudbuild.yaml</code>?</p>
<p>So far I can already generate a listing of the files which I'd like to upload:</p>
<pre><code>steps:
  - name: 'ubuntu'
    entrypoint: 'bash'
    args:
      - '-c'
      - |-
        find $_UPLOAD_DIRNAME -exec echo {} &gt;&gt; batch.txt \;
        cat ./batch.txt
    env:
      ...
</code></pre>","<p>I've came to the conclusion, that I don't want the FTP anti-pattern<br/>
and have therefore written an alternate SSH <code>cloudbuild.yaml</code>:</p>
<ul>
<li>generate a new pair of RSA keys.</li>
<li>use the private key for SSH login.</li>
<li>recursively upload the directory with <code>scp</code>.</li>
<li>run remote commands with <code>ssh</code>.</li>
</ul>
<p>It logs in as user <code>root</code>, therefore remote <code>/etc/ssh/sshd_config</code> needs <code>PermitRootLogin yes</code>.</p>
<hr />
<p>My variable substitutions meanwhile look alike this:</p>
<p><a href=""https://i.stack.imgur.com/BG1dh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BG1dh.png"" alt=""screenshot: variable substitutions"" /></a></p>
<p>And this would be the <code>cloudbuild.yaml</code>, which generally demonstrates how to set up SSH keys:</p>
<pre><code>steps:
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:latest'
    entrypoint: 'bash'
    args:
      - '-c'
      - |-
        echo Deploying $_UPLOAD_DIRNAME @ $SHORT_SHA
        gcloud config set compute/zone $_COMPUTE_ZONE
        gcloud config set project $PROJECT_ID
        mkdir -p /builder/home/.ssh
        gcloud compute config-ssh
        gcloud compute scp --ssh-key-expire-after=$_SSH_KEY_EXPIRE_AFTER --scp-flag=&quot;${_SSH_FLAG}&quot; --recurse ./$_UPLOAD_DIRNAME $_COMPUTE_INSTANCE:$_REMOTE_PATH
        gcloud compute ssh $_COMPUTE_INSTANCE --ssh-key-expire-after=$_SSH_KEY_EXPIRE_AFTER --ssh-flag=&quot;${_SSH_FLAG}&quot; --command=&quot;${_SSH_COMMAND}&quot;
    env:
      - '_COMPUTE_ZONE=$_COMPUTE_ZONE'
      - '_COMPUTE_INSTANCE=$_COMPUTE_INSTANCE'
      - '_UPLOAD_DIRNAME=$_UPLOAD_DIRNAME'
      - '_REMOTE_PATH=$_REMOTE_PATH'
      - '_SSH_FLAG=$_SSH_FLAG'
      - '_SSH_COMMAND=$_SSH_COMMAND'
      - '_SSH_KEY_EXPIRE_AFTER=$_SSH_KEY_EXPIRE_AFTER'
      - 'PROJECT_ID=$PROJECT_ID'
      - 'SHORT_SHA=$SHORT_SHA'
</code></pre>"
"Google Cloud Build with Dockerfile and copy files<p>I've got the problem during setting up deploying using cloudbuild and dockerfile.</p>
<p>My <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.8

ARG ENV
ARG NUM_WORKERS
ENV PORT=8080
ENV NUM_WORKERS=$NUM_WORKERS


RUN pip install poetry
COPY pyproject.toml poetry.lock ./

RUN poetry config virtualenvs.create false &amp;&amp; \
    poetry install --no-dev

COPY ./.env.$ENV /workspace/.env
COPY ./app-$ENV.yaml /workspace/app.yaml
COPY . /workspace

ENTRYPOINT [&quot;./entrypoint.sh&quot;]
</code></pre>
<p>My <code>cloudbuild.yaml</code>:</p>
<pre><code>steps:
  - name: 'gcr.io/cloud-builders/docker'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        docker pull gcr.io/$PROJECT_ID/my-repo:$BRANCH_NAME || exit 0
  - name: 'gcr.io/cloud-builders/docker'
    args: [
      'build',
      '-t',
      'gcr.io/$PROJECT_ID/my-repo:$BRANCH_NAME',
      '--cache-from',
      'gcr.io/$PROJECT_ID/my-repo:$BRANCH_NAME',
      '--build-arg', 'ENV=develop',
      '--build-arg', 'NUM_WORKERS=2',
      '.'
    ]

  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/$PROJECT_ID/my-repo:$BRANCH_NAME']

  - name: 'gcr.io/$PROJECT_ID/my-repo:$BRANCH_NAME'
    id: RUN-LINTERS
    entrypoint: sh
    args: ['scripts/linters.sh']

  - name: gcr.io/cloud-builders/docker
    id: START-REDIS
    args: ['run', '-d', '--network=cloudbuild', '--name=redisdb', 'redis']

  - name: 'gcr.io/$PROJECT_ID/my-repo:$BRANCH_NAME'
    id: RUN-TESTS
    entrypoint: sh
    args: ['scripts/run_tests.sh']
    env:
      - 'REDIS_HOST=redis://redisdb'
      - 'DATASTORE_EMULATOR_HOST=datastore:8081'
    waitFor:
      - START-REDIS
      - START-DATASTORE-EMULATOR

  - name: gcr.io/cloud-builders/docker
    id: SHUTDOWN-REDIS
    args: ['rm', '--force', 'redisdb']

  - name: gcr.io/cloud-builders/docker
    id: SHUTDOWN-DATASTORE_EMULATOR
    args: ['rm', '--force', 'datastore']

  - name: 'gcr.io/cloud-builders/gcloud'
    id: DEPLOY
    args:
      - &quot;app&quot;
      - &quot;deploy&quot;
      - &quot;--image-url&quot;
      - 'gcr.io/$PROJECT_ID/my-repo:$BRANCH_NAME'
      - &quot;--verbosity=debug&quot;
images: ['gcr.io/$PROJECT_ID/my-repo:$BRANCH_NAME']
timeout: &quot;1000s&quot;
</code></pre>
<p>Problem is that copied files <code>.env</code> and <code>app.yaml</code> are not presented in <code>workspace</code>
I don't know why cloudbuild ignore these files from image, because I've printed <code>ls -a</code> and have seen that files are copied properly during build, but they disappear during run-tests stage and also I can't deploy without app.yaml
Any help pleaseee</p>","<p>To fix it I've just copied it in cloudbuild before build</p>
<pre><code>- name: 'gcr.io/cloud-builders/gcloud'
  id: MOVE-DOTENV-APP-YAML
  entrypoint: bash
  args:
    - '-e'
    - '-c'
    - |
      cp app-$_ENVIRONMENT.yaml app.yaml
      cp .env.$_ENVIRONMENT .env
</code></pre>"
"gsutil cp creates nested directory<p>I’m trying to copy a folder (<code>/data/out</code>) from Cloud Build to Google Cloud Storage:</p>
<pre class=""lang-yaml prettyprint-override""><code>  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:slim'
    volumes:
      - name: 'vol1'
        path: '/data'
    entrypoint: 'gsutil'
    args:
      - -m
      - cp
      - -r
      - /data/out
      - gs://mybucket/files
</code></pre>
<p>When running this the first time, all the content from /data/out is copied to the bucket, which is what I wanted.</p>
<p>When I run it a second time to overwrite/update the files, there’s a folder <code>/files/out</code> instead, so the new files are in <code>mybucket/files/out/*</code> instead of <code>mybucket/files/*</code>.</p>
<p>How can I fix this?</p>
<p>I tried to add another step to remove the folder before copying it again (<code>rm -r gs://mybucket/files</code>), but this step fails the first time, as the folder does not exist yet, causing the build to fail/stop entirely.</p>","<p>Finally got this to work with <code>rsync</code> instead of <code>cp</code> thanks to <a href=""https://stackoverflow.com/q/67304543"">Cloud Build -&gt; Google Cloud Storage: Question about downtimes at deployment time</a></p>
<pre class=""lang-yaml prettyprint-override""><code>- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:slim'
    id: copy
    volumes:
      - name: 'vol1'
        path: '/data'
    entrypoint: 'gsutil'
    args:
      - -m
      - rsync
      - -r
      - -d
      - /data/out/
      - gs://mybucket/folder
</code></pre>"
"GCP CloudBuild Logs<p>How can I add a custom message to Cloud Build logs?</p>
<p>I've tried using the bash entrypoint with the Docker builder (for example) and echoing some strings, but they don't appear in the build logs. Is there a way to achieve this?</p>","<p>Make sure that the builder image you're using has <code>bash</code> in it. I tested this <a href=""https://cloud.google.com/build/docs/configuring-builds/run-bash-scripts#running_inline_bash_scripts"" rel=""nofollow noreferrer"">code</a> and replaced the <code>gcloud</code> builder with <code>docker</code> and it is working fine. Here's an example code:</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args:
  - '-eEuo'
  - 'pipefail'
  - '-c'
  - |-
    if (( $(date '+%-e') % 2 )); then
      echo &quot;today is an odd day&quot;
    else
      echo &quot;today is an odd day, with an even number&quot;
    fi
</code></pre>
<p>And here's the Logs:</p>
<p><a href=""https://i.stack.imgur.com/x2T40.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x2T40.png"" alt=""enter image description here"" /></a></p>"
"Cloud build completes with FAILURE status; other issues follow<p>I was following along this <a href=""https://www.youtube.com/watch?v=_w_idf928WY&amp;t=615s"" rel=""nofollow noreferrer"">video</a> from Google Tech Build youtube channel on their new &quot;Serverless Expeditions&quot; series which is really great and neat. Highly recommended.</p>
<p>This is my first time experimenting with GCP but everything was executing perfectly until I got a weird error:</p>
<pre><code>BUILD
Already have image (with digest): gcr.io/cloud-builders/docker
unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /workspace/Dockerfile: no such file or directory
ERROR
ERROR: build step 0 &quot;gcr.io/cloud-builders/docker&quot; failed: step exited with non-zero status: 1
----------------------------------------------------------------------------------------------------------------------
ERROR: (gcloud.builds.submit) build 0caec01a-ba4e-4a58-afd3-5f646258ae72 completed with status &quot;FAILURE&quot;
Deploying container to Cloud Run service [test-api-service] in project [test-api] region [us-central1]
X Deploying... Image 'gcr.io/test-api/test-api-service' not found.                                               
  X Creating Revision... Image 'gcr.io/test-api/test-api-service' not found.                                     
  . Routing traffic...                                                                                               
  ✓ Setting IAM Policy...                                                                                            
Deployment failed                                                                                                    
ERROR: (gcloud.run.deploy) Image 'gcr.io/test-api/test-api-service' not found.
</code></pre>
<p>When executing my deployment shell script:</p>
<pre><code>GOOGLE_PROJECT_ID=test-api
CLOUD_RUN_SERVICE=test-api-service
INSTANCE_CONNECTION_NAME=test-api:us-central-1:test-sql-instance
DB_USER=root
DB_PASS=testpw
DB_NAME=testdb

gcloud builds submit --tag gcr.io/$GOOGLE_PROJECT_ID/$CLOUD_RUN_SERVICE \
    --project=$GOOGLE_PROJECT_ID

gcloud run deploy $CLOUD_RUN_SERVICE \
    --image gcr.io/$GOOGLE_PROJECT_ID/$CLOUD_RUN_SERVICE \
    --add-cloudsql-instances $INSTANCE_CONNECTION_NAME \
    --update-env-vars INSTANCE_CONNECTION_NAME=$INSTANCE_CONNECTION_NAME,DB_PASS=$DB_PASS,DB_NAME=$DB_NAME,DB_USER=$DB_USER \
    --platform managed \
    --region us-central1 \
    --allow-unauthenticated \
    --project=$GOOGLE_PROJECT_ID
</code></pre>
<p>Note that this is the deployment script used by the tech demonstrators in the video. We are building a REST API with node.js and MySQL. I looked all around and really cannot seem to find a solution to this one. Perhaps some outside help might shed some light on some possible solutions to this.</p>
<p>Thank you in advance!</p>","<p>As @JohnHanley and @guillaumeblaquiere mentioned in the comments, the issue was caused by the Dockerfile.</p>
<p>The <a href=""https://www.youtube.com/watch?v=_w_idf928WY&amp;t=615s"" rel=""nofollow noreferrer"">tutorial</a> you went through doesn't cover the Dockerfile structure although this file is required.</p>
<p>You can use the <a href=""https://cloud.google.com/run/docs/quickstarts/build-and-deploy/nodejs#containerizing"" rel=""nofollow noreferrer"">containerizing guide for Node.js</a> to complete your configuration.</p>"
"Cloud Build yaml in repository and potential for bypassing permissions?<p>Say I have the following <code>cloudbuild.yaml</code> in my repository, and its referenced by a manual trigger during setup (ie. it looks in the repo for <code>cloudbuild.yaml</code>)</p>
<pre class=""lang-yaml prettyprint-override""><code># repo/main/cloudbuild.yaml

steps:
  - id: 'Safe step'
    name: 'gcr.io/cloud-builders/git'
    entrypoint: '/bin/bash'
    args: ['echo', 'hello world']
</code></pre>
<p>Assume that the intention is that users can manually run the trigger as they please. However the cloud build service account is also high-powered and has access to certain permissions that can be destructive. This is required for other manual triggers that need to be approved.</p>
<p>Can the user create a branch, edit the <code>cloudbuild.yaml</code> to do something destructive, push he branch up and then when they go to run the trigger they just reference their branch instead thereby bypassing the control to be able to edit a trigger?</p>
<pre class=""lang-yaml prettyprint-override""><code># repo/branch-xyz/cloudbuild.yaml

steps:
  - id: 'Safe step'
    name: 'gcr.io/cloud-builders/git'
    entrypoint: '/bin/bash'
    args: ['do', 'something', 'destructive']
</code></pre>","<p>Posting as a community wiki as this is based on @bhito's comment:</p>
<p>It depends on how the trigger is configured.</p>
<p>From what you're describing, looks like it is matching all the branches and the Service Account being used is Cloud Build's default one. With that setup, what you're describing is correct, the trigger will execute whatever the cloudbuild.yaml has defined.</p>
<p>You should be able to limit that behaviour by filtering the trigger by branch, limiting the service account permissions or use a custom one, or both options all together. With a combination of branch filtering and custom service accounts you have fine-grained access control.</p>"
"run gcloud script in cloud build<p>I would like to have a cloud source repo which contains:-</p>
<ul>
<li>The cloudbuild.yaml file.</li>
<li>A gcloud shell script (to create infra, eg a VM).</li>
</ul>
<p>Is it possible to set things up where on a push to that repo, cloud build runs the shell script? Is there an example of this anywhere? The only examples I can find are for Python or Node.js</p>
<p>Thanks</p>","<p>It's totally possible to run shell scripts within your cloud build pipeline.</p>
<p>Here you can find a few examples: <a href=""https://cloud.google.com/build/docs/configuring-builds/run-bash-scripts"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/configuring-builds/run-bash-scripts</a></p>
<p>Within Cloud Build, there is also a <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/gcloud"" rel=""nofollow noreferrer"">built in gcloud image</a> where you could directly run gcloud commands. Simple examples:</p>
<pre><code>steps:
- name: gcr.io/cloud-builders/gcloud
  args: ['compute', 'instances', 'add-metadata', 'example-instance','--metadata startup-script-url=gs://bucket/file']
</code></pre>
<pre><code>- name: 'gcr.io/cloud-builders/gcloud'
  args: [
        'deployment-manager',
        'deployments',
        'create', 
        'lgwm-$SHORT_SHA', 
        '--template', 
        'vm_template.jinja',
        '--properties',
        'zone:${_ZONE}'
        ]
</code></pre>"
"Optionally run Cloud Build trigger on commit<p>Currently we use Google Cloud Build to run tests, build and deploy the application. This is only run on merging with develop and live branches, not for commits on feature branches.</p>
<p>I would like to optionally run the CICD trigger and deploy when committing to a feature branch but would not want to do this every time I commit.</p>
<p>Is this possible? Can I add something to the commit message to access in the trigger like check for the text &quot;--CICD&quot;?</p>
<p>As a workaround I can use the regex matcher on the branch name to check for /CICD$/ and merge into the CICD version of the branch when I want to commit but this doesn't seem like a good approach and is error prone.</p>","<p>You can add a <a href=""https://cloud.google.com/build/docs/automating-builds/create-manage-triggers#skipping_a_build_trigger"" rel=""nofollow noreferrer"">skip ci</a> comment to not run your you pipeline</p>
<p>The other solution is to test the <a href=""https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values#using_default_substitutions"" rel=""nofollow noreferrer"">git branch name in the env vars</a> currently running the pipeline and to exit the pipeline at the first step if it's not compliant</p>"
"How can I deploy Google Cloud Functions in CI/CD without re-deploying unchanged Cloud Functions to avoid the quota?<p>Cloud Build has a create quota of 30. If we have more than 30 Cloud Functions, this quota can easily be reached. Is there a way to deploy more than 30 Cloud Functions that people use, that preferably is smart enough to not deploy unmodified Cloud Functions?</p>","<p>Following our conversation in GCP community slack channel, here is an idea with a small example. The example depicts one cloud function but easily can be extended to an arbitrary set of cloud functions.</p>
<p>Bear in mind - this is not my invention - one can find plenty of examples in the internet.</p>
<p>The CICD uses Terraform inside Cloud Build (simply speaking - cloud build yaml file contains 'terraform init' and `terraform apply'). Thus, push (or pull request) triggers a Cloud Build job, which executes Terraform.</p>
<p>In the scope of this question - terraform script should have 4 elements:</p>
<p>1/ A name of the zip archive with the cloud function code - as it should be in the GCS bucket:</p>
<pre><code>locals {
  cf_zip_archive_name = &quot;cf-some-prefix-${data.archive_file.cf_source_zip.output_sha}.zip&quot;
}
</code></pre>
<p>2/ A zip archive:</p>
<pre><code>data &quot;archive_file&quot; &quot;cf_source_zip&quot; {
  type        = &quot;zip&quot;
  source_dir  = &quot;${path.module}/..&lt;&lt;path + directory to the CF code&gt;&gt;&quot;
  output_path = &quot;${path.module}/tmp/some-name.zip&quot;
}

</code></pre>
<p>3/ A GCS object in a bucket (under assumption that the bucket is already exist, or created outside of the scope of this question):</p>
<pre><code>resource &quot;google_storage_bucket_object&quot; &quot;cf_source_zip&quot; {
  name         = local.cf_zip_archive_name
  source       = data.archive_file.cf_source_zip.output_path
  content_type = &quot;application/zip&quot;
  bucket       = google_storage_bucket.cf_source_archive_bucket.name
}
</code></pre>
<p>4/ A Cloud Function (only 2 parameters are shown):</p>
<pre><code>resource &quot;google_cloudfunctions_function&quot; &quot;sf_integrations&quot; {

  source_archive_bucket = google_storage_bucket.cf_source_archive_bucket.name
  source_archive_object = google_storage_bucket_object.cf_source_zip.name

}
</code></pre>
<p>How it works all together =&gt;</p>
<p>When the Terraform is triggered, the zip file is created in case the cloud function code has been modified. SHA hash code of the zip file is different (if the code has been modified). Thus, the local variable with the GCS object name gets different value. It means that the zip file is uploaded to the GCS bucket with the new name. As the source code object has now a new name <code>source_archive_object = google_storage_bucket_object.cf_source_zip.name</code>, terraform finds out that the cloud function has to be redeployed (because the state file has the old name of the archive object). The cloud function is redeployed.</p>
<p>On the other hand, if the code is not modified - the name <code>source_archive_object = google_storage_bucket_object.cf_source_zip.name</code> is not modified, so Terraform does not deploy anything.</p>
<p>Obviously, if other parameters are modified - the redeployment goes ahead anyway.</p>"
"Google cloudbuild secrets not substituted<p>I am trying to retrieve secrets from the secrets manager in the <code>cloudbuild.yaml</code> file but I can't find a way.</p>
<pre><code>- name: 'gcr.io/cloud-builders/gcloud'
    args:
      - beta
      - run
      - deploy
      - ${REPO_NAME}
      - --region=europe-west2
      - --image=gcr.io/$PROJECT_ID/${REPO_NAME}:$COMMIT_SHA
      - --service-account=${_SERVICE_ACCOUNT}
      - --cpu=2
      - --allow-unauthenticated
      - --set-env-vars=GCP_DB_INSTANCE_NAME=$$GCP_DB_INSTANCE_NAME
      - --set-env-vars=PG_DATABASE=$$PG_DATABASE
      - --set-env-vars=PG_PASSWORD=$$PG_PASSWORD
      - --set-env-vars=PG_USER=$$PG_USER
      - --set-env-vars=GCP_PROJECT=$$GCP_PROJECT
      - --set-env-vars=GCP_BUCKET_NAME=$$GCP_BUCKET_NAME
      - --add-cloudsql-instances=$$GCP_DB_INSTANCE_NAME
    secretEnv: [ 'GCP_DB_INSTANCE_NAME', 'PG_DATABASE', 'PG_PASSWORD', 'PG_USER', 'GCP_PROJECT', 'GCP_BUCKET_NAME' ]
availableSecrets:
  secretManager:
    - versionName: projects/$PROJECT_ID/secrets/GCP_DB_INSTANCE_NAME/versions/latest
      env: GCP_DB_INSTANCE_NAME
    - versionName: projects/$PROJECT_ID/secrets/PG_DATABASE/versions/latest
      env: PG_DATABASE
    - versionName: projects/$PROJECT_ID/secrets/PG_PASSWORD/versions/latest
      env: PG_PASSWORD
    - versionName: projects/$PROJECT_ID/secrets/PG_USER/versions/latest
      env: PG_USER
    - versionName: projects/$PROJECT_ID/secrets/GCP_PROJECT/versions/latest
      env: GCP_PROJECT
    - versionName: projects/$PROJECT_ID/secrets/GCP_BUCKET_NAME/versions/latest
      env: GCP_BUCKET_NAME
</code></pre>
<p>But the variables are not substituted. I have logged the values in my api and that is what I get:</p>
<pre><code>2021-08-05T22:31:33.437926Z key value PG_DATABASE $PG_DATABASE
2021-08-05T22:31:33.437965Z key value PG_USER $PG_USER
2021-08-05T22:31:33.437985Z key value PG_PASSWORD $PG_PASSWORD
2021-08-05T22:31:33.438063Z key value GCP_PROJECT $GCP_PROJECT
2021-08-05T22:31:33.438093Z key value GCP_BUCKET_NAME $GCP_BUCKET_NAME
</code></pre>
<p>How can I substitute the secrets in my step?</p>","<p>Instead of injecting these variables at <em>build</em> time, it would be better to inject them at <em>runtime</em>. As written, the secrets will be viewable in plaintext by anyone with permission to view the Cloud Run service. That's because they are resolved during the build step and set as environment variables. Furthermore, if you were to revoke or change one of these secrets, the Cloud Run service would continue to operate with the old value.</p>
<p>A better solution is to use the <a href=""https://cloud.google.com/run/docs/configuring/secrets"" rel=""nofollow noreferrer"">native Cloud Run Secret Manager integration</a>, which resolves secrets at instance boot. It would look like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>- name: 'gcr.io/cloud-builders/gcloud'
  args:
    - run
    - deploy
    - ${REPO_NAME}
    - --region=europe-west2
    - --image=gcr.io/$PROJECT_ID/${REPO_NAME}:$COMMIT_SHA
    - --service-account=${_SERVICE_ACCOUNT}
    - --cpu=2
    - --allow-unauthenticated
    - --set-secrets=GCP_DB_INSTANCE_NAME=projects/$PROJECT_ID/secrets/GCP_DB_INSTANCE_NAME:latest,PG_DATABASE=projects/$PROJECT_ID/secrets/PG_DATABASE:latest // continue
    - --add-cloudsql-instances=$$GCP_DB_INSTANCE_NAME
</code></pre>
<p>Cloud Run will automatically resolve the secrets when it boots a new instance. You'd need to grant <code>$SERVICE_ACCOUNT</code> permissions to access the secret.</p>"
"Changing directories in Cloud Build 'cd' no found<p>I'm using cloud build to clone a repository. I can confirm the repository clones successfully to the cloud build <code>/workspace</code> volume.</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - id: 'Clone repository'
    name: 'gcr.io/cloud-builders/git'
    args: ['clone', $_REPO_URL]
    volumes:
    - name: 'ssh'
      path: /root/.ssh
</code></pre>
<p>I then run the next step to confirm</p>
<pre class=""lang-yaml prettyprint-override""><code>  - id: 'List'
    name: 'alpine'
    args: ['ls']
</code></pre>
<p>and it shows me the repository is in the current directory. But when I try and <code>cd</code> into the directory the <code>cd</code> command doesn't work and throws an error:</p>
<pre><code>ERROR: build step 3 &quot;alpine&quot; failed: starting step container failed: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: &quot;cd &lt;repo-name&gt;&quot;: executable file not found in $PATH: unknown
</code></pre>
<p>My ultimate goal is to <code>cd</code> into the repository and run some git commands. I use alpine later on because the <code>git</code> builder image doesn't allow me to use <code>cd</code> either.</p>
<pre class=""lang-yaml prettyprint-override""><code>substitutions:
  _REPO_NAME: 'test-repo'
  _REPO_URL: 'git@bitbucket.org:example/test-repo.git'
  _BRANCH_NAME: 'feature/something'

steps:
  - id: 'Clone repository'
    name: 'gcr.io/cloud-builders/git'
    args: ['clone', $_REPO_URL]
    volumes:
    - name: 'ssh'
      path: /root/.ssh

  - id: 'Check Diff'
    name: 'alpine'
    args: ['cd $_REPO_NAME', '&amp;&amp;', 'git checkout $_BRANCH_NAME', '&amp;&amp;', 'git diff main --name-only']
</code></pre>","<p>You can use bash to run any commands you would like.
Here is one example I use for one of my projects:</p>
<pre><code>- name: 'gcr.io/cloud-builders/git'
  id: Clone env repository
  entrypoint: /bin/sh
  args:
  - '-c'
  - |
    git clone git@github.com:xyz/abc.git &amp;&amp; \
    cd gitops-env-repo/ &amp;&amp; \
    git checkout dev   
</code></pre>"
"Error publishing source code from cloud build to a bucket using triggers<p>I´m trying to publish the html code from one cloud source repository to a public storage bucket in gcp through a cloud build trigger . However , I get the following error in the build each time I push to the master branch.</p>
<pre><code>generic::invalid_argument: generic::invalid_argument: if 'build.service_account' is specified, the build must either (a) specify 'build.logs_bucket' (b) use the CLOUD_LOGGING_ONLY logging option, or (c) use the NONE logging option
</code></pre>
<p>I am using the following cloudbuild.yaml</p>
<pre><code>steps:
  - name: gcr.io/cloud-builders/gsutil
    args: [&quot;-m&quot;, &quot;rsync&quot;, &quot;-r&quot;, &quot;-c&quot;, &quot;-d&quot;, &quot;.&quot;, &quot;gs://somedomain.com&quot;]
</code></pre>
<p>I think this is related with the service account associated with the cloud build .</p>
<p>The tutorial I´m following for this solution is here : <a href=""https://cloud.google.com/community/tutorials/automated-publishing-cloud-build"" rel=""noreferrer"">https://cloud.google.com/community/tutorials/automated-publishing-cloud-build</a></p>","<p>The error was solved adding the logs specification at the end of the cloudbuild.yaml and enabling the IAM API .
The bucket and the cloud build configuration where in the same project so I didn´t have the need to grant additional roles to the cloud build service account .</p>
<p>straight under the steps put:</p>
<pre><code>options:
  logging: CLOUD_LOGGING_ONLY
</code></pre>"
"Cloud Build does not work anymore after upgrading to beam 2.30.0<p>i have been using this yaml file to kick off my dataflow flex workflow with beam 2.27.0, and it has always worked fine</p>
<pre><code>- name: gcr.io/$PROJECT_ID/$_IMAGE
  entrypoint: python
  args:
  - /dataflow/template/main.py
  - --runner=DataflowRunner
  - --project=$PROJECT_ID
  - --region=$_REGION
  - --job_name=$_JOB_NAME
  - --temp_location=$_TEMP_LOCATION
  - --sdk_container_image=gcr.io/$PROJECT_ID/$_IMAGE
  - --disk_size_gb=50
  - --year=2018 
  - --quarter=QTR1 
  - --fmpkey=$_FMPKEY
  - --setup_file=/dataflow/template/setup.py  
</code></pre>
<p>Today i decided to upgrade beam to 2.30.0, and when running exactly the same file i am now getting
this</p>
<pre><code>Unrecognized SDK container image: gcr.io/datascience-projects/pipeline:latestRun. Custom container images are only supportedfor Dataflow Runner v2.
</code></pre>
<p>Could anyone advise what i need to fix? I am suspecting i'd need to run  using a cloud-sdk instead of python.....</p>
<p>kind regards
marco</p>","<p>You will need to add <code>--experiment=use_runner_v2</code> as an argument, as the documentation is <a href=""https://cloud.google.com/dataflow/docs/guides/using-custom-containers#usage"" rel=""nofollow noreferrer"">covering</a>, when working with Apache Beam 2.30.0 or higher versions.</p>
<p>Therefore your updated yaml will look like the following:</p>
<pre><code>- name: gcr.io/$PROJECT_ID/$_IMAGE
  entrypoint: python
  args:
  - /dataflow/template/main.py
  - --runner=DataflowRunner
  - --project=$PROJECT_ID
  - --region=$_REGION
  - --job_name=$_JOB_NAME
  - --temp_location=$_TEMP_LOCATION
  - --sdk_container_image=gcr.io/$PROJECT_ID/$_IMAGE
  - --disk_size_gb=50
  - --year=2018 
  - --quarter=QTR1 
  - --fmpkey=$_FMPKEY
  - --setup_file=/dataflow/template/setup.py
  - --experiment=use_runner_v2
</code></pre>"
"gcloud alpha run deploy --set-secrets flag does not work: ""should be either `latest` or a positive integer""<p>I fail when trying to use injecting my cloud secret to my cloud run services as environment variable. I followed the documentation at <a href=""https://cloud.google.com/sdk/gcloud/reference/alpha/run/deploy#--set-secrets"" rel=""nofollow noreferrer"">https://cloud.google.com/sdk/gcloud/reference/alpha/run/deploy#--set-secrets</a></p>
<p>Here is the relevant portion of the <code>cloudbuild.yml</code> file:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: 'gcr.io/cloud-builders/docker'
    args: [ 'build', '-t', 'eu.gcr.io/$PROJECT_ID/backend:$BUILD_ID', '.' ]
  - name: 'gcr.io/cloud-builders/docker'
    args: [ 'push', 'eu.gcr.io/$PROJECT_ID/backend:$BUILD_ID' ]
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - 'alpha'
      - 'run'
      - 'deploy'
      - 'backend'
      - '--image=eu.gcr.io/$PROJECT_ID/backend:$BUILD_ID'
      - '--concurrency=10'
      - '--cpu=1'
      - '--memory=512Mi'
      - '--region=europe-west4'
      - '--min-instances=1'
      - '--max-instances=2'
      - '--platform=managed'
      - '--port=8080'
      - '--timeout=3000'
      - '--set-env-vars=SQL_CONNECTION=10.0.0.3, SQL_USER=root, SQL_PASSWORD=root, SQL_DATABASE=immobilien'
      - '--set-env-vars=^#^SPRING_PROFILES_ACTIVE=prod'
      - '--set-env-vars=MAIL_SMTP_HOST=smtp.foo.com'
      - '--set-env-vars=MAIL_SMTP_PORT=993'
      - '--set-env-vars=MAIL_SMTP_USER=root'
      - '--set-secrets=[MAIL_SMTP_PASSWORD=mail_smtp_password:1]'
      - '--ingress=internal'
      - '--vpc-connector=cloud-run'
      - '--vpc-egress=private-ranges-only'
      - '--set-cloudsql-instances=abc-binder-3423:europe-west4:data'

</code></pre>
<p>This is the error output:</p>
<pre><code>Step #2: Status: Downloaded newer image for gcr.io/google.com/cloudsdktool/cloud-sdk:latest
Step #2: gcr.io/google.com/cloudsdktool/cloud-sdk:latest
Step #2: Skipped validating Cloud SQL API and Cloud SQL Admin API enablement due to an issue contacting the Service Usage  API. Please ensure the Cloud SQL API and Cloud SQL Admin API are activated (see https://console.cloud.google.com/apis/dashboard).
Step #2: Deploying container to Cloud Run service [backend] in project [abc-binder-3423] region [europe-west4]
Step #2: Deploying...
Step #2: failed
Step #2: Deployment failed
Step #2: ERROR: (gcloud.alpha.run.deploy) should be either `latest` or a positive integer
Finished Step #2
ERROR
ERROR: build step 2 &quot;gcr.io/google.com/cloudsdktool/cloud-sdk:latest&quot; failed: step exited with non-zero status: 1
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
ERROR: (gcloud.builds.submit) build caf4fae8-daae-49d4-9349-6995b1f275e8 completed with status &quot;FAILURE&quot;
</code></pre>
<p>I do not understand what is meant by</p>
<pre><code>should be either `latest` or a positive integer
</code></pre>","<p>Don't surround the value with brackets.</p>
<pre><code>--set-secrets=MAIL_SMTP_PASSWORD=mail_smtp_password:1
</code></pre>"
"Cloud Build dotnet fails - does not support targeting version<p>Trying to build the aspnet core code with GCP Cloud Build.</p>
<p>the below <code>yaml</code>:</p>
<pre><code>steps:
- name: 'microsoft/dotnet:sdk'
  entrypoint: 'dotnet'
  args: [ 'publish', 'folder/proj.csproj', '-c', 'Release' ]
</code></pre>
<p>fails with next errors:</p>
<pre><code>/usr/share/dotnet/sdk/2.1.816/Sdks/Microsoft.NET.Sdk/targets/Microsoft.NET.TargetFrameworkInference.targets(150,5): error NETSDK1045: The current .NET SDK does not support targeting .NET Standard 2.1.  Either target .NET Standard 2.0 or lower, or use a version of the .NET SDK that supports .NET Standard 2.1. [/workspace/folder/common.csproj]
/usr/share/dotnet/sdk/2.1.816/Sdks/Microsoft.NET.Sdk/targets/Microsoft.NET.TargetFrameworkInference.targets(137,5): error NETSDK1045: The current .NET SDK does not support targeting .NET Core 3.1.  Either target .NET Core 2.1 or lower, or use a version of the .NET SDK that supports .NET Core 3.1. [/workspace/folder/proj.csproj]
</code></pre>
<p>Is there a way to support different version of dotnet?</p>","<p><a href=""https://cloud.google.com/build/docs/build-config-file-schema#name"" rel=""nofollow noreferrer"">Cloud Build</a> only pulls container images from certain registries, such as Docker Hub, Container Registry, Artifact Registry, etc. In this case, you would need to replace the image used in the <code>name</code> field in your script.</p>
<p>By using <code>microsoft/dotnet:sdk</code>, your script will be getting the default build which resulted in incompatibility. As per error it was using <code>2.1.816</code> whilst your application requires .NET 3.1.</p>
<p>With this, I recommend that you replace <code>microsoft/dotnet:sdk</code> with <code>mcr.microsoft.com/dotnet/sdk:3.1</code>. Refer to <a href=""https://hub.docker.com/_/microsoft-dotnet-sdk"" rel=""nofollow noreferrer"">Docker Hub Container Image Library</a> for more information.</p>"
"Update Cloud Build ref to show correct branch it ran on<p>I'm using a webhook trigger and part of the configuration requires setting a default branch. This webhook invokes on pull requests so when the trigger runs it check-out that branch.</p>
<p>Everything works great except that in the Cloud Build history it obviously doesn't show the branch it ran on but the default branch set in the configuration, ie. 'master'</p>
<p>Is it possible to update the <code>ref</code> during build to be the actual branch it executed on so there is a bit more clarity when reviewing build history?</p>
<p>Referring to this documentation here, have I found the correct variable and would reassigning it work?</p>
<pre><code>steps:
  - id: 'Setup Credentials'
    name: 'gcr.io/cloud-builders/git'
    entrypoint: '/bin/bash'
    args:
    - '-c'
    - |
      # checkout 'feature/my-branch' branch
      # do work on branch
      $_REF_EVENT_NAME='feature/my-branch' # overwrite the configured default branch

</code></pre>
<p><a href=""https://i.stack.imgur.com/tqJrN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tqJrN.png"" alt=""screenshot"" /></a></p>
<p>If its possible then I'd want to update the commit reference as well as that's from the masters last commit not the branch.</p>","<p>Actually, you can't. This ref column is important when you use other type of trigger, but for webhook, the value is generic and not updatable.</p>"
"What roles do my Cloud Build service account need to deploy an http triggered unauthenticated Cloud Function?<p>I was trying to deploy an http triggered Cloud Function with Cloud Build using this configuration.</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - beta
  - functions
  - deploy
  - myfunction
  - --source=start_shopify_installation
  - --trigger-http
  - --region=europe-west1
  - --runtime=nodejs14
  - --allow-unauthenticated
  - --ingress-settings=all
  - --security-level=secure-always
  - --set-secrets=env_1=secret_1:latest
</code></pre>
<p>When I got an error saying Cloud Build could't set an IAM policy.</p>
<pre><code>WARNING: Setting IAM policy failed, try:
gcloud alpha functions add-iam-policy-binding myfunction\
  --region=europe-west1 \
  --member=allUsers \
  --role=roles/cloudfunctions.invoker
</code></pre>
<p>The function gets deployed and when I check in the GCP console it looks like the <code>allUsers</code> member has the role <code>Cloud Functions Invoker</code>, but it doesn't have <code>allow unauthenticated</code> in the Authentication column. When I go to invoke the function I get a 'missing permissions' error.</p>
<p>When I execute the suggested command from my Cloud Shell it works just fine. However if I fill it in as an extra step in my deployment configuration that step fails.
I think that my Cloud Build service account must be missing a role in order to make the function accessible without authetication? Currently it has these roles: <code>Cloud Build Service Account</code>, <code>Cloud Functions Developer</code> and <code>Service Account User</code>.</p>
<p><strong>EDIT</strong></p>
<p>I added the <code>Project IAM Admin</code> role to the Cloud Build service account and tried again.
<a href=""https://i.stack.imgur.com/FcPBF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FcPBF.png"" alt=""Roles for my Cloud Build service account"" /></a>
Unfortunately, it didn't change anything.</p>","<p>I reproduced your error (warning) on my side and fixed it: I can see <code>allUsers</code> having <code>Cloud Functions Invoker</code> role in the function's PERMISSIONS tab.</p>
<p>In fact your cloud build service account needs the <code>cloudfunctions.functions.setIamPolicy</code> permission. <strong>So the solution is</strong> replace <code>Cloud Functions Developer</code> role with <code>Cloud Functions Admin</code> role.</p>
<blockquote>
<p>Use of the --allow-unauthenticated flag modifies IAM permissions. To ensure that unauthorized developers cannot modify function permissions, the user or service that is deploying the function must have the cloudfunctions.functions.setIamPolicy permission. This permission is included in both the Owner and Cloud Functions Admin roles.</p>
</blockquote>
<p>Ref: <a href=""https://cloud.google.com/functions/docs/securing/managing-access-iam#at_deployment"" rel=""nofollow noreferrer"">https://cloud.google.com/functions/docs/securing/managing-access-iam#at_deployment</a></p>"
"How to deploy Cloud Functions with secrets from Secret Manager using Cloud Build?<p>I have a Cloud Function that I want deployed in my CD pipeline using Cloud Build. The function needs a couple of secrets stored in Secret Manager that I want to pull in as environment variables using the <code>--set-secrets</code> flag.</p>
<p>When I deploy manually with the CLI I have no issue:</p>
<pre><code>gcloud beta functions deploy myfunction \
  --source src \
  --trigger-topic mytopic \
  --region europe-west1 \
  --runtime python39 \
  --set-secrets 'env_1=secret_1:latest','env_2=secret_2:latest'
</code></pre>
<p>However, when I try to deploy using Cloud Build with this configuration:</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - beta
  - functions
  - deploy
  - myfunction
  - --source=src
  - --trigger-topic=mytopic
  - --region=europe-west1
  - --runtime=python39
  - --set-secrets='env_1=secret_1:latest','env_2=secret_2:latest'
</code></pre>
<p>I get an error that the <code>--set-secrets</code> argument <code>must match the pattern 'SECRET:VERSION' or 'projects/{PROJECT}/secrets/{SECRET}:{VERSION}' or 'projects/{PROJECT}/secrets/{SECRET}/versions/{VERSION}' where VERSION is a number or the label 'latest'</code>. I don't understand why I get this error as I think my argument comforms to said pattern.</p>
<p>Is there something I am missing?</p>","<p>First, follow Guillaume's suggestion to remove the quotation marks  around each pair. Afterwards, it should look like this:</p>
<pre><code>--set-secrets=env_1=secret_1:latest,env_2=secret_2:latest
</code></pre>
<p>Or alternatively, my suggestion is to enclose all your arguments as a list like the example below. I tested the config below and it worked on my end.</p>
<pre><code>steps:
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  args: ['gcloud', 'beta','functions', 'deploy', 'myfunction', '--region=europe-west1', '--source=src', '--trigger-topic=mytopic', '--runtime=python39', '--set-secrets=env_1=secret_1:latest,env_2=secret_2:latest']
</code></pre>
<blockquote>
<p>Note: Do not put spaces in --set-secrets value if you have multiple secrets</p>
</blockquote>
<p>To learn more, check out this <a href=""https://cloud.google.com/build/docs/build-config-file-schema#args"" rel=""noreferrer"">documentation</a>.</p>"
"My build trigger is not recognizing the glob pattern<p>I have two folders in my main directory, cloud_functions and cloudbuild:</p>
<pre><code>├── cloud_functions
│   ├── batch_predict
│   │   ├── config.py
│       ├── main.py
│       ├── requirements.txt
│       └── utils.py
├── cloudbuild
│   ├── batch_predict_cloud_function
│   │   ├── config.yaml
│   │   ├── create_triggers.sh
</code></pre>
<p>In the Cloud build trigger I specified the glob patterns as:</p>
<ul>
<li>cloud_functions/batch_predict/**</li>
<li>cloudbuild/batch_predict_cloud_function/**</li>
</ul>
<p>This is accomplished with the following flag in the gcloud command that creates the trigger:</p>
<pre><code>--included-files=&quot;cloud_functions/batch_predict/**, cloudbuild/batch_predict_cloud_function/**&quot; \
</code></pre>
<p>I validated the globs are registered in the UI but changes in the cloudbuild folder don't trigger the build, any ideas why this might be happening?</p>","<p>To specify multiple glob patterns in the <code>gcloud</code> command you have to pass multiple arguments to the <code>--included-files</code> option using the following syntax:</p>
<pre><code>--included-files &quot;cloud_functions/batch_predict/**&quot;,&quot;cloudbuild/batch_predict_cloud_function/**&quot;
</code></pre>"
"Using 2 Dockerfiles in Cloud Build to re-use intermediary step image if CloudBuild fails<p>Cloud Build fails with Timeout Error (I'm trying to deploy CloudRun with Prophet). Therefore I'm trying to split the Dockerfile into two (saving the image in between in case it fails). I'd split the Dockerfile like this:</p>
<ul>
<li>Dockerfile_one: python + prophet's dependencies</li>
<li>Dockerfile_two: image_from_Dockerfile_one + prophet + other dependencies</li>
</ul>
<p>What should cloudbuild.yaml should look like to:</p>
<ol>
<li>if there is a previously image available skip the step, else run the step with the Dockerfile_one and save the image</li>
<li>use the image from the step (1), add more dependencies to it and save the image for deploy</li>
</ol>
<p>Here is how cloudbuild.yaml looks like right now</p>
<pre><code>steps:
#  create gcr source directory
- name: 'bash'
  args:
    - '-c'
    - |
      echo 'Creating gcr_source directory for ${_GCR_NAME}'
      mkdir _gcr_source
      cp -r cloudruns/${_GCR_NAME}/. _gcr_source

# Build the container image
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/$PROJECT_ID/${_GCR_NAME}', '.']
  dir: '_gcr_source'

# Push the container image to Container Registry
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/$PROJECT_ID/${_GCR_NAME}']

# Deploy container image to Cloud Run
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: gcloud
  args:
  - run
  - deploy
  - ${_GCR_NAME}
  - --image=gcr.io/$PROJECT_ID/${_GCR_NAME}
</code></pre>
<p>Thanks a lot!</p>","<p>You need to have 2 pipelines</p>
<ol>
<li>The first one create the base image. Like that, you can trigger it everytime that you need to rebuild this base image, with, possibly a different lifecycle than your application lifecycle. Something similar to that</li>
</ol>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/&lt;PROJECT_ID&gt;/base-image', '-f', 'DOCKERFILE_ONE', '.']
images: ['gcr.io/&lt;PROJECT_ID&gt;/base-image']
</code></pre>
<ol start=""2"">
<li>Then, in your second dockerfile, start from the base image and use a second Cloud Build pipeline to build, push and deploy it (as you do in your 3 last steps in your question)</li>
</ol>
<pre><code>FROM gcr.io/&lt;PROJECT_ID&gt;/base-image
COPY .....
....
...
</code></pre>"
"reading .env file from node - env file is not published<p>I am trying to read .env file using &quot;dotenv&quot; package but it returns undefined from process.env.DB_HOST after published to gcloud run.  I see all files except for the .env file in root directory when I output all files to log.  I do have .env file in my project on a root directory.  Not sure why it's not getting pushed to gcloud or is it?. I do get a value when I tested locally for process.env.DB_HOST.</p>
<p>I used this command to publish to google run.
gcloud builds submit --tag gcr.io/my-project/test-api:1.0.0 .</p>","<p>If you haven't a <a href=""https://cloud.google.com/sdk/gcloud/reference/topic/gcloudignore"" rel=""nofollow noreferrer""><code>.gcloudignore</code></a> file in your project, gcloud CLI use the <code>.gitignore</code> by default</p>
<p>Create a <code>.gcloudignore</code> and put the file that you don't want to upload when you use gcloud CLI command. So, don't put the <code>.env</code> in it!</p>
<hr />
<p><strong>EDIT 1</strong></p>
<p>When you add a <code>.gcloudignore</code>, the gcloud CLI no longer read the <code>.gitignore</code> file and use it instead.</p>
<p>Therefore, you can define 2 different logics</p>
<ul>
<li><code>.gitignore</code> list the file that you don't want to push to the repository. <strong>Put the <code>.env</code> file in it</strong> to NOT commit it</li>
<li><code>.gcloudignore</code> list the file that you don't want to send with the gcloud CLI. <strong>DON'T put the <code>.env</code> file in it</strong> to include it when you send your code with the gcloud CLI</li>
</ul>"
"Cloud build dynamic tags to help filter<p>I'm triggering a build on a web hook from a pull-request, and then it reads in the payload of the request. The payload has things like PR name, author, commit hash, etc.</p>
<p>I'd like to inject some of these into the tag attribute but their format doesn't comply:</p>
<pre><code>invalid build: invalid build tag &quot;Ari Example&quot;: must match format &quot;^[\\w][\\w.-]{0,127}$&quot;
</code></pre>
<p>Is there any way I could modify the tag dynamically, in the example above for a users name?</p>
<pre><code>Ari Example -&gt; ari-example
</code></pre>
<p>My <code>cloudbuild.yaml</code> looks like so:</p>
<pre class=""lang-yaml prettyprint-override""><code>substitutions:
# These a dynamically populated by the PR and substitutions
# below is just for illustrative purposes

  _PR_AUTHOR_ID: '827364872-23472634-2352'
  _PR_AUTHOR: 'Ari Example'
  _PR_TITLE: 'fix: update some file'

steps:
  - id: 'Pull Request Payload'
    name: 'gcr.io/cloud-builders/git'
    entrypoint: '/bin/bash'
    args:
    - '-c'
    - |
      echo Author: $_PR_AUTHOR (id: $_PR_AUTHOR_ID)
      echo Title:  $_PR_TITLE

tags: ['$_PR_AUTHOR']
</code></pre>","<p>Pure bash. Below you replace any spaces with dashes and convert the string to lowercase:</p>
<pre><code>_PR_AUTHOR='Ari Example'

echo ${_PR_AUTHOR// /-} | awk '{print tolower($0)}'
</code></pre>
<p>Output: ari-example</p>
<p>If you want to convert special characters and add more validation you might need to use regex.</p>"
"Is it possible to kick off two different cloud build which are based on subscription to same topic?<p>currently i have a cloud-build application which is being kicked off by a pub-sub trigger , subscribing to eg. topic1
I would like to know if i can kick off another cloud-build application from subscribing to the same topic. Is there a way to configure the message (or the trigger) so that if message1 is published to topic1, then cloudbuild1 is kicked off, and if message2 is published to topic1, then cloudbuild2 is kicked off?
Kind regards
marco</p>","<p>When you create a subscription on a topic, all the published messages in the topic are replicated in each subscription.</p>
<p>Therefore, if you have TOPIC and Sub1 and Sub2, if you publish 1 message in TOPIC, you will have this message in Sub1 and Sub2.</p>
<hr />
<p>However, you can set up a <a href=""https://cloud.google.com/pubsub/docs/filtering"" rel=""nofollow noreferrer"">filter on messages when you create a subscription</a>. You can set this filter only at the creation and you can't update it later. You need to delete and recreate the subscription if you want to update the filter.</p>
<p>In addition, you can filter only on message attributes, not on the message body content.</p>
<p>Therefore, with filter, think wisely your filter from the beginning and when you publish a message in TOPIC, add attributes that allow your to route the messages to the correct subscription.</p>"
"How to auto deploy latest image from Cloud Build in Cloud Run<p>My CI/CD pipeline for deploying my Angular app is close but I see that Google Cloud Run does not deploy a new revision when the container image has been updated.</p>
<p>I have Cloud Build set to trigger a build when a branch on GitHub has been updated. That is working fine and I see I get a new image named after the commit hash. I'm expecting Cloud Run to trigger its service, pick up the latest image and deploy it but it's not running. I'm not sure if I need to change the image names so they are not unique due to the SHA.</p>
<p>Jeremy</p>","<p>Add a Cloud Build step to deploy the new image to Cloud Run.</p>
<p>Modify this example with SERVICE_NAME, REGION, PROJECT_ID, and IMAGE.</p>
<pre><code># Deploy container image to Cloud Run
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: gcloud
  args: ['run', 'deploy', 'SERVICE-NAME', '--image', 'gcr.io/PROJECT_ID/IMAGE', '--region', 'REGION', '--platform', 'managed']
</code></pre>
<p><a href=""https://cloud.google.com/build/docs/deploying-builds/deploy-cloud-run"" rel=""noreferrer"">Documentation</a></p>"
"Cloud Run Container Not Building Possibly Due to Package Issue?<p>I am encountering an error when I am building a Cloud Run container from a particular directory in a GCP project. This particular directory has always built correctly in the past, and no changes were made to it since the last successful build.</p>
<p>Recently, however, I wanted to fire up a pipeline for this directory, so I added a few pound signs to the code (so the Cloud Build trigger could start building the Run container - additionally, the addition of these pound signs should not affect anything in the code). In the process, this error showed up:</p>
<pre><code>E: Unable to locate package python3.7-dev
E: Couldn't find any package by glob 'python3.7-dev'
E: Couldn't find any package by regex 'python3.7-dev'
The command '/bin/sh -c apt-get update   &amp;&amp; apt-get upgrade -y   &amp;&amp; apt-get install -y wget 
unzip xvfb libxtst6 libxrender1 python3.7-dev build-essential net-tools' returned a non-zero 
code: 100
</code></pre>
<p>Given that nothing changed, I'm not sure what caused this error. What could this error message mean, and what actions should I take to fix it? Any help is appreciated, and if more information is needed, I'll gladly update the question. Thanks!</p>
<p>EDIT: These are the dockerfile contents:</p>
<pre class=""lang-docker prettyprint-override""><code>FROM python:3.7-slim

# Allow statements and log messages to immediately appear in the Cloud Run logs
ENV PYTHONUNBUFFERED True

# install dependencies
RUN  apt-get update \
  &amp;&amp; apt-get upgrade -y \
  &amp;&amp; apt-get install -y wget unzip xvfb libxtst6 libxrender1 python3.7-dev build-essential net-tools
#RUN apt-get install -y procps

# set environment variables
ENV TWS_INSTALL_LOG=/root/Jts/tws_install.log \
    ibcIni=/root/ibc/config.ini \
    ibcPath=/opt/ibc \
    javaPath=/opt/i4j_jres \
    twsPath=/root/Jts \
    twsSettingsPath=/root/Jts

# make dirs
RUN mkdir -p /tmp &amp;&amp; mkdir -p ${ibcPath} &amp;&amp; mkdir -p ${twsPath}

# copy over IB Gateway
COPY Jts /root/Jts
COPY i4j_jres /home/mmr/.i4j_jres

# download IBC
RUN wget -q -O /tmp/IBC.zip https://github.com/IbcAlpha/IBC/releases/download/3.8.2/IBCLinux-3.8.2.zip
RUN unzip /tmp/IBC.zip -d ${ibcPath}
RUN chmod +x ${ibcPath}/*.sh ${ibcPath}/*/*.sh

# copy IBC/Jts configs
COPY ibc/config.ini ${ibcIni}

# copy cmd script
WORKDIR /home
COPY cmd.sh cmd.sh
RUN chmod +x cmd.sh

# set display environment variable (must be set after TWS installation)
ENV DISPLAY=:0


WORKDIR /home
COPY ib_server ./ib_server
COPY __init__.py .
COPY requirements.txt .
RUN pip install -r /home/requirements.txt

# execute cmd script to start Xvfb and gunicorn
CMD ./cmd.sh
# CMD tail -f /dev/null
</code></pre>
<p>The container creation is triggered when a change is made to the directory and pushed, which is why I had to add the pound signs and push to fire up the build.</p>","<p>The error lies in the python3.7-dev package you are trying to install in the container (line 9 of your Dockerfile), as the package no longer exists in the sources of the container. It has nothing to do with your change, or with Cloud Build, as probably the package was just removed from the sources since the last push.</p>
<p>If you need a python-dev package, I would recommend that you consider installing <code>python3-dev</code> especially, or <code>python3.9-dev</code>, as all changes in minor python versions should be backwards compatible with each other.</p>
<p>However if you really need <code>python3.7-dev</code>, you would need to manually recompile and install the packages, or add sources where it can be found and install it then. Bear in mind that the debian version this python image is based in has incompatibilities with this package you will need to manually solve, so it wouldn't be a straight forward solution.</p>"
"Cloud Build builds in worker pools stuck in Queued<p>We are using Google Cloud Build as CI/CD tool and we use private pools to be able to connect to our database using private IPs.</p>
<p>Since 08/27 our builds using private pools are stuck in <code>Queued</code> and are never executed ou fail due to timeout, they just hang there until we cancel them.</p>
<p>We have already tried without success:</p>
<ul>
<li><p>Change the worker pool to another region (from <code>southamerica-east1</code> to <code>us-central1</code>);</p>
</li>
<li><p>Recreate the worker pool with different configurations;</p>
</li>
<li><p>Recreate all triggers and connections.</p>
</li>
</ul>
<p>Removing the worker pool configuration (running the build in global) executed the build.</p>
<p>cloudbuild.yaml:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: Backup database
    args: ['gcloud', 'sql', 'backups', 'create', '--instance=${_DATABASE_INSTANCE_NAME}']

  - name: 'node:14.17.4-slim'
    id: Migrate database
    entrypoint: npm
    dir: 'build'
    args: ['...']
    secretEnv: ['DATABASE_URL']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: Migrate traffic to new version
    dir: 'build'
    entrypoint: bash
    args: ['-c', 'gcloud app services set-traffic ${_SERVICE_NAME} --splits ${_VERSION_NAME}=1']

availableSecrets:
  secretManager:
    - versionName: '${_DATABASE_URL_SECRET}'
      env: 'DATABASE_URL'

options:
  pool:
    name: 'projects/$PROJECT_ID/locations/southamerica-east1/workerPools/&lt;project-id&gt;'
</code></pre>
<p>our worker pool configuration:</p>
<pre><code>$ gcloud builds worker-pools describe &lt;worker-pool-id&gt; --region=southamerica-east1 --project=&lt;project-id&gt;

createTime: '2021-08-30T19:35:57.833710523Z'
etag: W/&quot;...&quot;
name: &lt;worker-pool-id&gt;
privatePoolV1Config:
  networkConfig:
    egressOption: PUBLIC_EGRESS
    peeredNetwork: projects/&lt;project-id&gt;/global/networks/default
  workerConfig:
    diskSizeGb: '1000'
    machineType: e2-medium
state: RUNNING
uid: ...
updateTime: '2021-08-30T20:14:13.918712802Z'
</code></pre>","<p>It was my last week discussion with the Cloud Build PM... TL;DR: if you haven't support subscription, or a corporate account, you can't (for now)</p>
<p>In detail, you can check the 1. link of RJC, you will get that
<a href=""https://i.stack.imgur.com/DYY0r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DYY0r.png"" alt=""enter image description here"" /></a></p>
<p>If you have a closer look, you can see (with my personal account, even if I have an Organization structure) the Concurrent Builds per worker pool is set to 0. That is the reason of your infinite queue of your build job.</p>
<p>The most annoying part is this one. Click on a Concurrent build per worker pool line checkbox and then click on edit, to change the limit. Here what you get</p>
<p><a href=""https://i.stack.imgur.com/ZPbGb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZPbGb.png"" alt=""enter image description here"" /></a></p>
<p>Read carefully: set a limit between 0 and 0.</p>
<p>Therefore, if you haven't support subscription (like me) you can't use the feature with your personal account. <em>I was able to use it with my corporate account, even if I shouldn't...</em></p>
<p>For now, I haven't a solution, only this latest message from the PM</p>
<blockquote>
<p>The behaviour around quota restrictions in private pools is a recent change that we're still iterating on and appreciate the feedback to make it easier for personal accounts to try out the feature.</p>
</blockquote>"
"Worker pod resource limit in spring batch and spring cloud deployer kubernetes<p>I am trying to run a Spring Batch application in kubernetes cluster. I am able to enforce resource limits to the main application pod by placing the following snippet in the deployment yaml:</p>
<pre><code>resources:
  limits:
    cpu: 500m
    ephemeral-storage: 500Mi
    memory: 250Mi
</code></pre>
<p>These settings are getting applied and can be seen in the pod yaml (kubectl edit pod batch).</p>
<p>However, these limits are not propagated to the worker pods. I tried adding the following properties in configmap of batch to set the cpu and memory limits:</p>
<pre><code>SPRING.CLOUD.DEPLOYER.KUBERNETES.CPU: 500m
SPRING.CLOUD.DEPLOYER.KUBERNETES.MEMORY: 250Mi
</code></pre>
<p>However, the worker pods are not getting these limits. I tried providing the following env variables too, but still the limits were not applied to the worker pod:</p>
<pre><code>SPRING_CLOUD_DEPLOYER_KUBERNETES_CPU: 500m
SPRING_CLOUD_DEPLOYER_KUBERNETES_MEMORY: 250Mi
</code></pre>
<p>The versions involved are:</p>
<ul>
<li>Spring Boot: 2.1.9.RELEASE</li>
<li>Spring Cloud: 2020.0.1</li>
<li>Spring Cloud Deployer: 2.5.0</li>
<li>Spring Cloud Task: 2.1.1.RELEASE</li>
<li>Kubernetes: 1.21</li>
</ul>
<p>How can I set these limits?</p>
<p>EDIT: Adding code for DeployerPartitionerHandler:</p>
<pre><code>public PartitionHandler partitionHandler(TaskLauncher taskLauncher, JobExplorer jobExplorer) {

    Resource resource = this.resourceLoader.getResource(resourceSpec);

    DeployerPartitionHandler partitionHandler = new DeployerPartitionHandler(taskLauncher, jobExplorer, resource,
            &quot;worker&quot;);

    commandLineArgs.add(&quot;--spring.profiles.active=worker&quot;);
    commandLineArgs.add(&quot;--spring.cloud.task.initialize.enable=false&quot;);
    commandLineArgs.add(&quot;--spring.batch.initializer.enabled=false&quot;);
    commandLineArgs.add(&quot;--spring.cloud.task.closecontext_enabled=true&quot;);
    commandLineArgs.add(&quot;--logging.level.root=DEBUG&quot;);

    partitionHandler.setCommandLineArgsProvider(new PassThroughCommandLineArgsProvider(commandLineArgs));
    partitionHandler.setEnvironmentVariablesProvider(environmentVariablesProvider());
    partitionHandler.setApplicationName(appName + &quot;worker&quot;);
    partitionHandler.setMaxWorkers(maxWorkers);

    return partitionHandler;
}

@Bean
public EnvironmentVariablesProvider environmentVariablesProvider() {
    return new SimpleEnvironmentVariablesProvider(this.environment);
}
</code></pre>","<p>Since the <code>DeployerPartitionHandler</code> is created using the <code>new</code> operator in the <code>partitionHandler</code> method, it is not aware of the values from the properties file. The <code>DeployerPartitionHandler</code> provides a setter for <code>deploymentProperties</code>. You should use this parameter to specify deployment properties for worker tasks.</p>
<p>EDIT: Based on comment by Glenn Renfro</p>
<p>The deployment property should be <code>spring.cloud.deployer.kubernetes.limits.cpu</code> and not <code>spring.cloud.deployer.kubernetes.cpu</code>.</p>"
"Google Cloud Build Windows Builder Error ""The system cannot find the path specified""<p>I am trying to implement automatic deployments for my Windows Kubernetes container app. I'm following instructions from Google's <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/windows-builder"" rel=""nofollow noreferrer"">windows-builder</a>. The trigger completes without an explicit error, but it doesn't initiate the docker build &amp; push. No update has been made in Container Registry. The closest thing I can find to an error message is &quot;The system cannot find the path specified.&quot; Screenshot below:</p>
<p><a href=""https://i.stack.imgur.com/eStq2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eStq2.png"" alt=""enter image description here"" /></a></p>
<p>The container, gcr.io/[my-project-id]/windows-builder, definitely exists, and it's located in the same GCP project as the Cloud Build trigger, just as the windows-builder documentation commanded.</p>
<p>I'm guessing that it's unable to find either my build.ps1 file or my worker Dockerfile. I can't understand why, though. I structured my code based on Google's <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/windows-builder/images/docker-windows"" rel=""nofollow noreferrer"">docker-windows example</a>. Here is my repository file structure:</p>
<pre><code>repository
   cloud build.yaml
   builder.ps1
   worker
      Dockerfile
</code></pre>
<p>Here is my cloudbuild.yaml:</p>
<pre><code>steps:
# WORKER
- name: 'gcr.io/[my-project-id]/windows-builder'
  args: [ '--create-external-ip', 'true', '--command', 'powershell.exe -file build.ps1' ]

# OPTIONS
options:
  logging: CLOUD_LOGGING_ONLY
</code></pre>
<p>Here is my builder.ps1:</p>
<pre><code>docker build -t gcr.io/[my-project-id]/test-worker ./worker;
if ($?) {
  docker push gcr.io/[my-project-id]/test-worker;
}
</code></pre>
<p>Here is my worker Dockerfile:</p>
<pre><code>FROM gcr.io/[my-project-id]/test-windows-node-base:onbuild
</code></pre>
<p>Does anybody know what I'm doing wrong here? Any help would be appreciated.</p>","<p>I figured it out. I removed the &quot;true&quot; argument for my create-external-ip flag from the cloudbuild.yaml command and that did the trick. Working file is below.</p>
<pre><code>steps:
# WORKER
- name: 'gcr.io/[my-project-id]/windows-builder'
  args: [ '--create-external-ip', '--command', 'powershell.exe -file build.ps1' ]

# OPTIONS
options:
  logging: CLOUD_LOGGING_ONLY
</code></pre>"
"How to retain existing env variables in a new shell<p>I know I must be doing something silly here, but I'm trying to pass environment variables to a command being run under <code>/bin/sh -c</code> in Cloud Build.</p>
<p>My Cloud Build file looks like this:</p>
<pre><code> - id: db-migrate
  name: node:16.13.0
  dir: 'packages/backend'
  entrypoint: '/bin/sh'
  args: 
 - '-c'
 - '(/workspace/cloud_sql_proxy -dir=/workspace -instances=$_DB_CONNECTION=tcp:127.0.0.1:5432 &amp; sleep 2) &amp;&amp; yarn db:migrate'
  env:
 - 'DB_HOST=127.0.0.1'
 - 'DB_USER=$_DB_USER'
 - 'DB_PASSWORD=$_DB_PASSWORD'
 - 'DB_NAME=$_DATABASE'
</code></pre>
<p>My Cloud Build Trigger has the substitutions set, and when I look at the build details it shows the environment variables as set.</p>
<p>However the command <code>yarn db:migrate</code> acts as if there are no env variables set. I believe this is because they aren't being passed from the machine to the command.</p>
<p>Any idea what I'm doing wrong?</p>
<p>The problem here is that when we call bin/sh it's creating a new shell with it's own environment variables. While I read through the manual on SH/Dash I will leave this question here:
<strong>How do I retain existing env variables in a new shell?</strong></p>","<p>Alright, I figured this out.</p>
<p>We were using TypeORM, and originally used a ormconfig.json file. Turns out, this was still being picked up somehow on the system and was overriding all env variables.</p>
<p>Posting this response to help others in case they make this same mistake.</p>"
"Running Pytest on files mentioned in a Github Pull Request using Cloud Build Trigger<p>I have a Cloud Build trigger which runs only when a new Pull Request is created on master branch of a Github repo.</p>
<p>However, right now, pytest is running on the whole repo, while I require pytest to be run only on files that have been created/edited in the Pull Request.</p>
<p>My cloudbuild file is as follows:</p>
<pre><code>#Testing
steps:
- name: 'python:3.7'
  id: Testing
  entrypoint: 'bash'
  args:
    - '-c'
    - |
       pip install --upgrade pip \
       &amp;&amp; pip install -r requirements.txt \
       &amp;&amp; pytest **/test_*.py
</code></pre>
<p>Any idea how to implement this?</p>","<p>I believe that Cloud Build does not expose the list of modified files in pull requests, but as a workaround you can use Github's webhooks.</p>
<ol>
<li><p>Get the “_PULL_REQUEST_ID” in cloud build using substitution process as per <a href=""https://cloud.google.com/build/docs/configuring-builds/use-bash-and-bindings-in-substitutions#payload_bindings"" rel=""nofollow noreferrer"">documentation</a>
<code>_PULL_REQUEST_ID $(pull_request.pull_request.id)</code></p>
</li>
<li><p>Use the above pull request id received in the substitution process in github rest api and get the list of modified/created files.</p>
<p>a) To know how this api works refer <a href=""https://developer.github.com/v3/pulls/#list-pull-requests-files"" rel=""nofollow noreferrer"">api link</a>.</p>
<p>b) How to use rest api refer <a href=""https://docs.github.com/en/rest/guides/getting-started-with-the-rest-api"" rel=""nofollow noreferrer"">rest link</a>.</p>
<p>c) Refer to example code at github <a href=""https://github.community/t/get-list-of-files-on-pull-request-merge/17226"" rel=""nofollow noreferrer"">github link</a>.</p>
<p>Update GITHUB_REPOSITORY to $REPO_NAME as per <a href=""https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values"" rel=""nofollow noreferrer"">doc</a> and update github.event.pull_request.number to the number received in step 1</p>
<p><code> URL=&quot;https://api.github.com/repos/${GITHUB_REPOSITORY}/pulls/${{ github.event.pull_request.number }}/files&quot;
FILES=$(curl -s -X GET -G $URL | jq -r '.[] | .filename')
echo “$FILES” &gt; modified.txt </code></p>
</li>
<li><p>As per above example code, list of files will be saved to modified.txt. As you get the list of modified/created files you can perform tests on those by getting the file names from modified.txt by using bash while loops or python loop.</p>
</li>
</ol>"
"(GCP) ERROR: (gcloud.builds.submit) INVALID_ARGUMENT: could not resolve source: googleapi: Error 403: 354778943856@cloudbuild.gserviceaccount.com<p>I'm trying to upload a docker image to <strong>GCR(Google Container Registry)</strong> with <strong>Cloud Shell</strong> but I got this error message:</p>
<blockquote>
<p>ERROR: (gcloud.builds.submit) INVALID_ARGUMENT:
could not resolve source: googleapi: Error 403:
354778943856@cloudbuild.gserviceaccount.com does not have
storage.objects.get access to the Google Cloud Storage object.,
forbidden</p>
</blockquote>
<p>This is the steps which I've done.</p>
<p><strong>(1st step)</strong> I ran this command:</p>
<pre><code>gcloud builds submit --tag gcr.io/my_project_id/hello_world
</code></pre>
<p><strong>(2nd step)</strong> Then, I was asked whether or not enabling and retrying <strong>&quot;API [cloudbuild.googleapis.com]&quot;</strong> then, I put and ran <strong>&quot;y&quot;</strong>:</p>
<blockquote>
<p>Creating temporary tarball archive of 2 file(s) totalling 478 bytes
before compression. Uploading tarball of [.] to
[gs://my_project_id_cloudbuild/source/1642137449.192753-983fc894e2f24fa086f55fa3b56d58aa.tgz]
API [cloudbuild.googleapis.com] not enabled on project [354778943856].
Would you like to enable and retry (this will take a few minutes)?
(y/N)?  y</p>
</blockquote>
<p><strong>(3rd step)</strong> Finally, I got this message with the error message:</p>
<blockquote>
<p>Enabling service [cloudbuild.googleapis.com] on project
[354778943856]... Operation
&quot;operations/acf.p2-354778943856-e99f6fd8-78ec-4cbd-94a2-07e0697d5455&quot;
finished successfully. ERROR: (gcloud.builds.submit) INVALID_ARGUMENT:
could not resolve source: googleapi: Error 403:
354778943856@cloudbuild.gserviceaccount.com does not have
storage.objects.get access to the Google Cloud Storage object.,
forbidden</p>
</blockquote>
<p>Even though I enabled <strong>&quot;API [cloudbuild.googleapis.com]&quot;</strong> by running <strong>&quot;y&quot;</strong>, I got the error message.</p>
<blockquote>
<p>ERROR: (gcloud.builds.submit) INVALID_ARGUMENT:
could not resolve source: googleapi: Error 403:
354778943856@cloudbuild.gserviceaccount.com does not have
storage.objects.get access to the Google Cloud Storage object.,
forbidden</p>
</blockquote>
<p>Are there any ways to solve this error?</p>","<p>Did you find the message below at the end of the message of <strong>2nd step</strong>?:</p>
<blockquote>
<p>Would you like to enable and retry (this will take a few minutes)?
(y/N)?  y</p>
</blockquote>
<p>As the message says, it takes <strong>a few minutes</strong> after enabling <strong>&quot;API [cloudbuild.googleapis.com]&quot;</strong> by running <strong>&quot;y&quot;</strong>.</p>
<p>So run the command again a few minutes after enabling <strong>&quot;API [cloudbuild.googleapis.com]&quot;</strong> by running <strong>&quot;y&quot;</strong>:</p>
<pre><code>gcloud builds submit --tag gcr.io/my_project_id/hello_world
</code></pre>
<p>Then, it will be successful:</p>
<blockquote>
<p>ID: f4478e51-557b-407d-9c30-c379ef707258 CREATE_TIME:
2022-01-14T05:22:29+00:00 DURATION: 19S SOURCE:
gs://my_project_id_cloudbuild/source/1642137748.745566-d75b61b6c6bc4acb9aba900650f201b2.tgz
IMAGES: gcr.io/my_project_id/hello_world(+1 more) STATUS: SUCCESS</p>
</blockquote>"
"How do I programmatically find and delete a container image built through Google Cloud Build with the googleapis Artifacts API?<p>My goal is to find and delete a Docker container image that I previously built through Google Cloud Build (via the Node googleapis library, using the Artifacts API). I have the information passed back to me when I built the container, and I'd like to delete it via the Node googleapis library.</p>
<p>The image path returned by Cloud Build looks like this: &quot;us-central1-docker.pkg.dev/project-1234/dev/foo:bar&quot;</p>
<p>I've tried unsuccessfully to delete the artifact as follows:</p>
<pre><code>const artifacts = artifactregistry({ version: 'v1beta2', auth: authClient});
await artifacts.projects.locations.repositories.packages.delete({name: config.containerLocation})
</code></pre>
<p>This results in an error: <em>The requested URL /v1beta2/us-central1-docker.pkg.dev/project-1234/dev/foo:bar was not found on this server.</em> It looks like I am not designating the location properly, but it's not clear how to resolve that.</p>
<p>How should I delete the image?</p>","<p>I was able to delete a package using Artifact Registry Cloud Client Library as this is now the recommended way to access the APIs since it is much easier to use and well documented. See <a href=""https://cloud.google.com/apis/docs/client-libraries-explained#cloud-client-libraries"" rel=""nofollow noreferrer"">Cloud Client Library docs</a>.</p>
<p>The code below shows how to delete a package in Artifact Registry, given that you already know what package to delete. This is defined in <code>namePackage</code> variable and be passed to <code>callDeletePackage()</code> which will call <a href=""https://cloud.google.com/nodejs/docs/reference/artifact-registry/latest/artifact-registry/v1beta2.artifactregistryclient#_google_cloud_artifact_registry_v1beta2_ArtifactRegistryClient_deletePackage_member_1_"" rel=""nofollow noreferrer"">deletePackage()</a> to proceed with the delete.</p>
<pre><code>const {ArtifactRegistryClient} = require('@google-cloud/artifact-registry');
        const client = new ArtifactRegistryClient();
async function main() {

        const projectId = 'project-id';
        const location = 'us-central1';
        const repositoryName = 'your-repository-name';
        const packageName = 'your-package-name';
        var namePackage = `projects/${projectId}/locations/${location}/repositories/${repositoryName}/packages/${packageName}`;
      
        callDeletePackage(namePackage);

}
async function callDeletePackage(name) {
        const deletePack = await client.deletePackage({name: name});
        console.log(deletePack);
}
main();
</code></pre>
<hr />
<p>If ever you need to programmatically get your repositories and packages you can check this code. It has <code>listsRepositories()</code>, <code>listsPackages()</code> and <code>callDeletePackage()</code>.</p>
<ul>
<li><code>listsRepositories()</code> lists the available repositories, you can choose which repository has the package to delete.</li>
<li><code>listsPackages()</code> to list the packages of the selected repository</li>
<li><code>callDeletePackage()</code> to delete the package.</li>
</ul>
<p>Code below just gets the 1st element in the returned list for both repository and package and delete afterwards. Its up to you on how to get your desired package to delete.</p>
<pre><code>const {ArtifactRegistryClient} = require('@google-cloud/artifact-registry');
        const client = new ArtifactRegistryClient();
async function main() {

        const projectId = 'your-project-id';
        const location = 'us-central1';
        var parentRepo = `projects/${projectId}/locations/${location}`;

        names = await listsRepositories(parentRepo);
        var nameRepo = names[0];
        console.log(nameRepo);
        var repos = await listsPackages(nameRepo);
        console.log(repos[0])
        callDeletePackage(repos[0]);
}
async function listsRepositories(parent) {
        var nameArr = []
        const repositories = await client.listRepositories({parent : parent});
        const repo = repositories[0];
        for (const data in repo) {
                nameArr.push(repo[data].name);
        }
        return nameArr;
}

async function listsPackages(parent) {
        var namePackArr = []
        const packages = await client.listPackages({parent : parent});
        const pack = packages[0];
        for (const data in pack) {
                namePackArr.push(pack[data].name);
        }
        return namePackArr;
}

async function callDeletePackage(name) {
        const deletePack = await client.deletePackage({name: name});
        console.log(deletePack);
}

main();

</code></pre>"
"Google Cloud Build windows builder error ""Failed to get external IP address: Could not get external NAT IP from list""<p>I am trying to implement automatic deployments for my Windows Kubernetes container app. I'm following instructions from the Google's <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/windows-builder"" rel=""nofollow noreferrer"">windows-builder</a>, but the trigger quickly fails with this error at about 1.5 minutes in:</p>
<pre><code>2021/12/16 19:30:06 Set ingress firewall rule successfully
2021/12/16 19:30:06 Failed to get external IP address: Could not get external NAT IP from list
ERROR
ERROR: build step 0 &quot;gcr.io/[my-project-id]/windows-builder&quot; failed: step exited with non-zero status: 1
</code></pre>
<p>The container, gcr.io/[my-project-id]/windows-builder, definitely exists and it's located in the same GCP project as the Cloud Build trigger just as the windows-builder documentation commanded.</p>
<p>I structured my code based off of Google's <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/windows-builder/images/docker-windows"" rel=""nofollow noreferrer"">docker-windows example</a>. Here is my repository file structure:</p>
<pre><code>repository
   cloudbuild.yaml
   builder.ps1
   worker
      Dockerfile
</code></pre>
<p>Here is my cloudbuild.yaml:</p>
<pre><code>steps:
# WORKER
- name: 'gcr.io/[my-project-id]/windows-builder'
  args: [ '--command', 'powershell.exe -file build.ps1' ]

# OPTIONS
options:
  logging: CLOUD_LOGGING_ONLY
</code></pre>
<p>Here is my builder.ps1:</p>
<pre><code>docker build -t gcr.io/[my-project-id]/test-worker ./worker;
if ($?) {
  docker push gcr.io/[my-project-id]/test-worker;
}
</code></pre>
<p>Here is my Dockerfile:</p>
<pre><code>FROM gcr.io/[my-project-id]/test-windows-node-base:onbuild
</code></pre>
<p>Does anybody know what I'm doing wrong here? Any help would be appreciated.</p>","<p>Replicated the steps from GitHub and got the same error. It is throwing <code>Failed to get external IP address...</code> error because the External IP address of the VM is disabled by default in the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/blob/8c5d1ea98e86cd25823e83b1f436aa13d359a775/windows-builder/builder/main.go#L37"" rel=""nofollow noreferrer"">source code</a>. I was able to build it successfully by adding <code>'--create-external-ip', 'true'</code> in <code>cloudbuild.yaml</code>.</p>
<p>Here is my <code>cloudbuild.yaml</code>:</p>
<pre><code>steps:
- name: 'gcr.io/$PROJECT_ID/windows-builder'
  args: [  '--create-external-ip', 'true',
           '--command', 'powershell.exe -file build.ps1' ]
</code></pre>"
"How to set the content-type, when uploading to Cloud Storage bucket?<p>When uploading an <code>*.tar.gz</code> build artifact to Cloud Storage bucket, it wrongfully applies MIME type <code>application/tar</code>, while it would have to apply MIME type <code>application/tar+gzip</code> (or the official MIME type <code>application/gzip</code>), in order to be able to download and extract the uploaded  <code>*.tar.gz</code> archive then again. This only works, when I manually set the MIME type afterwards (in the object details), but I'd be looking for a way, to define the proper MIME type right away. How this can be done?</p>
<p>The <code>cloudbuild.yaml</code> which produces the issue roughly looks alike this:</p>
<pre><code>steps:
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:latest'
    entrypoint: 'bash'
    args:
      - '-c'
      - |-
      - tar -zcf ./test_${SHORT_SHA}.tar.gz ./$_UPLOAD_DIRNAME
    env:
      - '_UPLOAD_DIRNAME=$_UPLOAD_DIRNAME'
      - 'SHORT_SHA=$SHORT_SHA'
artifacts:
  objects:
    location: 'gs://some-bucket/'
    paths: ['*.tar.gz']
</code></pre>","<p>After reading: <a href=""https://cloud.google.com/storage/docs/gsutil/addlhelp/WorkingWithObjectMetadata"" rel=""nofollow noreferrer"">Working With Object Metadata</a>, it turned out that the <code>artifacts</code> node won't suffice.
<code>gsutil</code> seems to be the only way to explicitly pass the desired MIME type <code>application/tar+gzip</code>:</p>
<pre><code>steps:
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:latest'
    entrypoint: 'bash'
    args:
      - '-c'
      - |-
        tar -zcf ./test_${SHORT_SHA}.tar.gz ./$_UPLOAD_DIRNAME
        gsutil -h &quot;Content-Type:application/tar+gzip&quot; cp ./test_${SHORT_SHA}.tar.gz ${_GOOGLE_STORAGE_BUCKET}
        gsutil ls -L ${_GOOGLE_STORAGE_BUCKET}test_${SHORT_SHA}.tar.gz
    env:
      - '_UPLOAD_DIRNAME=$_UPLOAD_DIRNAME'
      - '_GOOGLE_STORAGE_BUCKET=$_GOOGLE_STORAGE_BUCKET'
      - 'SHORT_SHA=$SHORT_SHA'
</code></pre>"
"Build a container image from inside a cloud function<p>Context: I am training a very similar model per bigquery dataset in Google Vertex AI, but I want to have a custom training image for each existing dataset (in Google BigQuery). In that sense, I need to programatically build a custom Docker Image in the container registry on demand. My idea was to have a Google Cloud Function do it, being triggered by PubSub topic with information regarding which dataset I want to build the training container for. So naturally, the function will write the Dockerfile and pertinent scripts to a /tmp folder within Cloud Functions (the only writable place as per my knowledge). However, when I try to actually build the container within this script, apparently, it doesn't find the /tmp folder or its contents, even though they are there (checked with logging operations).</p>
<p>The troubling code so far:</p>
<pre class=""lang-py prettyprint-override""><code>def build_container(dataset=str):

    with open('container_template/Dockerfile','r') as f:
        dockerfile = f.read()
    dockerfile = dockerfile.replace('@dataset',dataset)
    f.close()

    os.makedirs(os.path.dirname('/tmp/script-location'), exist_ok=True)

    with open('/tmp/Dockerfile','w') as docker_config:
        docker_config.write(dockerfile)

    docker_config.close()

    shutil.copy('container_template/script-location/script.py','/tmp/script-location/script.py')

    build_client = cloudbuild_v1.CloudBuildClient()

    build = cloudbuild_v1.Build()

    build.steps = [{'name':'gcr.io/cloud-builders/docker',
                    'args':['build', '-t', 'us-central1-docker.pkg.dev/myproject/myrepo/imagename:latest','/tmp']},
                    {'name':'gcr.io/cloud-builders/docker',
                    'args':['push', 'us-central1-docker.pkg.dev/myproject/myrepo/imagename:latest']}]  

    build_operation = build_client.create_build(project_id=myprojectid,build=build)

    build_result = build_operation.result()

    logger.info('Build Result: {}'.format(build_result.status))
</code></pre>
<p>When I check the cloud build logs I get:
Step #0: unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /tmp/Dockerfile: no such file or directory</p>","<p>I've locally tested building a container image using <a href=""https://cloud.google.com/build/docs/api/reference/libraries"" rel=""nofollow noreferrer"">Cloud Build Client Python library</a>. It turns out to have the same error even the <code>Dockerfile</code> file is existing in current directory:</p>
<p><strong>error:</strong></p>
<blockquote>
<p>Step #0: unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /workspace/Dockerfile: no such file or directory</p>
</blockquote>
<p><strong>build steps:</strong></p>
<pre class=""lang-py prettyprint-override""><code>    build_client = cloudbuild_v1.CloudBuildClient()

    build = cloudbuild_v1.Build()

    build.steps = [{'name':'gcr.io/cloud-builders/docker',
                    'args':['build', '-t', 'us-central1-docker.pkg.dev/myproject/myrepo/imagename:latest','.']},
                    {'name':'gcr.io/cloud-builders/docker',
                    'args':['push', 'us-central1-docker.pkg.dev/myproject/myrepo/imagename:latest']}]  

    build_operation = build_client.create_build(project_id=myprojectid,build=build)

    build_result = build_operation.result()
</code></pre>
<p>Since it uses the API method I followed this <a href=""https://cloud.google.com/build/docs/running-builds/start-build-command-line-api#running_builds"" rel=""nofollow noreferrer"">documentation</a>. You will see the <code>source</code> is present in API method. It is the missing key to move forward the problem. In <a href=""https://googleapis.dev/python/cloudbuild/latest/cloudbuild_v1/types.html?highlight=build#google.cloud.devtools.cloudbuild_v1.types.StorageSource"" rel=""nofollow noreferrer""><code>StorageSource</code></a>, you must to specify the <code>bucket</code> and <code>object_</code>. You need to compress your source code and upload it in Cloud Storage bucket. For example:</p>
<ol>
<li>Run the following command to compress your source code:</li>
</ol>
<pre><code>tar -cvzf sourcecode.tar.gz .
</code></pre>
<ol start=""2"">
<li>Upload to Cloud Storage bucket(you can use Cloud Build bucket):</li>
</ol>
<pre><code>gsutil cp sourcecode.tar.gz gs://myproject_cloudbuild
</code></pre>
<ol start=""3"">
<li><strong>build.source</strong>:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>    build_client = cloudbuild_v1.CloudBuildClient()

    build = cloudbuild_v1.Build()
    
    build.source = {&quot;storage_source&quot;:{&quot;bucket&quot;:&quot;myproject_cloudbuild&quot;, &quot;object_&quot;:&quot;gs://myproject_cloudbuild/sourcecode.tar.gz&quot;}}

    build.steps = [{'name':'gcr.io/cloud-builders/docker',
                    'args':['build', '-t', 'us-central1-docker.pkg.dev/myproject/myrepo/imagename:latest','.']},
                    {'name':'gcr.io/cloud-builders/docker',
                    'args':['push', 'us-central1-docker.pkg.dev/myproject/myrepo/imagename:latest']}]  

    build_operation = build_client.create_build(project_id=myprojectid,build=build)

    build_result = build_operation.result()
</code></pre>
<p>As a result it solved the problem building an image using Client Library. I recommended to do all of this inside your Cloud Function.</p>"
"How to build TypeScript in Google Cloud Build when using Google Cloud Functions<p>I have TypeScript functions that I'm building locally and then submitting to Google Cloud Functions, at which point they go through Google Cloud Build. I'd like to have all my building done in a single place. Is there any way to compile TypeScript in Google Cloud Build when using Google Cloud Functions?</p>","<p>Submitting code to Google Cloud Functions goes through one of the following HTTP calls, respectively for creating new functions and modifying existing functions:</p>
<pre><code>POST https://cloudfunctions.googleapis.com/v1/{location}/functions
PATCH https://cloudfunctions.googleapis.com/v1/{function.name}
</code></pre>
<p>This triggers a build in Google Cloud Build using a hardcoded <a href=""https://buildpacks.io/docs/concepts/components/builder/"" rel=""nofollow noreferrer"">builder</a>. Assuming a Yarn project, there is no extension point at any part of this process.</p>
<p>In particular, the builder runs <code>NODE_ENV=production yarn install --non-interactive --frozen-lockfile</code> (and does little else). Specifying <code>NODE_ENV=production</code> runs <code>yarn install</code> without lifecycle scripts, so you can't just define a <code>prepare</code> or <code>postinstall</code> script that compiles your TypeScript output. You need to build your code <em>before</em> you submit it to GCF.</p>"
"How to run angular test using Cloud Build?<p>I use out-of-the-box Angular testing framework: Jasmine and karma for Unit Test and GCP Cloud Build to deploy the app. However, there is a problem in testing because there is no Chrome installed for the node Docker image.</p>
<p>What is the best way to handle that? Create a customized docker image? Or is there any well-known docker image I can leverage for build and test Angular app in Cloud Build?</p>
<p><strong>cloudbuild.yaml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  # Install
  - name: node:14
    entrypoint: npm
    args: [&quot;install&quot;]
  #Build
  - name: node:14
    entrypoint: npm
    args: [&quot;run&quot;, &quot;build&quot;, &quot;--&quot;, &quot;--aot&quot;]

  # Test &lt;- This step fails. See the error message below
  - name: node:14
    entrypoint: npm
    args: [&quot;run&quot;, &quot;test&quot;, &quot;--&quot;, &quot;--watch=false&quot;]

  # Deploy to Firebase
  - name: gcr.io/$PROJECT_ID/firebase
    args: [&quot;deploy&quot;, &quot;--project=$PROJECT_ID&quot;, &quot;--only=hosting&quot;]
</code></pre>
<h3>Error</h3>
<pre><code>Already have image: node:14

&gt; carealth@0.0.0 test /workspace
&gt; ng test &quot;--watch=false&quot;

- Generating browser application bundles (phase: setup)...
18 01 2022 19:22:45.740:INFO [karma-server]: Karma v6.3.11 server started at http://localhost:9876/
18 01 2022 19:22:45.744:INFO [launcher]: Launching browsers Chrome with concurrency unlimited
18 01 2022 19:22:45.749:INFO [launcher]: Starting browser Chrome
18 01 2022 19:22:45.756:ERROR [launcher]: No binary for Chrome browser on your platform.
  Please, set &quot;CHROME_BIN&quot; env variable.
npm ERR! code ELIFECYCLE
npm ERR! errno 1
npm ERR! carealth@0.0.0 test: `ng test &quot;--watch=false&quot;`
npm ERR! Exit status 1
npm ERR! 
npm ERR! Failed at the carealth@0.0.0 test script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.

npm ERR! A complete log of this run can be found in:
npm ERR!     /builder/home/.npm/_logs/2022-01-18T19_22_45_880Z-debug.log
</code></pre>","<p>If you need headless chrome in your container, choose a container with node14 and headless chrome installed. I found <a href=""https://hub.docker.com/layers/zenika/alpine-chrome/89-with-node-14/images/sha256-8452fd9c7bab3b3b114492e9ad4384f3765d33a32af44e7540b66dc3795a8121?context=explore"" rel=""nofollow noreferrer"">this one</a> with Chrome 89 and released 10 months ago. You could find better source I guess</p>
<p>Else, you can use your <code>node:14</code> container and, if it's possible, install headless Google Chrome on it (something like that, install work, but I haven't node file to test on it to validate completely that example)</p>
<pre class=""lang-yaml prettyprint-override""><code>  - name: node:14
    entrypoint: bash
    args: 
      - -c
      - |
        wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
        apt update &amp;&amp; apt install -y libappindicator1 fonts-liberation ./google-chrome-stable_current_amd64.deb
        npm run test -- --watch=false
</code></pre>"
"Difference between storing Google Cloud Build logs in Cloud Logging vs storing them in GCS<p>Google Cloud Build allows us to store logs in either GCS or Cloud Logging or both or none. I just want to know what difference it makes when storing logs in GCS vs storing them in Cloud Logging. Maybe some of the things you can highlight are:</p>
<ul>
<li>Advantages and disadvantages, or typical scenarios when I should use one over the other.</li>
<li>Difference in pricing, which may cost more than the other.</li>
</ul>","<p>It's 2 different places with different features</p>
<p>Cloud Logging buckets allow you to</p>
<ul>
<li>Store data during a different retention periods</li>
<li>Query the data from Cloud Logging (and soon via SQL expression)</li>
<li>Sink the logs from different projects in a single place (I <a href=""https://medium.com/google-cloud/multiple-projects-log-monitoring-in-a-single-place-59f1156721bb"" rel=""nofollow noreferrer"">wrote an article</a> on that)</li>
<li>Use cloud monitoring/cloud alerting features</li>
</ul>
<p>Cloud Storage sink allows you to</p>
<ul>
<li>Store data during a different retention periods, with different storage class cost</li>
<li>Sink the logs from different projects in a single bucket</li>
<li>Move, copy, manage your files as you wish.</li>
<li>Ingest log file in third party tool (like splunk)</li>
</ul>
<hr />
<p>IMO, the main difference is the query capacity and the Cloud Monitoring and Alerting integration. Things that you loose when you store logs in Cloud Storage.</p>
<p>However, you gain file management with Cloud Storage.</p>"
"Use $() Operator in Google Cloud Build YAML File<p>I'm currently building pipeline with Google Cloud Build that has this step</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'dataflow'
      - 'jobs'
      - 'cancel'
      - '$(gcloud dataflow jobs list --filter=&quot;name=iot&quot; --status=active --limit=1 --format=&quot;value(JOB_ID)&quot;)'
      - '--region'
      - 'asia-southeast2'
</code></pre>
<p>The problem with this configuration is that it treats this <code>'$(gcloud dataflow jobs list --filter=&quot;name=iot&quot; --status=active --limit=1 --format=&quot;value(JOB_ID)&quot;)'</code> as string instead of executing the command inside it.</p>
<p>So, my question is, it is possible to have <code>$()</code> operator in Google Cloud Build YAML file? and how?</p>
<p>Thanks!</p>","<p>You can but you have to use the bash entry point</p>
<pre><code>steps:
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: bash
    args:
      - '-c'
      - |
          gcloud dataflow jobs cancel $(gcloud dataflow jobs list --filter=&quot;name=iot&quot; --status=active --limit=1 --format=&quot;value(JOB_ID)&quot;) --region asia-southeast2

</code></pre>"
"How to setup source with cloud build java api?<p>Looks like the <code>gcloud</code> command line does a lot of things by default when I run:</p>
<pre><code>gcloud builds submit --tag gcr.io/$PROJECT_ID/$IMAGE_ID
</code></pre>
<p>for example it uploads the repo to GCS and then when the build starts it downloads it to the working directory.</p>
<p>I want to use the java api instead of invoking <code>gcloud</code> from another process.</p>
<p>Can I do the same using the java api, without implementing it manually? I couldn't find any samples on how to use the cloud build java api.</p>
<p>Kotlin code of what I have so far:</p>
<pre><code>val buildDockerStep = BuildStep.newBuilder()
  .setName(&quot;gcr.io/cloud-builders/docker&quot;)
  .addAllArgs(listOf(&quot;build&quot;, &quot;-t&quot;, imageRepository, &quot;.&quot;))

val build: Build = Build.newBuilder()
  // .setSource() ? does this need to happen manually?
  .addSteps(buildDockerStep)
  .build()

val request = CreateBuildRequest.newBuilder()
  .setBuild(build)
  .setProjectId(PROJECT_ID)
  .build()

val result = cloudBuildClient.createBuildAsync(request)
</code></pre>","<p>Replying to your comment, you would <a href=""https://cloud.google.com/sdk/gcloud/reference/builds/submit"" rel=""nofollow noreferrer"">indeed need</a> to zip your source files and upload them to GCS if you’d like to build a <code>StorageSource</code>. While there isn’t any sample code or examples for the Java API client out there, I pieced together this code from the <a href=""https://googleapis.dev/java/google-cloud-build/latest/"" rel=""nofollow noreferrer"">API reference</a>. It builds a <code>StorageSource</code> using my bucket and the zipped source archives, and builds a new image to upload to my Artifact Registry.</p>
<pre class=""lang-java prettyprint-override""><code>StorageSource.Builder gcsSource = StorageSource.newBuilder()
    .setBucket(bucketName)
    .setObject(objectName); //Name of the zipped archive in GCS 

Source.Builder buildSource = Source.newBuilder().setStorageSource(gcsSource);

Builder buildStep = BuildStep.newBuilder()
    .setName(&quot;gcr.io/cloud-builders/docker&quot;)
    .addAllArgs(Arrays.asList(new String[] {&quot;build&quot;, &quot;-t&quot;, imageDir, &quot;.&quot;}));

Build testBuild = Build.newBuilder()
    .setSource(buildSource)
    .addSteps(buildStep)
    .addImages(imageDir) //Adds the image to Artifact Registry
    .build();

CreateBuildRequest req = CreateBuildRequest.newBuilder()
    .setBuild(testBuild)
    .setProjectId(projectId)
    .build();

Build res = CloudBuildClient.create().createBuildAsync(req).get();
</code></pre>
<p>Let me know if this worked for you, as I could not find any other samples out there of the Java client. I tested it and it worked fine on my end.</p>"
"Google Cloud Build Trigger - Problem Fetching Submodules<p>I have source code hosted in Google Cloud Source Repositories. It has a single git submodule which is also hosted in Google Cloud Source Repositories (in the same GCP project). The <code>.gitmodules</code> file looks something like this:</p>
<pre><code>[submodule &quot;src/my-repo&quot;]
    path = src/my-repo
    url = ssh://source.developers.google.com:2022/p/my-project/r/my-repo
</code></pre>
<p>I have a Google Cloud Build trigger configured, but the build is failing because the git submodule is not present (it seems that it's an ongoing shortcoming that a clone of a Cloud Source Repositories repo doesn't init and update git submodules).</p>
<p>I added a step to the <code>cloudbuild.yaml</code> file to init and update submodules, but I get a <code>Host key verification failed</code> error. I did something like this,</p>
<p><code>cloudbuild.yaml</code></p>
<pre><code>steps:
  - name: 'gcr.io/cloud-builders/git'
    args: ['submodule', 'update', '--init']
  ...
</code></pre>
<p>I can confirm that the default service account is being used for the trigger. And that service account does have permissions for the submodule's repo.</p>
<p>I would welcome any suggestions on how to debug this problem.</p>","<p>I solved this by rewriting the <code>url</code> of the submodule so that it uses HTTPS instead of SSH, based on the answer from <a href=""https://stackoverflow.com/questions/59180893/google-cloud-build-cant-update-submodules"">this question</a>. Thanks, Mousumi Roy.</p>
<p>Specifically, my <code>.gitmodules</code> file was unchanged</p>
<pre><code>[submodule &quot;src/my-repo&quot;]
    path = src/my-repo
    url = ssh://source.developers.google.com:2022/p/my-project/r/my-repo
</code></pre>
<p>but the first step in my <code>cloudbuild.yaml</code> was changed to</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: 'gcr.io/cloud-builders/git'
    entrypoint: 'bash'
    args:
    - -c
    - |
      git config -f .gitmodules submodule.src/my-repo.url https://source.developers.google.com/p/my-project/r/my-repo
      git submodule update --init
</code></pre>
<p>Somewhat magically, the submodule could then be checked out. I think that it's the <a href=""https://cloud.google.com/build/docs/cloud-build-service-account"" rel=""nofollow noreferrer"">Cloud Build service account</a> that's being used to authenticate against the repo by default, but I don't know how that authentication is being performed in the container when the build is running.</p>"
"Programmatically Connecting a GitHub repo to a Google Cloud Project<p>I'm working on a Terraform project that will set up all the GCP resources needed for a large project spanning multiple GitHub repos. My goal is to be able to recreate the cloud infrastructure from scratch completely with Terraform.</p>
<p>The issue I'm running into is in order to setup build triggers with Terraform within GCP, the GitHub repo that is setting off the trigger first needs to be connected. Currently, I've only been able to do that manually via the Google Cloud Build dashboard. I'm not sure if this is possible via Terraform or with a script but I'm looking for any solution I can automate this with. Once the projects are connected updating everything with Terraform is working fine.</p>
<p>TLDR; How can I programmatically connect a GitHub project with a GCP project instead of using the dashboard?</p>","<p>Currently there is no way to programmatically connect a GitHub repo to a Google Cloud Project. This must be done manually via Google Cloud.</p>
<p>My workaround is to manually connect an &quot;admin&quot; project, build containers and save them to that project's artifact registry, and then deploy the containers from the registry in the programmatically generated project.</p>"
"Cloudbuild can't access Artifacts Registery when building cloud run docker container<p>I'm using a package from Artifacts Registery in my cloud run nodejs container.
When I try to gcloud builds submit I get the following error:</p>
<pre><code>Step #1: npm ERR! 403 403 Forbidden - GET https://us-east4-npm.pkg.dev/....
Step #1: npm ERR! 403 In most cases, you or one of your dependencies are requesting
Step #1: npm ERR! 403 a package version that is forbidden by your security policy.
</code></pre>
<p>Here is my cloudbuild.yaml:</p>
<pre><code>steps:
 - name: gcr.io/cloud-builders/npm
   args: ['run', 'artifactregistry-login']

 - name: 'gcr.io/cloud-builders/docker'
   args: ['build', '-t', 'gcr.io/...', '.']
 
 - name: 'gcr.io/cloud-builders/docker'
   args: ['push', 'gcr.io/...']
 - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
   entrypoint: gcloud
   args:
   - 'run'
   - 'deploy'
   - 'admin-api'
   - '--image'
   - 'gcr.io/...'
   - '--region'
   - 'us-east4'
   - '--allow-unauthenticated'
images:
 - 'gcr.io/....'
</code></pre>
<p>and Dockerfile</p>
<pre><code>FROM node:14-slim

WORKDIR /usr/src/app

COPY --chown=node:node .npmrc ./

COPY package*.json ./


RUN npm install

COPY . ./

EXPOSE 8080

CMD [ &quot;npm&quot;,&quot;run&quot; ,&quot;server&quot; ]
</code></pre>
<p>.npmrc file:</p>
<pre><code>@scope_xxx:registry=https://us-east4-npm.pkg.dev/project_xxx/repo_xxx/
//us-east4-npm.pkg.dev/project_xxx/repo_xxx/:always-auth=true
</code></pre>
<p>the google build service account already has the permission &quot;Artifact Registry Reader&quot;</p>","<p>The solution that worked with me can be found in this blog post:</p>
<p><a href=""https://dev.to/brianburton/cloud-build-docker-and-artifact-registry-cicd-pipelines-with-private-packages-5ci2"" rel=""nofollow noreferrer"">https://dev.to/brianburton/cloud-build-docker-and-artifact-registry-cicd-pipelines-with-private-packages-5ci2</a></p>"
"Can I push an image built in Cloud Build to an image registry that is not Cloud Storage?<p>Using Cloud Build, can I push the resulting image to a different image registry rather than gcr.io?</p>","<p>If your build produces artifacts such as binaries or tarballs, you can choose to store them in Cloud Storage or any private third-party repository.</p>
<p>You can check <a href=""https://cloud.google.com/build/docs/interacting-with-dockerhub-images#pushing_images_to_docker_hub"" rel=""nofollow noreferrer"">this example</a> to push images to Docker Hub. Note in the example build config file that <em><strong>REPOSITORY</strong></em> is the name of your Docker repository to which you're pushing the image. You should try similar steps in order to push the resulting image to a private image registry.</p>
<p>Doing this could also imply building repositories as in the <a href=""https://cloud.google.com/build/docs/automating-builds/build-repos-from-github"" rel=""nofollow noreferrer"">repositories from GitHub</a> and from <a href=""https://cloud.google.com/build/docs/automating-builds/build-repos-from-github-enterprise"" rel=""nofollow noreferrer"">repositories from GitHub Enterprise</a> .</p>"
"Unable to deploy Google Cloud Function due to mysterious --production=false flag<p>For some reason, I can no longer deploy existing google functions from my local machine or from github actions. Whenever I deploy using the <code>gcloud functions deploy</code> command, I get the following error in the console: <code>ERROR: (gcloud.functions.deploy) OperationError: code=3, message=Build failed: Unknown Syntax Error: Invalid option name (&quot;--production=false&quot;).</code> I am not using a --production=false option in my gcloud deploy command, so I don't really understand where that is coming from.</p>
<p>Build logs always failing on:<br>
<code>Step #1 - &quot;build&quot;: Unable to delete previous cache image: DELETE https://us.gcr.io/v2/{{projectId}}/gcf/{{region}}/{{guid}}/cache/manifests/sha256:{{imageId}}: GOOGLE_MANIFEST_DANGLING_TAG: Manifest is still referenced by tag: latest</code>.</p>
<p>Deploy command:<br></p>
<pre><code>gcloud functions deploy --runtime=nodejs16 --region=us-central1 {{function_name}} --entry-point={{node_function}} --trigger-topic={{topic_name}}
</code></pre>
<p>Attempted with the following gcloud versions and got the same result each time:<br>
370, 371, 369, 360</p>
<p>I am not sure where this is coming from. I did not have this problem when I deployed just yesterday and it is not specific to my local machine.</p>","<p>This was due to a regression issue on Google's part. They released a fix for it today and deploys are working again now.</p>
<p>Issue: <a href=""https://github.com/GoogleCloudPlatform/buildpacks/issues/175#issuecomment-1030519240"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/buildpacks/issues/175#issuecomment-1030519240</a></p>"
"Getting a build type error on cloud build that doesn't happen in docker build<h1>My Problem</h1>
<p>I am getting an error ONLY when doing gcloud builds submit. My build is working locally and when creating a docker image locally. I am assuming that this is a problem where a file is not getting copied into the tarball correctly; however, in my debugging I found that it appears it is being copied.</p>
<h2>My error:</h2>
<p><code>Type error: Parameter 'service' implicitly has an 'any' type</code></p>
<p>Which is from some code that looks like this:</p>
<pre><code>const {
    firestore,
    collections: {
      rentals: { services },
    },
  } = useFirebaseCtx();
const newServiceEvents = services.map((service, index) =&gt; {
   ...
})
</code></pre>
<p>Where useFirebaseCtx is extensively typed (with no explicit any inside it)</p>
<p>services has an explicit type of <code>Organizations.Rentals.Services.Flat[]</code></p>
<h2>Debugging I have tried</h2>
<ol>
<li><p>When I run <code>yarn run eslint . --ext .js,.jsx,.ts,.tsx</code> I do not get any errors</p>
</li>
<li><p>When I run <code>docker build -t my-docker-image-name .</code> I do not get any errors (and the image builds to completion)</p>
</li>
<li><p>I have confirmed (by running <code>gcloud meta list-files-for-upload</code>) that my type folder is indeed being uploaded in the tarball</p>
</li>
</ol>
<p>Abbreviated output of <code>gcloud meta list-files-for-upload</code>:</p>
<pre><code>...
types/organizations/index.d.ts &lt;- this is the file the type is defined in
...
</code></pre>
<h2>Supplementary Files</h2>
<h3>.dockerignore</h3>
<pre><code>Dockerfile
.dockerignore
.gitignore
node_modules
npm-debug.log
README.md
.next
functions
fire*
jest*
__tests__
</code></pre>
<h3>.gcloudignore</h3>
<pre><code>.gcloudignore
.git
.gitignore
node_modules
npm-debug.log
README.md
.next
functions
fire*
jest*
__tests__
!.env*
.env.development*
</code></pre>
<h3>Dockerfile</h3>
<pre><code># Install dependencies only when needed
FROM node:14-alpine AS deps
# Check https://github.com/nodejs/docker-node/tree/b4117f9333da4138b03a546ec926ef50a31506c3#nodealpine to understand why libc6-compat might be needed.
RUN apk add --no-cache libc6-compat
WORKDIR /app
COPY package.json yarn.lock ./
# COPY ./private/sitemap/generate-sitemap.mjs ./private/sitemap/generate-sitemap.mjs
RUN yarn install --frozen-lockfile

# Rebuild the source code only when needed
FROM node:14-alpine AS builder
WORKDIR /app
COPY . .
COPY --from=deps /app/node_modules ./node_modules

ENV NODE_ENV production

# Failing here
RUN yarn build 

# Production image, copy all the files and run next
FROM node:14-alpine AS runner
WORKDIR /app


RUN addgroup -g 1001 -S nodejs
RUN adduser -S nextjs -u 1001

# You only need to copy next.config.js if you are NOT using the default configuration
COPY --from=builder /app/next.config.js ./
COPY --from=builder /app/public ./public
COPY --from=builder /app/.env* ./
COPY --from=builder --chown=nextjs:nodejs /app/.next ./.next
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/package.json ./package.json
COPY --from=builder /app/next.config.js ./next.config.js

USER nextjs

EXPOSE 3000

# Next.js collects completely anonymous telemetry data about general usage.
# Learn more here: https://nextjs.org/telemetry
# Uncomment the following line in case you want to disable telemetry.
# ENV NEXT_TELEMETRY_DISABLED 1

CMD [&quot;yarn&quot;, &quot;start&quot;]
</code></pre>","<h1>TL;DR</h1>
<p>Don't name ANY folder <code>firebase*</code> inside your src code.</p>
<h2>What was wrong</h2>
<p>The problem was I had an internal folder named <code>firebase</code> that was not being included in the tarball. I had to rename the folder do that it would be included.</p>
<h2>How I found what was wrong and my solution</h2>
<ol>
<li>I went into the cloud console and downloaded the tarball (they give you a line in the CLI)</li>
<li>CD'ed into the folder and ran <code>yarn install</code></li>
<li>I traced the error up the tree and found that I was missing an entire folder from my src folder</li>
<li>I ran <code>gcloud meta list-files-for-upload</code> in my original folder and found the the files (firebase/*.(ts|tsx)) were not being included. I thus concluded that naming a folder &quot;firebase&quot; was a bad idea.</li>
<li>Renamed the folder.</li>
</ol>
<h2>Another possible solution</h2>
<p>Add the file as a don't ignore to your <code>.gcloudignore</code></p>"
"CloudBuild Composer No change in configuration. Must specify a change to configuration.software_configuration.pypi_dependencies<ol>
<li>I am using CloudBuild to update the Composer environment if there is any update to requirements.txt. My cloudbuild.yaml looks like</li>
</ol>
<pre><code>- name: 'gcr.io/cloud-builders/gcloud'
  args: [&quot;composer&quot;, &quot;environments&quot;, &quot;update&quot;,'$_COMPOSER_NAME', &quot;--location&quot; , &quot;$_COMPOSER_REGION&quot;,&quot;--update-pypi-packages-from-file&quot;, &quot;$_DAG_FOLDER/requirements.txt&quot;]
  id: 'update-composer-env'
  timeout: 3600s
</code></pre>
<p>However, if there is no change in the requirements.txt, the cloudbuild fails with the following error</p>
<pre><code>ERROR: (gcloud.composer.environments.update) INVALID_ARGUMENT: No change in configuration. Must specify a change to configuration.software_configuration.pypi_dependencies
</code></pre>
<p>How can I make the build not fail?</p>
<ol start=""2"">
<li>Even if the build is successful, I do not see anything in the cloudbuild logs, how can I fix that?</li>
</ol>","<p>This works</p>
<pre><code>- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      custom_comm=$(gcloud composer environments update $_COMPOSER_NAME --update-pypi-packages-from-file $_DAG_FOLDER/requirements.txt --location $_COMPOSER_REGION --verbosity=debug 2&gt;&amp;1); \
      echo &quot;$custom_comm&quot;; \
      if [[ &quot;$custom_comm&quot; == *&quot;ERROR: (gcloud.composer.environments.update) INVALID_ARGUMENT: No change in configuration. Must specify a change to configuration.software_configuration.pypi_dependencies&quot;* ]]; then \
        echo &quot;This failure is expected if there is no change to requirements.txt&quot;; exit 0; \ 
      else exit 1; \
      fi
  id: 'update-composer-env'
  timeout: 3600s
</code></pre>"
"Terraform, weird patterns in the log [32m+ [0m<p>I am following <a href=""https://github.com/GoogleCloudPlatform/solutions-terraform-cloudbuild-gitops"" rel=""nofollow noreferrer"">this tutorial</a> from the official GCP repo on how to deploy terraform from a cloud build:</p>
<p><strong>cloudbuils.yaml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- id: 'branch name'
  name: 'alpine'
  entrypoint: 'sh'  
  args: 
  - '-c'
  - | 
      echo &quot;***********************&quot;
      echo &quot;$BRANCH_NAME&quot;
      echo &quot;***********************&quot;
- id: 'tf init'
  name: 'hashicorp/terraform:1.0.0'
  entrypoint: 'sh'
  args: 
  - '-c'
  - |
      if [ -d &quot;environments/$BRANCH_NAME/&quot; ]; then
        cd environments/$BRANCH_NAME
        terraform init
      else
        for dir in environments/*/
        do 
          cd ${dir}   
          env=${dir%*/}
          env=${env#*/}
          echo &quot;&quot;
          echo &quot;*************** TERRAFORM INIT ******************&quot;
          echo &quot;******* At environment: ${env} ********&quot;
          echo &quot;*************************************************&quot;
          terraform init || exit 1
          cd ../../
        done
      fi 
</code></pre>
<p>and I am having the following pattern in the log:</p>
<pre class=""lang-sh prettyprint-override""><code>2022-02-21 07:13:13.094 CETStep #3 - &quot;tf apply&quot;: [32m+[0m [0m[1m[0munique_id[0m[0m = (known after apply)
2022-02-21 07:13:13.094 CETStep #3 - &quot;tf apply&quot;: }
2022-02-21 07:13:13.094 CETStep #3 - &quot;tf apply&quot;:
2022-02-21 07:13:13.094 CETStep #3 - &quot;tf apply&quot;: [0m[1mPlan:[0m 7 to add, 0 to change, 0 to destroy.
</code></pre>
<p>Which makes the debug a lot more painful that it needs to be, I am sure it is a simple formatting issue, but I could not find its origin.</p>","<p>There is a similar issue occur in the <a href=""https://github.com/jenkinsci/ansicolor-plugin/issues/145#issuecomment-916164416"" rel=""nofollow noreferrer"">link1</a>, <a href=""https://github.com/hashicorp/terraform/issues/3278#issuecomment-141497096"" rel=""nofollow noreferrer"">link2</a> and <a href=""https://github.com/hashicorp/terraform/issues/12305#issuecomment-283282913"" rel=""nofollow noreferrer"">link3</a> where it has been mentioned that :</p>
<blockquote>
<p>Apparently Terraform attempts to format the output using color. However, Jenkins does not output this formatting correctly and thus you see those unusual characters in the output.
was able to make Terraform omit those characters in the output by using the -no-color flag.
This option is available for at least three of the major Terraform functions: show, plan, and apply.
It resolved by using <code>terraform show -no-color</code></p>
</blockquote>
<p>For more information on how to manage infrastructure as code with terraform, cloud build and gitops, you can refer to this <a href=""https://cloud.google.com/architecture/managing-infrastructure-as-code"" rel=""nofollow noreferrer"">documentation</a>.</p>"
"Why Google Cloud Build is passing data between steps without any configuration in cloudbuild.yaml file<p>Refer to Google Cloud documentation <a href=""https://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps"" rel=""nofollow noreferrer"">Passing data between build steps</a></p>
<p>It says -</p>
<blockquote>
<p>Cloud Build runs your tasks as a series of build steps, which execute
in isolated and containerized environments. <strong>After each step, the
container is discarded.</strong> This allows you to have totally different
tools and environments for each step, and by default, any data created
in one step can't contaminate the next step. But sometimes you may
need to persist state from one step of a build to use in subsequent
steps.</p>
</blockquote>
<p>Now refer to the below cloudbuild.yaml code from <a href=""https://cloud.google.com/build/docs/building/build-nodejs#configuring_nodejs_builds"" rel=""nofollow noreferrer"">Google Cloud Documentation example.</a></p>
<pre> <code>
steps:
# Step 1
- name: node
  entrypoint: npm
  args: ['install']

# Step 2
- name: node
  entrypoint: npm
  args: ['test']
</pre> </code>
<p>Question: Why and how the above example runs successfully even if we don’t have a step to
install the requirements again in step 2. Because as per my thinking, the step 2 should get fail (but it is not actually) as the installations done in step 1 <strong>should get discarded</strong> as per the documentation.
Thanks!</p>","<p>The reason both steps succeed is because the command <code>npm install</code> will download/install the dependencies of your application in a persistent volume in CloudBuild runners which is shared between all of your steps and then the <code>npm test</code> will run against your app code which is sitting under the same shared volume. This volume name is <code>/workspace</code>.</p>
<p>Take a closer look at this part of the documentation which will give you more details on how the data is shared between steps: <a href=""https://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps#passing_data_using_workspaces"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/configuring-builds/pass-data-between-steps#passing_data_using_workspaces</a></p>"
"gcloud builds submit substitutions not working properly. (ERROR: (gcloud.builds.submit) INVALID_ARGUMENT.)<p>I have the following config:</p>
<pre><code>steps:
- name: 'alpine'
  args: ['echo', 'B: ${_BRANCH}', 'T: ${_TAG}', 'C =&gt; ${_CLIENT}']
</code></pre>
<p>If I run with:</p>
<pre><code>gcloud builds submit --config=gcp/cloudbuild-main.yaml --substitutions _CLIENT='client',_BRANCH='branch',_TAG='tag' .
</code></pre>
<p>I get the following message:</p>
<pre><code>ERROR: (gcloud.builds.submit) INVALID_ARGUMENT: generic::invalid_argument: key in the template &quot;_BRANCH&quot; is not matched in the substitution data; substitutions = map[_CLIENT:client _BRANCH=branch _TAG=tag];key in the template &quot;_TAG&quot; is not matched in the substitution data; substitutions = map[_CLIENT:client _BRANCH=branch _TAG=tag];key &quot;_CLIENT&quot; in the substitution data is not matched in the template
</code></pre>
<p>If I declare the substitutions:</p>
<pre><code>steps:
- name: 'alpine'
  args: ['echo', 'B: ${_BRANCH}', 'T: ${_TAG}', 'C =&gt; ${_CLIENT}']
substitutions:
  _BRANCH: b1
  _TAG: latest
  _CLIENT: c
</code></pre>
<p>It runs but the substitutions take only the first variable and other become values of it:</p>
<pre><code>BUILD
Pulling image: alpine
Using default tag: latest
latest: Pulling from library/alpine
Digest: sha256:21a3deaa0d32a8057914f36584b5288d2e5ecc984380bc0118285c70fa8c9300
Status: Downloaded newer image for alpine:latest
docker.io/library/alpine:latest
B: b1 T: latest C =&gt; client _BRANCH=branch _TAG=tag
PUSH
DONE
</code></pre>","<p>There's a syntax nit in your command which should be resolved by:</p>
<pre><code>gcloud builds submit --config=gcp/cloudbuild-main.yaml --substitutions=_CLIENT=&quot;client&quot;,_BRANCH=&quot;branch&quot;,_TAG=&quot;tag&quot; .
</code></pre>
<p>After submitting the build:</p>
<blockquote>
<p>B: branch T: tag C =&gt; client</p>
</blockquote>
<p>Reference: <a href=""https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values</a></p>"
"gcp cloud build - cannot use SHORT_SHA substitution<p>Execute <code>gcloud builds submit</code> from the local directory in my PC where the cloudbuild.yaml is located.</p>
<p><strong>cloudbuild.yaml</strong></p>
<pre><code>steps:
  # Docker Build
  - name: 'gcr.io/cloud-builders/docker'
    args:
    - 'build'
    - '-t'
    - 'us-central1-docker.pkg.dev/$PROJECT_ID/diabetes-prediction/diabetes-prediction-api:$SHORT_SHA'
    - '-f'
    - 'src/serving/Dockerfile'
    - '.'
</code></pre>
<p>The build fails because <code>SHORT_SHA</code> is empty causing invalid tag name ending with <code>diabetes-prediction-api:</code>.</p>
<pre><code>Starting Step #0
Step #0: Already have image (with digest): gcr.io/cloud-builders/docker
Step #0: invalid argument &quot;us-central1-docker.pkg.dev/default-338305/diabetes-prediction/diabetes-prediction-api:&quot; for &quot;-t, --tag&quot; flag: invalid reference format
</code></pre>
<p>Please advise what is wrong.</p>","<p>You need to invoke the build as a trigger (from a repo) for the repo-specific values to be set.</p>
<p>See <a href=""https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values#using_default_substitutions"" rel=""nofollow noreferrer"">Using default substitutions</a></p>"
"Dockerfile COPY command is missing a single file when using `gcloud build`<p>I have run into an incredibly frustrating problem where a COPY command in my Dockerfile successfully copies all of my apps files except one. I do not have a .dockerignore file so I know the file isn't being excluded from the build that way.</p>
<p>Note: I do have a <code>.gitignore</code> which is excluding <code>file2.json</code> that I do not want to version. But as you will see below, I'm building from my local folder, not remotely from a clone/checkout so I don't see why <code>.gitignore</code> would influence the docker build in this case.</p>
<p>Below is what my directory looks like:</p>
<pre class=""lang-none prettyprint-override""><code>$ tree -a -I .git app
app
├── app
│   ├── data
│   │   ├── file1.txt
│   │   ├── file2.json
│   │   ├── file3.txt
│   │   └── file4.yml
│   ├── somefile2.py
│   └── somefile.py
├── Dockerfile
├── .gitignore
├── requirements.txt
└── setup.py
</code></pre>
<p>And this is what is in my Dockerfile looks like</p>
<pre class=""lang-none prettyprint-override""><code>FROM ubuntu:18.04
FROM python:3.7
  
COPY . /app
  
RUN cp app/app/data/file2.json ~/.somenewhiddendirectory
   
RUN pip install app/.
   
ENTRYPOINT [&quot;python&quot;, &quot;app/app/somefile.py&quot;]
</code></pre>
<p>For some reason, <code>file2.json</code> is not being copied during the <code>COPY . /app</code> call and I am getting an error when I try to <code>cp</code> it somewhere else. I have done a call like <code>RUN ls app/app/data/</code> and all the files except <code>file2.json</code> are in there. I checked the files permissions and made sure they are the same as all the other files. I have tried doing a direct <code>COPY</code> of that file which results in an error since Docker says that file doesn't exist.</p>
<p>On my system, that file exists, I can see it with <code>ls</code>, and I can <code>cat</code> its contents. I have played around with ensuring the context within the image is squarely within the root directory of my app, and like I said, all files are correctly copied except that json file. I can't for the life of my figure out why Docker hates this file.</p>
<p>For some added context, I am using Google's cloud build to build the image and the yaml config looks like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: gcr.io/cloud-builders/docker
    id: base-image-build
    waitFor: [-]
    args:
      - build
      - .
      - -t
      - us.gcr.io/${PROJECT_ID}/base/${BRANCH_NAME}:${SHORT_SHA}
        
images:
  - us.gcr.io/${PROJECT_ID}/base/${BRANCH_NAME}:${SHORT_SHA}
</code></pre>
<p>and the command I am executing looks like this:</p>
<pre><code>gcloud builds submit --config=cloudbuild.yaml . \
  --substitutions=SHORT_SHA=&quot;$(git rev-parse --short HEAD)&quot;,BRANCH_NAME=&quot;$(git rev-parse --abbrev-ref HEAD)&quot;
</code></pre>","<p><em>Disclaimer: I have never used Google's cloud build so my answer is only based on read theory.</em></p>
<hr />
<blockquote>
<p>I don't see why <code>.gitignore</code> would influence the docker build in this case</p>
</blockquote>
<p>Indeed, <code>docker build</code> in itself does not care about your <code>.gitignore</code> file. But you are building through Google's cloud build and this is a totally different story.</p>
<p>Quoting the <a href=""https://cloud.google.com/sdk/gcloud/reference/builds/submit"" rel=""nofollow noreferrer"">documentation</a> for the source specification in <code>gcloud build</code> command:</p>
<blockquote>
<p>[SOURCE]<br/>
The location of the source to build. The location can be a directory on a local disk or a gzipped archive file (.tar.gz) in Google Cloud Storage. If the source is a local directory, this command skips the files specified in the <code>--ignore-file</code>. If <code>--ignore-file</code> is not specified, use <code>.gcloudignore</code> file. If a <code>.gcloudignore</code> file is absent and a <code>.gitignore</code> file is present in the local source directory, gcloud will use a generated Git-compatible <code>.gcloudignore</code> file that respects your <code>.gitignore</code> files. The global <code>.gitignore</code> is not respected. For more information on <code>.gcloudignore</code>, see <code>gcloud topic gcloudignore</code></p>
</blockquote>
<p>So in your given case, your file will be ignored even for a build from your local directory. At this point I see 2 options to workaround this problem:</p>
<ol>
<li>Remove the entry for your file in <code>.gitignore</code> so that the default gcloud mechanism does not ignore it during your build</li>
<li>Provide a <code>--ignore-file</code> or a default <code>.gcloudignore</code> which actually re-includes the local file that is ignored for versioning.</li>
</ol>
<p>I would personally go for the second option with something super simple like the following <code>.gcloudignore</code> file (crafted from the <a href=""https://cloud.google.com/sdk/gcloud/reference/topic/gcloudignore"" rel=""nofollow noreferrer"">relevant documentation</a>)</p>
<pre class=""lang-none prettyprint-override""><code>.git
.gcloudignore
.gitignore
</code></pre>"
"Make CloudBuild git push last commits to a branch in another repo<p>I have branch_a (development branch) in repo_A and branch_b in repo_B (production branch). Both repos are on Cloud Source.
Their history is the same, except for the last commit(s) in branch_a.</p>
<p><strong>How to make CloudBuild push last commit(s) from repo_A/branch_a to repo_B/branch_b?</strong></p>
<p>CloudBuild gets triggered with a push to repo_A/branch_a.  CloudBuild fetches repo_A/branch_a into the working directory</p>
<p>I managed to add repo_B/branch_b as remote. But when I try to push 'master' to repo_B/branch_b I get:</p>
<pre><code>src refspec branch_b does not match any
failed to push some refs to repo_B
</code></pre>
<p><code>git remote -v</code> lists both remotes: repo_A and repo_B</p>
<p>CloudBuild service account has access to both repos</p>","<p><em>For the future generations</em></p>
<p>The problem was that Cloud Build was pulling only the last commit from repo_A, thus comparing to repo_B/branch_B was showing inconsistent history.</p>
<p><code>git pull --unshallow</code> did the trick.</p>"
"Access GCP Secret during Cloud Build build step<p>Lets say I have a cloudbuild.yaml file that looks like this:</p>
<pre><code>steps:
  - name: 'gcr.io/cloud-builders/docker'
    id: build
    args: ['build', '-t', 'us.gcr.io/${PROJECT_ID}/image_name', '--build-arg', 'secret=$$SECRET', '.']
    secretEnv: ['SECRET']
 
images:
  - 'us.gcr.io/${PROJECT_ID}/image_name'

availableSecrets:
  secretManager:
  - versionName: projects/project/secrets/my_secret/versions/latest
    env: 'SECRET'
</code></pre>
<p>Right now, the --build-arg is assigning to the Docker <code>secret</code> arg the value <code>$SECRET</code> instead of the value actually stored in the secret. How can I access the secret value during this step? All of the examples I can find online say to add a bash entrypoint however only for steps that aren't actually doing the build call.</p>","<p>It's a usual issue with Cloud Build and Secret Manager integration. You can access to the secret only in a script, not in entry-point and arguments (your case)</p>
<p>Try that</p>
<pre><code>steps:
  - name: 'gcr.io/cloud-builders/docker'
    id: build
    entrypoint: 'bash'
    args: 
      - -c
      - |
          docker build -t us.gcr.io/${PROJECT_ID}/image_name --build-arg secret=$$SECRET .
    secretEnv: ['SECRET']
</code></pre>"
"How to inlcude Container Registry vulnerability scans in the CI/CD script using Cloud Build on GCP<p>Is there a way to add the Container Registry vulnerability scans as a step in the .yaml script doing the CI/CD pipeline using Cloud Build. The idea will be to not deploy a image if the severity is critical or hight.</p>

<p><a href=""https://i.stack.imgur.com/egrJ1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/egrJ1.png"" alt=""using""></a></p>

<p>Right now the scan is done on Container Registry after an image is pushed but this is independant of the CI/CD pipeline script. I don't know what is the best practices in this area in particular in compagny that have heavy security rules.</p>","<p>Now <code>GCP</code> is providing a &quot;on demand scanning&quot; functionality that can be used from a <code>Cloud Build</code> pipeline <a href=""https://cloud.google.com/container-analysis/docs/on-demand-scanning"" rel=""nofollow noreferrer"">link</a></p>
<p>Right now the tool require 2-3 time the size of the container in RAM (Max with Cloud Build is 32 GB). I hope this will be improve soon. In such case we can use the --remote option, to scan a container already store in Artifac Registry</p>"
"Cloud Build: using with a monorepo<p>I have a <a href=""http://github.com/akauppi/GroundLevel-firebase-es"" rel=""nofollow noreferrer"">repo</a> with three rather independent subpackages:</p>
<pre><code>- packages
  - backend
  - app
  - app-deploy-ops
</code></pre>
<p>Cloud Build samples show only a single <code>cloudbuild.yaml</code>.</p>
<p>Would the strategy to make a repo like this CI/CD-able (run tests on PRs, for example, but relevant tests based on what changed), involve making three build definitions? That's the only approach I can think of. Then pick those in the Cloud Build triggers, as appropriate.</p>
<p>My question likely is: am I missing something?</p>","<p>You can have only one trigger with all your code in Cloud Build and customize only one big <code>cloudbuild.yaml</code> file to build only the relevant part.</p>
<p>The other option is to create a trigger per part of your project. the <code>cloudbuild.yaml</code> file is in each subdir of your code (you can precise that in the trigger) and thus the file is smaller and dedicated to the part to build.</p>
<p>I recommend you the 2nd approach, but yes, you need 3 files and to define 3 triggers.</p>"
"Is it possible to push an image to AWS ECR using Google CloudBuild?<p>The following snippet (Node/Typescript) utilizes Google's CloudBuild API (v1) to build a container and push to Google's Container Registry (GCR). If it is possible, what's the right way to have CloudBuild push the image to AWS ECR instead of GCR?</p>
<pre class=""lang-js prettyprint-override""><code>import { cloudbuild_v1 } from &quot;googleapis&quot;;

[...]

const manifestLocation = `gs://${manifestFile.bucket}/${manifestFile.fullpath}`;
const buildDestination = `gcr.io/${GOOGLE_PROJECT_ID}/xxx:yyy`;

const result = await builds.create({
    projectId: GOOGLE_PROJECT_ID,
    requestBody: {
        steps: [
            {
                name: 'gcr.io/cloud-builders/gcs-fetcher',
                args: [
                    '--type=Manifest',
                    `--location=${manifestLocation}`
                ]
            },
            {
                name: 'docker',
                args: ['build', '-t', buildDestination, '.'],
            }
        ],
        images: [buildDestination]
    }
})```
</code></pre>","<p>Yes you can by setting a custom step where you do that.</p>
<p>for this you can have a step with the docker image that makes the build and pushes it to AWS ECR.</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  args: [ 'build', '-t', '&lt;AWS_ACCOUNT_ID&gt;.dkr.ecr.&lt;REGION&gt;.amazonaws.com/&lt;IMAGE_NAME&gt;', '.' ]
</code></pre>
<p><a href=""https://cloud.google.com/build/docs/building/build-containers#use-buildconfig"" rel=""nofollow noreferrer"">Here</a> is a guide on how to use cludbuild which can be useful to you.</p>
<p>BAsically on your usecase you can just change the value of destination to the AWS ECR URL like this:</p>
<pre><code>import { cloudbuild_v1 } from &quot;googleapis&quot;;

[...]

const manifestLocation = `gs://${manifestFile.bucket}/${manifestFile.fullpath}`;
const buildDestination = `&lt;AWS_ACCOUNT_ID&gt;.dkr.ecr.&lt;REGION&gt;.amazonaws.com/&lt;IMAGE_NAME&gt;`;

const result = await builds.create({
    projectId: GOOGLE_PROJECT_ID,
    requestBody: {
        steps: [
            {
                name: 'gcr.io/cloud-builders/gcs-fetcher',
                args: [
                    '--type=Manifest',
                    `--location=${manifestLocation}`
                ]
            },
            {
                name: 'docker',
                args: ['build', '-t', buildDestination, '.'],
            }
        ],
        images: [buildDestination]
    }
})```


</code></pre>"
"Including a subfolder from an excluded folder in .gcloudignore<p>I'm trying to deploy a Node project with a Dockerfile to Google Cloud Run with the <a href=""https://cloud.google.com/sdk/gcloud/reference/beta/run/deploy"" rel=""nofollow noreferrer""><code>gcloud beta run deploy</code></a> command.</p>
<p>Long story short, I would like to copy my local <code>node_modules/my-module</code> after running <code>RUN npm install</code> in the Dockerfile:</p>
<pre><code>COPY node_modules/my-module /app/node_modules/my-module/
</code></pre>
<p>(I only do this while in development, to avoid committing and pushing every change from <code>my-module</code> for testing).</p>
<p>Unfortunately, Docker cannot copy this directory since, apparently, <code>node_modules</code> is not uploaded to Cloud Build by default.</p>
<p>So I created this <a href=""https://cloud.google.com/sdk/gcloud/reference/topic/gcloudignore"" rel=""nofollow noreferrer""><code>.gcloudignore</code></a> file to override the default:</p>
<pre><code>.gcloudignore
.git
.gitignore

node_modules/
!node_modules/my-module/
</code></pre>
<p>I've tried a lot of other syntaxes but none allowed me to exclude <code>node_modules</code> while including <code>node_modules/my-module</code>.</p>
<p>However, I can include the whole <code>node_modules</code> directory by omitting it from the <code>.gcloudignore</code> file, but this obviously takes forever to upload.</p>
<p>Do you know how I could upload my local module to Cloud Build?</p>","<p>After reading <a href=""https://gist.github.com/cbandy/b58d32fe44dcbaa91f35e821efb72066#gistcomment-2870193"" rel=""nofollow noreferrer"">this Gist's comments</a>, I realized you have to also include the parent directories, like so:</p>
<pre><code>node_modules/**

!node_modules/
!node_modules/my-module/**
</code></pre>
<p>This will exclude all subfolders from <code>node_modules</code> except <code>my-module</code> and its content (the <code>**</code> is important, otherwise only the empty folder would be included).</p>"
"Application size in appengine suddenly much bigger (go 1.14)<p>We are using google appengine with go114 runtime. Our application size was usually around the 40mb. Suddenly from one moment to the other the size of our application versions went to 240mb, without any change on our side.</p>
<p>The strange thing is that the same code version resulted in one environment still to 40mb and in the other to 240mb. Later version also increased in the first environment.</p>
<p>There are not changes on our code whatsoever which could have impacted any of this, but I have also no clue what could have caused this on the appengine side of things.</p>
<p>Anyone knows what could have caused this rise of application size?</p>","<p>I opened ticket to Google and I got the following answer:</p>
<blockquote>
<p>According to our App Engine product team, the previous version size
was computed as the sum of the sizes of user code files, error blobs
and static files.</p>
<p>When the infrastructure change was made, due to other dependencies,
version size started being computed using the sizes of the user layers
of the container (considering values such as raw_bytes, tar_bytes,
file_bytes, etc) rather than just the size of the code itself. Since
user layers may contain far more than just the code files, the version
size appears significantly larger.</p>
</blockquote>"
"Google Cloud Build mysteriously wrong git hashes<p>I've been using Google Cloud Build with GitHub triggers without issue for a while now; all of a sudden today, something changed. Somehow the hashes that are showing up in Cloud Build do not correspond to any commits in any of my repositories...</p>
<p><a href=""https://i.stack.imgur.com/rK9zT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rK9zT.png"" alt=""google cloud build wrong git hashes"" /></a></p>
<p>I've tried searching for documentation, or other folks who've encountered this problem, but found nothing. I uninstalled and reinstalled <a href=""https://cloud.google.com/build/docs/automating-builds/create-github-app-triggers#creating_github_app_triggers"" rel=""nofollow noreferrer"">the GitHub app</a> to no avail.</p>
<p>Has anyone encountered this, and if so, how do you fix it?</p>
<p><strong>EDIT</strong></p>
<p>I found a workaround, looks like builds can be manually run</p>
<p><a href=""https://i.stack.imgur.com/6ee6K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6ee6K.png"" alt=""manual run button"" /></a></p>
<p>Notice in the build history, the manual run has found a real commit, whereas the build triggered by a push to GitHub is coming up with bogus commits...</p>
<p><a href=""https://i.stack.imgur.com/f8eLQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f8eLQ.png"" alt=""manual trigger correct commit, push trigger wrong commit"" /></a></p>","<p>Check if the issue persists (especially after you trigger a manual run and find back normal existing SHA).</p>
<p>There was an <a href=""https://status.cloud.google.com/incident/cloud-tasks/21001"" rel=""nofollow noreferrer"">incident affecting Cloud Tasks scheduler jobs</a> which might have a side-effect on your build status. Considering the incident is resolved, you should not see such an issue again.</p>
<p>The <a href=""https://stackoverflow.com/users/680920/quickshiftin"">OP quickshiftin</a> confirms <a href=""https://stackoverflow.com/questions/66809588/google-cloud-build-mysteriously-wrong-git-hashes/66811795#comment118113802_66811795"">in the comments</a> it was more a GitHub issue than a Google Cloud one:</p>
<blockquote>
<ul>
<li>[It] looks like that incident was in <code>us-central1</code>, my servers are in <code>us-west1</code>.</li>
<li>Also, I got a response from a GitHub ticket I filed, where they said it appears to relate to changes they made &quot;very recently&quot; and reverted last night.<br />
As of this morning, the build is working again (nothing changed on my side). &gt;
Presumably the issue was from either Google or GitHub.</li>
</ul>
</blockquote>"
"App Engine Cloud Build throwing incorrect composer install error<p>So...I have a weird one. I have a composer.json file formatted like this:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;require&quot;: {
        &quot;twilio/sdk&quot;: &quot;^6.20&quot;
    },
    &quot;autoload&quot;: {
        &quot;classmap&quot;: [
            &quot;model/*&quot;
        ]
    }
}
</code></pre>
<p>And I am trying to deploy a PHP application with that composer.json to an App Engine app service. But, when Cloud Build runs <code>composer install --no-dev --no-progress --no-suggest --no-interaction</code> while building the application, Composer throws this error:</p>
<pre class=""lang-sh prettyprint-override""><code>Step #6 - &quot;builder&quot;: 
Step #6 - &quot;builder&quot;:                                                                                                  
Step #6 - &quot;builder&quot;:   [RuntimeException]                                                                             
Step #6 - &quot;builder&quot;:   Could not scan for classes inside &quot;model/*&quot; which does not appear to be a file nor a folder  
Step #6 - &quot;builder&quot;:                                                                                                  
Step #6 - &quot;builder&quot;: 
</code></pre>
<p>The problem is, when I run that exact command in the directory App Engine is building, I generate the autoload classes fine.</p>
<p>There is one class recursively in the model folder right now.</p>
<p>Also, when I remove the <code>autoload</code> section from the <code>composer.json</code>, it deploys to app engine just fine, and I have verified the existence of the directory in the deployed app files.</p>
<p>So my question is:</p>
<p><strong>Why is Cloud Build getting a composer error that I can't reproduce locally?</strong></p>
<p>---- EDIT ----</p>
<p>My file structure looks like this:
<a href=""https://i.stack.imgur.com/w1Zk7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w1Zk7.png"" alt=""File structure for this project"" /></a></p>
<p><a href=""https://i.stack.imgur.com/9KuQs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9KuQs.png"" alt=""File Structure again"" /></a></p>","<p>Wildcard is supported in classmap path since Composer 2.0. I assume that during build Composer 1.x is used, and it does not recognize this syntax.</p>
<p>I suggest to just remove asterisk from path, since the meaning is exactly the same and it works on older Composer too.</p>
<pre><code>&quot;autoload&quot;: {
    &quot;classmap&quot;: [
        &quot;model/&quot;
    ]
}
</code></pre>"
"Google Cloud Build mount volume in another path<p>Locally I run this command in order to build a PDF file:</p>
<pre><code>docker run -it -v $PWD:/doc/ -v $PWD/fonts/:/usr/share/fonts/external/ thomasweise/texlive sh -c 'xelatex *.tex'
</code></pre>
<p>And I have a <code>cloudbuild.yaml</code> file to build on Google Cloud</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: thomasweise/texlive
    entrypoint: sh
    args:
      - -c
      - |
        xelatex *.tex
  - name: gcr.io/cloud-builders/gsutil
    args: [&quot;cp&quot;, &quot;*.pdf&quot;, &quot;gs://storage.skhaz.io&quot;]
</code></pre>
<p>But my build fails because I need to mount a extra volume: <code>-v $PWD/fonts/:/usr/share/fonts/external/</code></p>
<p>How can I translate the command above to <code>cloudbuild.yaml</code>?</p>","<p>As you can see in <a href=""https://cloud.google.com/build/docs/build-config#volumes"" rel=""nofollow noreferrer"">the documentation</a>, you can achieve this in 2 steps</p>
<pre><code>#1st step: Create the volume and populate it
- name: 'gcr.io/cloud-builders/gcloud'
  volumes:
  - name: 'vol1'
    path: '/usr/share/fonts/external'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
        cp /workspace/path/to/fonts/* /usr/share/fonts/external/

# Then, use the volume with your fonts in it
- name: thomasweise/texlive
    entrypoint: sh
    args:
      - -c
      - |
        xelatex *.tex
  volumes:
  - name: 'vol1'
    path: '/usr/share/fonts/external'
</code></pre>"
"How to use relative dependencies when deploying to App Engine with Cloud Build<p>I have a full stack project that have some common modules, mostly type definitions. I want to keep them in the same repository to keep a consistent state of my application. However, the deploying doesn't work as I expect it. During my first two build steps yarn install and yarn build there are no apparent issues with my relative dependency.</p>
<p>Gcloud, however, doesn't seem to find it. At the 6th build step it shows the follwing error message:</p>
<p><a href=""https://i.stack.imgur.com/eUq79.png"" rel=""nofollow noreferrer"">Gcloud building steps</a></p>
<pre><code>Already have image (with digest): eu.gcr.io/gae-runtimes/buildpacks/nodejs12/builder:nodejs12_20210308_12_21_0_RC00
=== Node.js - Yarn (google.nodejs.yarn@0.9.0) ===
--------------------------------------------------------------------------------
Running &quot;bash -c command -v yarn || true&quot;
/usr/bin/yarn
Done &quot;bash -c command -v yarn || true&quot; (53.639626ms)
DEBUG: Yarn is already installed, skipping installation.
--------------------------------------------------------------------------------
Running &quot;node -v&quot;
v12.21.0
Done &quot;node -v&quot; (35.463448ms)
DEBUG: Current dependency hash: &quot;8d626f89a4d8d27dba2aab32f998ebdf2b45531f6abae7f55eabb00d85cff016&quot;
DEBUG:   Cache dependency hash: &quot;e6c8baadd1eb6d1fd69fc3255ba5936ea43d709f6502d54719fdb17c9d9865af&quot;
Installing application dependencies.
DEBUG: ***** CACHE MISS: &quot;prod dependencies&quot;
--------------------------------------------------------------------------------
Running &quot;node -v&quot;
v12.21.0
Done &quot;node -v&quot; (5.746529ms)
--------------------------------------------------------------------------------
Running &quot;yarn install --non-interactive --frozen-lockfile (NODE_ENV=production)&quot;
yarn install v1.22.4
[1/4] Resolving packages...
error Package &quot;common&quot; refers to a non-existing file '&quot;/common&quot;'.
info Visit https://yarnpkg.com/en/docs/cli/install for documentation about this command.
Done &quot;yarn install --non-interactive --frozen-lockfile (NODE_ENV=p...&quot; (528.261919ms)
Failure: (ID: 98e389b6) yarn install v1.22.4
[1/4] Resolving packages...
error Package &quot;common&quot; refers to a non-existing file '&quot;/common&quot;'.
info Visit https://yarnpkg.com/en/docs/cli/install for documentation about this command.
--------------------------------------------------------------------------------
Running &quot;mv -f /builder/outputs/output-5577006791947779410 /builder/outputs/output&quot;
Done &quot;mv -f /builder/outputs/output-5577006791947779410 /builder/o...&quot; (15.651484ms)
ERROR: failed to build: exit status 1
</code></pre>
<p>Folder structure:</p>
<pre><code>client/
    package.json
    cloudbuild.json
common/
    package.json
server/
    package.json
    cloudbuild.json
</code></pre>
<p>client/package.json</p>
<pre><code>{
    &quot;name&quot;: &quot;client&quot;,
    &quot;dependencies&quot;: {
        &quot;common&quot;: &quot;file:../common&quot;
    }
}
</code></pre>
<p>client/cloudbuild.json</p>
<pre><code>{
    &quot;steps&quot;: [
        {
            &quot;name&quot;: &quot;node&quot;,
            &quot;entrypoint&quot;: &quot;yarn&quot;,
            &quot;args&quot;: [&quot;--cwd&quot;, &quot;client&quot;, &quot;install&quot;]
        },
        {
            &quot;name&quot;: &quot;node&quot;,
            &quot;dir&quot;: &quot;client&quot;,
            &quot;entrypoint&quot;: &quot;yarn&quot;,
            &quot;args&quot;: [&quot;run&quot;, &quot;build&quot;]
        },
        {
            &quot;name&quot;: &quot;gcr.io/google.com/cloudsdktool/cloud-sdk&quot;,
            &quot;entrypoint&quot;: &quot;bash&quot;,
            &quot;dir&quot;: &quot;client&quot;,
            &quot;args&quot;: [&quot;gcloud&quot;, &quot;app&quot;, &quot;deploy&quot;]
        }
    ],
    &quot;timeout&quot;: &quot;1600s&quot;
}
</code></pre>
<p>How do I need to adjust my cloudbuild file so the builder will find my relative dependency?</p>","<p>You should combine everything (<code>client</code>, <code>server</code> and <code>common</code>) into a single Cloud Build config.</p>
<p>When you <code>gcloud build submit</code>, the source directory is transferred to the Cloud Build service (VM) and placed in a directory called <code>/workspace</code>. If you want to be able to make references to <code>common</code> from <code>client</code> or <code>server</code> then this directory must be copied as part of their build process. To save energy, it's convenient to build <code>common</code> once and then build <code>client</code> and <code>server</code> referencing it.</p>
<p>You would then able to build the <code>client</code> and <code>server</code> by referencing <code>common</code> because it's been transferred too.</p>
<p>You are able to produce multiple images from one Cloud Build job and so you should produce one for the <code>client</code> and one for the <code>server</code>.</p>
<p>Folder structure:</p>
<pre><code>cloudbuild.json
client/
    package.json

common/
    package.json
server/
    package.json
</code></pre>
<p>Alternatively and I'm not as familiar with Node.JS package management, you could (Cloud) Build <code>common</code> first and make this accessible to the (Cloud) Builds for <code>client</code> and <code>server</code>. Customarily this would be through some (network-accessible) private package management solution that <code>client</code> and <code>server</code> would reference as part of their Build.</p>"
"Cloud Build Trigger - Not in Firebase app directory error<p>I have problem with deploying node.js (Angular) application from Bitbucket repository to Firebase using Cloud Build Trigger.
I performed steps from this article - <a href=""https://cloud.google.com/build/docs/deploying-builds/deploy-firebase"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/deploying-builds/deploy-firebase</a></p>
<ul>
<li>Necessary APIs are turned on,</li>
<li>Service account has all required permissions,</li>
<li>Firebase community builder is deployed and visible in Container Registry,</li>
<li>Cloudbuild.json file is added to repository,</li>
<li>Cloud Build Trigger is created and it points to one specific branch of my repository.</li>
</ul>
<p>Problem is that after running Cloud Build Trigger I receive following error: &quot;Error: Not in Firebase app directory (could not locate firebase.json)&quot;</p>
<p>What could be reason of such error? Is it possible to point to the trigger where in repository is firebase application and firebase.json file?</p>
<p>EDIT:<br />
My cloudbuild.json file is quite simple:</p>
<pre><code>{
  &quot;steps&quot;: [
   {
      &quot;name&quot;: &quot;gcr.io/PROJECT_NAME/firebase&quot;,
      &quot;args&quot;: [
         &quot;deploy&quot;,
         &quot;--project&quot;,
         &quot;PROJECT_NAME&quot;,
         &quot;--only&quot;,
         &quot;functions&quot;
       ]
  }
  ]
}
</code></pre>
<p>Logs in Cloud Build Trigger history:</p>
<pre><code>  starting build &quot;build_id&quot;

FETCHSOURCE
Initialized empty Git repository in /workspace/.git/
From https://source.developers.google.com/p/link
 * branch            branch_id -&gt; FETCH_HEAD
HEAD is now at d8bc2f1 Add cloudbuild.json file
BUILD
Pulling image: gcr.io/PROJECT/firebase
Using default tag: latest
latest: Pulling PROJECT/firebase
1e987daa2432: Already exists
a0edb687a3da: Already exists
6891892cc2ec: Already exists
684eb726ddc5: Already exists
b0af097f0da6: Already exists
154aee36a7da: Already exists
37e5835696f7: Pulling fs layer
62eb6e670f1d: Pulling fs layer
47e62615d9f9: Pulling fs layer
428cea824ccd: Pulling fs layer
765a2c722bf9: Pulling fs layer
b3f5d0a285e3: Pulling fs layer
428cea824ccd: Waiting
765a2c722bf9: Waiting
b3f5d0a285e3: Waiting
47e62615d9f9: Verifying Checksum
47e62615d9f9: Download complete
62eb6e670f1d: Verifying Checksum
62eb6e670f1d: Download complete
765a2c722bf9: Verifying Checksum
765a2c722bf9: Download complete
b3f5d0a285e3: Verifying Checksum
b3f5d0a285e3: Download complete
37e5835696f7: Verifying Checksum
37e5835696f7: Download complete
428cea824ccd: Verifying Checksum
428cea824ccd: Download complete
37e5835696f7: Pull complete
62eb6e670f1d: Pull complete
47e62615d9f9: Pull complete
428cea824ccd: Pull complete
765a2c722bf9: Pull complete
b3f5d0a285e3: Pull complete
Digest: sha256:4b6b7214d6344c8247130bf3f5443a2851e39aed3ececb32dfb2cc25a5c07e44
Status: Downloaded newer image for gcr.io/PROJECT/firebase:latest
gcr.io/PROJECT/firebase:latest

Error: Not in a Firebase app directory (could not locate firebase.json)
ERROR
ERROR: build step 0 &quot;gcr.io/PROJECT/firebase&quot; failed: step exited with non-zero status: 1
</code></pre>
<p>Firebase.json file is placed in repository in following path: /apps/firebase/firebase.json
I can't see any possibility to point that path in Cloud Build Trigger config.</p>","<p>I'm pretty sure that you aren't in the correct directory when you run the command. To check this, you can add this step</p>
<pre><code>{
  &quot;steps&quot;: [
   {
      &quot;name&quot;: &quot;gcr.io/cloud-builders/gcloud&quot;,
      &quot;entrypoint&quot;:&quot;ls&quot;
      &quot;args&quot;: [&quot;-la&quot;]
  }
  ]
}
</code></pre>
<p>If you aren't in the correct directory, and you need to be in the <code>./app/firebase</code> to run your deploy, you can add this parameter in your step</p>
<pre><code>{
  &quot;steps&quot;: [
   {
      &quot;name&quot;: &quot;gcr.io/PROJECT_NAME/firebase&quot;,
      &quot;dir&quot;: &quot;app/firebase&quot;,
      &quot;args&quot;: [
         &quot;deploy&quot;,
         &quot;--project&quot;,
         &quot;PROJECT_NAME&quot;,
         &quot;--only&quot;,
         &quot;functions&quot;
       ]
  }
  ]
}
</code></pre>
<p>Let me know if it's better</p>"
"POST body for Google Cloud Build - Webhook Triggers<p>The Google Cloud Build - Webhook Trigger <a href=""https://cloud.google.com/build/docs/automating-builds/create-webhook-triggers#creating_webhook_triggers"" rel=""nofollow noreferrer"">create trigger documentation</a> shows the proper URL to POST to invoke the build trigger.  However the documentation does not describe the POST body, which seems to be required.  I have successfully triggered the cloud build webhooks using <code>content-type: application/json</code> header with a POST body of <code>{}</code>, but it would be nice to know:</p>
<ul>
<li>What is the POST body supposed to be?</li>
<li>Are we able to pass substitution variables in the POST body?</li>
</ul>
<p>The <a href=""https://cloud.google.com/build/docs/api/reference/rest/v1/projects.triggers/webhook"" rel=""nofollow noreferrer"">Google Cloud Build - REST API documentation</a> provides some additional hints that a <a href=""https://cloud.google.com/build/docs/api/reference/rest/v1/projects.triggers/webhook#HttpBody"" rel=""nofollow noreferrer"">HttpBody</a> payload is accepted, but no additional information past that as for as I can tell.</p>","<p>The body is what you want! In fact, in your trigger you customize your substitution variable like this (from the documentation)</p>
<pre><code> --subtitutions=\
         _SUB_ONE='$(body.message.test)', _SUB_TWO='$(body.message.output)'
</code></pre>
<p>So, your body need to be like that</p>
<pre><code>{
  &quot;message&quot;: {
    &quot;test&quot;: &quot;test value&quot;,
    &quot;ourput&quot;: &quot;my output&quot;
  }
}
</code></pre>
<p>The data are automatically extracted from your body content. So you can add more substitutions or change the format of your JSON and thus of your substitutions value.</p>"
"Google Cloud Build - Different scopes of Dockerfile and cloudbuild.yaml<p>I recently asked a question about why I get the error <code>Specified public directory 'dist/browser' does not exist, can't deploy hosting to site PROJECT-ID</code> when I´m trying to deploy to Firebase Hosting in my <code>cloudbuild.yaml</code>. However, since I find the question too bloated with information I tried to break it down.</p>
<p>I created a simple image to visualize what happens when I call <code>gcloud builds submit --config=cloudbuild.yaml</code>. So why can´t I access the directory <code>dist/browser</code> from <code>cloudbuild.yaml</code> even though it is processed after the <code>Dockerfile</code> where the directory <code>dist/browser</code> is created?</p>
<p><a href=""https://i.stack.imgur.com/5b6z0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5b6z0.png"" alt=""enter image description here"" /></a></p>","<p>Cloud Build is best conceptualized as a series of functions (steps) applied to data in the form of a local file system (often just <code>/workspace</code> as this is a default volume mount added to each step, but you can add other volume mounts) and the Internet.</p>
<p>Output of each function (step) is self-contained unless you explicitly publish data back to one of these two sources (one of the step's volume mounts or the Internet).</p>
<p>In this case <code>docker build</code> consumes local files (not shown in your example) and generates <code>dist/browser</code> in the image that results but this folder is <strong>only</strong> accessible within that image; nothing is added to e.g. <code>/workspace</code> that you could use in subsequent steps.</p>
<p>In order to use that directory subsequently:</p>
<ul>
<li>Hack a way to mount the (file system of the) image generated by the step and extract the directory from it (not advised; possible not permitted).</li>
<li>You'd need to run that image as a container and then <code>docker cp</code> files from it back into the Cloud Build's (VM's) file system (perhaps somewhere on <code>/workspace</code>).</li>
<li>Not put the directory in an image in the first place (see below)</li>
</ul>
<h3>Proposal</h3>
<p>Instead of <code>docker build</code>'ing an image containing the directory, deconstruct the Dockerfile into a series of Cloud Build steps. This way, the artifacts you want (if written somewhere under one of the step's volume mounts), will be available in subsequent steps:</p>
<pre><code>steps:
- name: gcr.io/cloud-builders/npm
  args:
  - install
- name: gcr.io/cloud-builders/npm
  args:
  - run
  - build:ssr # Presumably this is where dist/browser is generated?
- name: firebase
  args:
  - deploy # dist/browser
</code></pre>
<blockquote>
<p><strong>NOTE</strong> Every Cloud Build step has an implicit:</p>
<pre><code>- name: some-step
  volumes:
  - name: workspace
    path: /workspace
</code></pre>
</blockquote>
<h3>Proof</h3>
<p>Here's a minimal Cloud Build config that uses a volume called <code>testdir</code> that maps to the Cloud Build VM's <code>/testdir</code> directory.</p>
<blockquote>
<p><strong>NOTE</strong> The example uses <code>testdir</code> to prove the point. Each Cloud Build step automatically mounts <code>/workspace</code> and this could be used instead.</p>
</blockquote>
<p>The config:</p>
<ul>
<li>Lists the empty <code>/testdir</code></li>
<li>Creates a file <code>freddie.txt</code> in <code>/testdir</code></li>
<li>Lists <code>/testdir</code> now containing <code>freddie.txt</code></li>
</ul>
<pre><code>options:
# volumes:
#   - name: testdir
#     path: /testdir

steps:
  - name: busybox
    volumes:
      - name: testdir
        path: /testdir
    args:
      - ash
      - -c
      - &quot;ls -1a /testdir&quot;
  - name: busybox
    volumes:
      - name: testdir
        path: /testdir
    args:
      - ash
      - -c
      - 'echo &quot;Hello Freddie&quot; &gt; /testdir/freddie.txt'
  - name: busybox
    volumes:
      - name: testdir
        path: /testdir
    args:
      - ash
      - -c
      - &quot;ls -1a /testdir&quot;
</code></pre>
<blockquote>
<p><strong>NOTE</strong> Uncommenting <code>volumes</code> under <code>options</code> would remove the need to reproduce the <code>volumes</code> in each step.</p>
</blockquote>
<p>The edited output is:</p>
<pre class=""lang-sh prettyprint-override""><code>gcloud builds submit \
--config=./cloudbuild.yaml \
--project=${PROJECT}

# Lists (empty) /testdir
Starting Step #0
Step #0: Pulling image: busybox
Step #0: .
Step #0: ..

# Creates /test/freddie.txt
Starting Step #1
Step #1: Already have image: busybox
Finished Step #1

# List /testdir containing freddie.txt
Starting Step #2
Step #2: .
Step #2: ..
Step #2: freddie.txt
Finished Step #2
</code></pre>"
"App Engine deploy composer memory limit exceeded<p>Recently without any code change except environmental variable in app.yaml, app engine deployments fails during cloud build process where it exceeds memory limit, and I can't understand where can I change it, or why it became a problem... I tried to set &quot;gcp-build&quot; to overwrite composer install command, but getting this error during cloud build:</p>
<pre><code>Step #3 - &quot;detector&quot;: ======== Output: google.php.composer-gcp-build@0.9.0 ========
Step #3 - &quot;detector&quot;: unmarshalling composer.json: json: cannot unmarshal array into Go struct field     composerScriptsJSON.scripts.gcp-build of type string [id:070ec49f]
Step #3 - &quot;detector&quot;: ======== Results ========
</code></pre>
<p>Here is my composer.json:</p>
<pre><code>{
&quot;name&quot;: &quot;laravel/laravel&quot;,
&quot;type&quot;: &quot;project&quot;,
&quot;description&quot;: &quot;The Laravel Framework.&quot;,
&quot;keywords&quot;: [
    &quot;framework&quot;,
    &quot;laravel&quot;
],
&quot;license&quot;: &quot;MIT&quot;,
&quot;require&quot;: {
    &quot;php&quot;: &quot;^7.2.5&quot;,
    &quot;doctrine/dbal&quot;: &quot;^2.10&quot;,
    &quot;fideloper/proxy&quot;: &quot;^4.2&quot;,
    &quot;fruitcake/laravel-cors&quot;: &quot;^2.0&quot;,
    &quot;google/cloud&quot;: &quot;^0.142.0&quot;,
    &quot;google/cloud-core&quot;: &quot;^1.39&quot;,
    &quot;google/cloud-error-reporting&quot;: &quot;^0.18.0&quot;,
    &quot;google/cloud-firestore&quot;: &quot;^1.14&quot;,
    &quot;google/cloud-logging&quot;: &quot;^1.21&quot;,
    &quot;guzzlehttp/guzzle&quot;: &quot;^7.0.1&quot;,
    &quot;intervention/image&quot;: &quot;^2.5&quot;,
    &quot;kreait/firebase-php&quot;: &quot;^5.0&quot;,
    &quot;kreait/firebase-tokens&quot;: &quot;^1.10&quot;,
    &quot;kreait/laravel-firebase&quot;: &quot;^2.2&quot;,
    &quot;kyslik/column-sortable&quot;: &quot;^6.3&quot;,
    &quot;laravel/framework&quot;: &quot;^8.0&quot;,
    &quot;laravel/tinker&quot;: &quot;^2.0&quot;,
    &quot;laravel/ui&quot;: &quot;^3.0&quot;,
    &quot;owen-it/laravel-auditing&quot;: &quot;^10.0&quot;,
    &quot;predis/predis&quot;: &quot;^1.1&quot;,
    &quot;propaganistas/laravel-phone&quot;: &quot;^4.2&quot;,
    &quot;superbalist/laravel-google-cloud-storage&quot;: &quot;^2.2&quot;,
    &quot;tymon/jwt-auth&quot;: &quot;^1.0&quot;
},
&quot;require-dev&quot;: {
    &quot;fzaninotto/faker&quot;: &quot;^1.9.1&quot;,
    &quot;mockery/mockery&quot;: &quot;^1.3.1&quot;,
    &quot;phpunit/phpunit&quot;: &quot;^9.0&quot;,
    &quot;nunomaduro/collision&quot;: &quot;^5.0&quot;
},
&quot;config&quot;: {
    &quot;optimize-autoloader&quot;: true,
    &quot;preferred-install&quot;: &quot;dist&quot;,
    &quot;sort-packages&quot;: true
},
&quot;extra&quot;: {
    &quot;laravel&quot;: {
        &quot;dont-discover&quot;: []
    }
},
&quot;autoload&quot;: {
    &quot;psr-4&quot;: {
        &quot;App\\&quot;: &quot;app/&quot;,
        &quot;Database\\Factories\\&quot;: &quot;database/factories/&quot;,
        &quot;Database\\Seeders\\&quot;: &quot;database/seeders/&quot;
    },
    &quot;classmap&quot;: [
        &quot;database/seeders&quot;,
        &quot;database/factories&quot;
    ]
},
&quot;autoload-dev&quot;: {
    &quot;psr-4&quot;: {
        &quot;Tests\\&quot;: &quot;tests/&quot;
    }
},
&quot;minimum-stability&quot;: &quot;dev&quot;,
&quot;prefer-stable&quot;: true,
&quot;scripts&quot;: {
    &quot;post-autoload-dump&quot;: [
        &quot;Illuminate\\Foundation\\ComposerScripts::postAutoloadDump&quot;
    ],
    &quot;post-root-package-install&quot;: [
        &quot;@php -r \&quot;file_exists('.env') || copy('.env.example', '.env');\&quot;&quot;
    ],
    &quot;post-create-project-cmd&quot;: [
        &quot;@php artisan key:generate --ansi&quot;
    ],
    &quot;post-install-cmd&quot;: [
        &quot;Illuminate\\Foundation\\ComposerScripts::postInstall&quot;,
        &quot;composer dump-autoload&quot;,
        &quot;php artisan optimize&quot;,
        &quot;chmod -R 755 bootstrap\/cache&quot;,
        &quot;php artisan migrate  --no-interaction --force&quot;
    ],
    &quot;gcp-build&quot;: [
        &quot;COMPOSER_MEMORY_LIMIT=-1 composer install --no-dev&quot;
    ]
}
</code></pre>
<p>}</p>
<p>Also, is there any chance to update composer to v2 as I getting this warning during cloud build:</p>
<pre><code>Step #6 - &quot;builder&quot;: Warning from https://repo.packagist.org: You are using an outdated version of Composer. Composer 2.0 is now available and you should upgrade. See https://getcomposer.org/2
</code></pre>
<p>And here is actual issue that fails the build:</p>
<pre><code>Step #6 - &quot;builder&quot;: Updating dependencies
Step #6 - &quot;builder&quot;: 
Step #6 - &quot;builder&quot;: Fatal error: Allowed memory size of 1610612736 bytes exhausted (tried to allocate 4096 bytes) in phar:///usr/local/bin/composer/src/Composer/DependencyResolver/RuleWatchGraph.php on line 52
Step #6 - &quot;builder&quot;: 
Step #6 - &quot;builder&quot;: Check https://getcomposer.org/doc/articles/troubleshooting.md#memory-limit-errors for more info on how to handle out of memory errors.Done &quot;composer install --no-dev --no-progress --no-suggest --no-in...&quot; (56.105751566s)
Step #6 - &quot;builder&quot;: Failure: (ID: 5888fcc4) Loading composer repositories with package information
Step #6 - &quot;builder&quot;: Warning from https://repo.packagist.org: You are using an outdated version of Composer. Composer 2.0 is now available and you should upgrade. See https://getcomposer.org/2
Step #6 - &quot;builder&quot;: Updating dependencies
Step #6 - &quot;builder&quot;: 
Step #6 - &quot;builder&quot;: Fatal error: Allowed memory size of 1610612736 bytes exhausted (tried to allocate 4096 bytes) in phar:///usr/local/bin/composer/src/Composer/DependencyResolver/RuleWatchGraph.php on line 52
Step #6 - &quot;builder&quot;: 
Step #6 - &quot;builder&quot;: Check https://getcomposer.org/doc/articles/troubleshooting.md#memory-limit-errors for more info on how to handle out of memory errors.
</code></pre>
<p>Runtime used in app.yaml is php74, also tried to change it to php72, same issue</p>","<p>Since your deployed application already has a <code>composer.lock</code> file generated from App Engine deployment and by default, App Engine caches fetched dependencies to reduce build times. It will prevents you from automatically getting the latest versions of your dependencies and you will encounter the error:</p>
<blockquote>
<p>You are using an outdated version of Composer</p>
</blockquote>
<p>To resolve the issue, run command <code>composer install</code> locally to pin your dependencies to their current version and to have a <code>composer.lock</code> and <code>composer update</code> if <code>composer.lock</code> is <strong>existing</strong>. Then deploy using the command 'gcloud beta app deploy --no-cache' to install an uncached version of the dependency.</p>"
"Google-cloud-build cannot be canceled<p>This is a very specific question about one of our cloud-builds.</p>
<p>A certain build, that is usually running within 10 minutes, is running almost 2 hours now. It seems that it is stuck in one build step. I tried cancel the build through the website and the cli via <code>gcloud builds cancel</code> but both didnt' work.</p>
<p>Although gcloud is returning <code>Cancelled [https://cloudbuild.googleapis.com/v1/projects/.../builds/...]</code>, the build still is up and running.</p>
<p>Our build-timeout is set for 30 minutes but it was simply ignored.</p>
<p>Any idea how to cancel this build?</p>","<p>Ok, problem &quot;solved&quot; itself after waiting long enough. After almost 2 1/2 hours the build finally canceld itself.</p>
<p>The same build triggered afterwards finished without any problems.</p>"
"Design advice calling Cloud Build from Airflow to execute a python function that takes args<p>I’m seeking some design advice and any examples using Cloud Build to execute a python function that takes args.</p>
<p>I have a data pipeline using Airflow on GCP. Some files need cleaning before loading into BigQuery. I have a Python function that does the job.
What I would like is to run that function on a Cloud Build image and pass the function args from my pipeline.</p>
<p>So far I have a Cloud Build trigger that installs and runs a simple Python function with no args.</p>
<p>From my research I have found two options, the cloudbuild_v1 library and a Cloud Build Airflow operator.</p>
<p>I would like advice on which option I should use or if other options should be considered and any example to work from.</p>
<p>Thank you.</p>","<p>There is not just one correct or best option of which one of them you should use. It actually would really depend on your application, your preferences and requisites, etc. I would say for you to take a deeper look at both of them and confirm which one fits you better. If I would say one of them, I would say Cloud Build Airflow, as it has more content available for learning and searching.</p>
<p>Regarding to examples, you can check them <a href=""https://googleapis.dev/python/cloudbuild/latest/_modules/google/cloud/devtools/cloudbuild_v1/gapic/cloud_build_client.html"" rel=""nofollow noreferrer"">here</a> for the <code>cloudbuild_v1</code> library and <a href=""https://airflow.apache.org/docs/stable/howto/operator/gcp/cloud_build.html"" rel=""nofollow noreferrer"">here</a> or <a href=""https://towardsdatascience.com/testing-airflow-jobs-on-google-cloud-composer-using-pytest-9e0a1198b4cd"" rel=""nofollow noreferrer"">here</a> for Cloud Build Airflow.</p>"
"Airflow Cloud Build Operator not executing python function<p>I can’t get the second arg statement to execute:</p>
<pre><code>&quot;python&quot;, &quot;-c&quot;, &quot;from main import foo;print(foo('hello'))&quot;
</code></pre>
<p>I’m triggering a Cloud Build using the Airflow operator CloudBuildCreateBuildOperator.
Both the triggering DAG and Cloud Build return successful.</p>
<p>The first arg to install the requirements.txt file appears to work based on the Cloud Build returned log message:</p>
<pre><code>Requirement already satisfied: setuptools&gt;=40.3.0 in /usr/local/lib/python3.7/site-packages (from google-auth&gt;=1.2-&gt;gcsfs&gt;=0.6.2
</code></pre>
<p>My function is using the gcsfs package. The function I have used in the question is just for demonstration.</p>
<p>Before using Airflow I had this working in a yaml file.
Tried a number of different ways of stating the argument, such as:</p>
<pre><code>&quot;args&quot;:[&quot;-c&quot;,&quot;pip install -V -r requirements.txt; python -c from main import foo;print(foo('hello'))&quot;]
</code></pre>
<p>But now going round in circles.</p>
<p>I would greatly appreciate any help to get that args statement working.</p>
<p>This is the CloudBuildCreateBuildOperator from the DAG:</p>
<pre><code>create_build_from_storage_body = {
  &quot;source&quot;: {
    &quot;repoSource&quot;: {
      &quot;projectId&quot;: &quot;dev-6767&quot;,
      &quot;repoName&quot;: &quot;bq-pipeline&quot;,
      &quot;branchName&quot;: &quot;master&quot;,
    }
  },
  &quot;steps&quot;: [{
    &quot;name&quot;: &quot;docker.io/library/python:3.7&quot;,
    &quot;entrypoint&quot;: &quot;/bin/bash&quot;,
    &quot;args&quot;: [&quot;-c&quot;, &quot;pip install -V -r requirements.txt&quot;,
      &quot;python&quot;, &quot;-c&quot;, &quot;from main import foo;print(foo('hello'))&quot;
    ],
  }],
}
</code></pre>","<p>As explained in this example <a href=""https://1904labs.com/2020/08/14/how-to-test-and-deploy-airflow-dags-to-cloud-composer/"" rel=""nofollow noreferrer"">here</a> the best way to chain commands and send multiple <code>args</code> with Python would be with adding an <code>&amp;&amp;</code> between them.</p>
<p>Below is an example of how to send the args using this format, for chain of commands.</p>
<pre><code>#testing1
steps:
- name: 'docker.io/library/python:3.7'
  id: Test
  entrypoint: /bin/sh
  args:
  - -c
  - 'pip install pytest &amp;&amp; pip install -r requirements.txt &amp;&amp; pytest test/*_test.py'
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
</code></pre>
<p>This way, it's possible use more than one argument within the Cloud Build trigger.</p>"
"Is there a way to allow cloudbuild steps to access the Cloud SQL in GCP<p>I'm setting up a cloud build trigger in order to deploy a PHP/Symfony Application. When the docker file runs the <code>php app/console assetic:dump</code> command in order to create the assets I get the following error.</p>

<pre><code>SQLSTATE[HY000] [2002] Connection timed out 
[PDOException] 

SQLSTATE[HY000] [2002] Connection timed out
[Doctrine\DBAL\Driver\PDOException] 


An exception occurred in driver: SQLSTATE[HY000] [2002] 
Connection timed out 
[Doctrine\DBAL\Exception\ConnectionException] 
</code></pre>

<p>I have resolved to trying to get the docker container to connect to the database instead of trying to fix the symfony application because I don't know enough about the framework or php.</p>

<p>Is it possible to set this up so that I can allow some kind of IP on the CLOUDSQL side to allow these connections?</p>","<p>A solution to setup the proxy in the same step is described in the answer here:</p>
<p><a href=""https://stackoverflow.com/questions/52352103/run-node-js-database-migrations-on-google-cloud-sql-during-google-cloud-build/64599510#64599510"">Run node.js database migrations on Google Cloud SQL during Google Cloud Build</a></p>"
"My cloudbuild.yaml is failing. Please review my cloudbuild.yaml<p>I am trying to set a react app to a kubernetes cluster. All my kubernetes files resides in k8s/ folder. In k8s/ folder I have a deployment.yaml and service.yaml file.</p>
<p>The below is my cloudbuild.yaml file which resides in the root folder. This part <code>gcr.io/cloud-builders/kubectl</code> Stage 3 is failing. I get the below error</p>
<pre><code>build step 2 &quot;gcr.io/cloud-builders/kubectl&quot; failed: step exited with non-zero status: 1
</code></pre>
<pre><code>steps:

# Build the image - Stage 1
- name: 'gcr.io/cloud-builders/docker'
  args: ['build','-t','gcr.io/${_PROJECT}/${_CONTAINERNAME}:${_VERSION}','.']
  timeout: 1500s

# Push the image - Stage 2
- name: 'gcr.io/cloud-builders/docker'
  args: ['push','gcr.io/${_PROJECT}/${_CONTAINERNAME}:${_VERSION}']
  
# Deploy changes to kubernetes config files - Stage 3
- name: &quot;gcr.io/cloud-builders/kubectl&quot;
  args: [&quot;apply&quot;, &quot;-f&quot;, &quot;k8s/&quot;]
  env:
  - 'CLOUDSDK_COMPUTE_ZONE=${_ZONE}'
  - 'CLOUDSDK_CONTAINER_CLUSTER=${_GKE_CLUSTER}'

# These are variable substitutions  
substitutions:
    #GCP Specific configuration. Please DON'T change anything
    _PROJECT: my-projects-121212
    _ZONE: us-central1-c
    _GKE_CLUSTER: cluster-1
    
    #Repository Specific configuration. DevOps can change this settings
    _DEPLOYMENTNAME: react
    _CONTAINERNAME: react    
    _REPO_NAME: react-app
    
    # Developers ONLY change
    _VERSION: v1.0
    
options:
    substitution_option: 'ALLOW_LOOSE'
    machineType: 'N1_HIGHCPU_8' 
timeout: 2500s
</code></pre>","<p>In step 3, there are double quotes <code>name: &quot;gcr.io/cloud-builders/kubectl&quot;</code></p>
<p>If you replace them with single quotes, the issue should be fixed.</p>"
"Deployment.yaml Template file string substitution with sed, all that match a pattern with env var $_<pattern><p>I have a kubernetes deployment template like the following</p>
<pre class=""lang-yaml prettyprint-override""><code>[...other stuff...]
data:
  ENV_VAR_1: _ENV_VAR_1
  ENV_VAR_2: _ENV_VAR_2
  ENV_VAR_ETC: _ENV_VAR_ETC
</code></pre>
<p>I want to instruct my <code>cloudbuild.yaml</code> to replace occurences of _SOME_THING to the actual value of the environment variable <code>$_SOME_THING</code>. I know I can execute Bash commands so I tried this with sed:</p>
<pre class=""lang-yaml prettyprint-override""><code># \x27 is the character '
entrypoint: 'bash'
    args:
      - '-eEuo'
      - 'pipefail'
      - '-c'
      - |-
        sed -n 's%\([A-Z][A-Z_0-9]*\):[[:space:]]*_\1[[:space:]]*$%sed -i \
        \x27s;_\1[[:space:]]*$;\x27\$_\1\x27;g\x27 $_K8S_YAML_FILE%gp' \
        $_K8S_YAML_FILE | sh
</code></pre>
<p>The idea is generating a substitution commands for each variable and then piping them to shell. The generated commands look good:</p>
<pre class=""lang-sh prettyprint-override""><code>sed -i 's;_ENV_VAR_1[[:space:]]*$;'$_ENV_VAR_1';g' $_K8S_YAML_FILE
sed -i 's;_ENV_VAR_2[[:space:]]*$;'$_ENV_VAR_2';g' $_K8S_YAML_FILE
sed -i 's;_ENV_VAR_ETC[[:space:]]*$;'$_ENV_VAR_ETC';g' $_K8S_YAML_FILE
</code></pre>
<p>And the whole thing work when I execute it in git bash after setting the env variables with <code>export</code>, but when executing it through the cloudbuild.yaml the file is modified like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>[...other stuff...]
data: 
  ENV_VAR_1: 1
  ENV_VAR_2: 1
  ENV_VAR_ETC: 1
</code></pre>
<p>I think I'm missing some escape or the command line options passed to bash (-eEuo) are messing things up, but I have no clue. Any solution, even not using sed for the task (but requiring no additional tool to install) would be greatly appreciated. Thank you.</p>","<p>I came to the conclusion the thing I'm trying to achieve is not possible. Starting a shell via cloudbuild as above and trying to print out env variables (via <code>printenv</code>) and shell variables (<code>set -o posix ; set</code>) does not show any of my variables, that where set as trigger substitution. As far as I know this may actually be what the name says, that is string replacement that are performed on the actual file before it is processed. The evidence is in no way i can pull out the value of those reference if not directly writing them in the file. Any strategy of building a reference and then having it evaluated has failed. Final solution I adopted (that keep the nuisance of having to update the file for every new variable but reads and deal with escapes much better than sed) is the envsubst cloudbuilder (<a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/envsubst"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/envsubst</a>), propagating each var from file to the shell (<code>env: ['_MY_VAR=$_MYVAR', ...]</code>)</p>"
"Facing error while deploying node.js app on google cloud<p>I am trying to deploy <code>node.js</code> app on google cloud but getting following error. I am facing this error since 10th October. don't know how to resolve:</p>
<pre><code>Step #2: ERROR: (gcloud.beta.app.deploy) PERMISSION_DENIED: You do not have permission to act as '[xxxxxxx]@appspot.gserviceaccount.com'
Step #2: - '@type': type.googleapis.com/google.rpc.ResourceInfo
Step #2:   description: You do not have permission to act as this service account.
Step #2:   resourceName: [xxxxxx]@appspot.gserviceaccount.com
Step #2:   resourceType: serviceAccount
Finished Step #2
ERROR
ERROR: build step 2 &quot;gcr.io/cloud-builders/gcloud&quot; failed: step exited with non-zero status: 1
</code></pre>
<p><code>CloudBuild.yaml</code>:</p>
<pre><code>steps:

- name: node:10.15.1
  entrypoint: npm
  args: ['install']

- name: node:10.15.1
  entrypoint: npm
  args: [ 'run', 'build', '--prod --verbose']

- name: 'gcr.io/cloud-builders/gcloud'
  args: ['beta', 'app', 'deploy', '--version=prod', '--no-cache']


timeout: '4800s'
</code></pre>","<p>I believe you should grant the <code>Service Account User</code> permission to my CI/CD service account.</p>
<p>You can use the console UI or gcloud command:</p>
<p><a href=""https://cloud.google.com/iam/docs/granting-changing-revoking-access"" rel=""nofollow noreferrer"">Granting, changing, and revoking access to resources</a></p>
<pre><code>gcloud iam service-accounts add-iam-policy-binding \
    ${PROJECT_ID}@appspot.gserviceaccount.com \
    --member=serviceAccount:${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com \
    --role=roles/iam.serviceAccountUser \
    --project=${PROJECT_ID}
</code></pre>"
"Google Cloud: Build always fails, MANIFEST_UNKNOWN error<p>I'm trying to deploy an app engine application, but no matter which code I'm trying to deploy, even if it's taken from their samples at:</p>
<p><a href=""https://github.com/GoogleCloudPlatform/golang-samples"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/golang-samples</a></p>
<p>I get the same error:</p>
<pre><code>Step #1: error building image: getting stage builder for stage 0: MANIFEST_UNKNOWN: &quot;Manifest with digest 'sha256:249859465bcde1cb15128ff0d9eb2bb54de67f72a834a7576e6649cfe0a27698' has media type 'application/vnd.docker.distribution.manifest.list.v2+json', but client accepts 'application/vnd.docker.distribution.manifest.v2+json'.&quot;
</code></pre>
<p>I'm not good at devops, so no idea howto work this around</p>","<p>I've encountered the same issue recently. After some digging, it turns out that the issue was reported on the GCP Issue Tracker already: <a href=""https://issuetracker.google.com/issues/171756486"" rel=""nofollow noreferrer"">https://issuetracker.google.com/issues/171756486</a></p>
<p>As a workaround update the <code>app.yaml</code> file to contain <code>runtime: go1.13</code>.</p>
<p><strong>Update:</strong></p>
<p>Deploying with <code>runtime: go1.13</code> only works for the first run.
It will still fail when attempting to re-deploy.</p>
<p><strong>Update 2:</strong></p>
<p>The Google team has rolled back the issue on their side. (<a href=""https://issuetracker.google.com/issues/171756486"" rel=""nofollow noreferrer"">source</a>)</p>
<p>To make it work you will need to downgrade your <code>gcloud</code> to version <code>315.0.0</code>:</p>
<pre><code>gcloud components update --version 315.0.0
</code></pre>
<p>After that, the deployment will proceed as usual.</p>"
"Google app engine - disable automatic start of new deployed service versions<p>I'm using cloud build to automatically deploy new version of app when a specific github branch is updated.</p>
<p>Everything works great but I'd like to deploy the new version without starting it, keeping my previous version running.
I really prefer to schedule start of the new version in specific hours, but when cloud build finish the process, my new version is automatically running.</p>
<p>How can I change this behaviour?</p>","<p>To avoid that the new version is started you can deploy using the <code>--no-promote</code> <a href=""https://cloud.google.com/sdk/gcloud/reference/app/deploy#--promote"" rel=""nofollow noreferrer"">flag</a>.</p>
<p>To use this flag when deploying using gcloud just need to add it to the command and deploy like this:</p>
<p><code>gcloud app deploy --no-promote</code></p>
<p>To implement this as part of the CD with Cloud Build then you can edit the cloudbuild.yaml to add the flag like this:</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['app', 'deploy', '--no-promote']
  timeout: '1600s'

</code></pre>"
"Using Google Cloud Secret as environment variables in Google Cloud Build<p>I'm deploying my Node apps to Google Cloud Run using Cloud Build and I want to run some tests during the build. My tests require some environment variables, so I have been following <a href=""https://cloud.google.com/cloud-build/docs/securing-builds/use-encrypted-secrets-credentials#yaml"" rel=""nofollow noreferrer"">this guide</a> to achieve this.</p>
<p>The guide makes the following note:</p>
<blockquote>
<p>Note: To use the secret in an environment variable, you need to prefix
the variable name with an underscore &quot;_&quot; and escape the value using
'('. For example: _VARIABLE_NAME=$(cat password.txt) &amp;&amp; echo -n
\)_VARIABLE_NAME.</p>
</blockquote>
<p>However, I am not quite sure how to implement this.
I have attempted the following in my <code>cloudbuild.yaml</code>.</p>
<pre class=""lang-yaml prettyprint-override""><code>  - id: Execute tests
    name: node
    args: ['_VAR_ONE=$(cat var-one.txt)', '_VAR_TWO=$(cat var-two.txt)', 'jest -V']
</code></pre>
<p>Which returns the following: <code>Error: Cannot find module '/workspace/_VAR_ONE=$(cat var-one.txt)'</code>.
I also tried a few variations of the escape that the above note mentions, but they result in the same error.</p>
<p>What's the best way to get the secrets into my code as environment variables?
Also, if I need to use multiple environment variables, is it better to use Cloud KMS with an <code>.env</code> file?</p>
<p>Thanks!</p>","<p>It looks like you are incorrectly using the <code>entrypoint</code> provided by the <code>node</code> image. You are effectively running the command:</p>
<pre><code>node _VAR_ONE=$(cat var-one.txt) _VAR_TWO=$(cat var-two.txt) jest -V
</code></pre>
<p>I want to digress for a moment and say this pattern does not work in Node, you need to specify the environment variables first <strong>before</strong> calling <code>node</code>, for example <code>VAR_ONE=$(cat foo.txt) VAR_TWO=bar node run test</code></p>
<p>Anyway, I think what you want to run is:</p>
<pre><code>_VAR_ONE=$(cat var-one.txt) _VAR_TWO=$(cat var-two.txt) jest -V
</code></pre>
<p>This is how we will do that - Assuming you have a previous step where you write out the contents of the secret into the files <code>var-one.txt</code> and <code>var-two.txt</code> in a previous step - here is how you would use it in the <code>node</code> step, it's just the standard way you use environment variables when running a command from the command line:</p>
<pre><code>- id: Execute tests
  name: node
  entrypoint: '/bin/bash'
  args:
     '-c', 
     '_VAR_ONE=$(cat var-one.txt) _VAR_TWO=$(cat var-two.txt) jest -V'
  ]
</code></pre>
<p>You need to ensure in the node environment you are using the variables as specified (ie. <code>process.env._VAR_ONE</code> or <code>process.env._VAR_TWO</code>). I don't think you need to have the <code>_</code> character prefixed here but I haven't tested it to confirm that. You can try the above and it should get you much further I think.</p>"
"How to access log files created by GCP cloud build steps?<p>My Cloud build fails with a timeout on <code>npm test</code>, and no useful information is sent to stdout. A complete log can be found in a file, but I couldn't find a way to ssh in the cloud build environment.</p>
<pre><code>Already have image: node

&gt; project-client@v3.0.14 test
&gt; jest

ts-jest[versions] (WARN) Version 4.1.0-beta of typescript installed has not been tested with ts-jest. If you're experiencing issues, consider using a supported version (&gt;=3.8.0 &lt;5.0.0-0). Please do not report issues in ts-jest if you are using unsupported versions.
npm ERR! path /workspace/v3/client
npm ERR! command failed
npm ERR! signal SIGTERM
npm ERR! command sh -c jest

npm ERR! A complete log of this run can be found in:
npm ERR!     /builder/home/.npm/_logs/2020-11-09T07_56_23_573Z-debug.log
</code></pre>
<p>Since I have no problem running the tests locally, I'd like to see the content of that <code>2020-11-09T07_56_23_573Z-debug.log</code> file to hopefully get a hint at what might be wrong.</p>
<p>Is there a way to retrieve the file content ?</p>
<ul>
<li>Ssh in a cloud build environment?</li>
<li>Get npm to print the complete log to stdout?</li>
<li>Some way to save the log file artifact to cloud storage ?</li>
</ul>","<p>I had a similar issue with error management on Gitlab CI and my workaround is inspired from there.</p>
<p>The trick is to embed your command in something that exit with a return code 0. Here an example</p>
<pre><code> - name: node
   entrypoint: &quot;bash&quot;
   args:
     - &quot;-c&quot;
     - |
          RETURN_CODE=$$(jtest &gt; log.stdout 2&gt;log.stderr;echo $${?})
          cat log.stdout
          cat log.stderr
          if [ $${RETURN_CODE} -gt 0 ]; 
          then 
            #Do what you want in case of error, like a cat of the files in the _logs dir
            # Break the build: exit 1
          else 
            #Do what you want in case of success. Nothing to continue to the next step
          fi
</code></pre>
<p>Some explanations:</p>
<ul>
<li>echo $${?}: the double $ is to indicate to Cloud Build to not use substitution variables but to ignore it and let Linux command being interpreted.</li>
<li>The $? allows you to get the exit code of the previous command</li>
<li>Then you test the exit code, if &gt; 0, you can perform actions. At the end, I recommend to break the build to not continue with erroneous sources.</li>
<li>You can parse the log.stderr file to get useful info in it (the log file for example)</li>
</ul>"
"Google Cloud Build gsutil with python entrypoint<p>I want to use a python script to download several files from Google Storage by calling <code>gsutil</code> via <code>subprocess.check_call()</code> inside the Python script during a Cloud Build step.</p>
<p>Therefore, I use the following step inside the <code>cloudbuild.yaml</code>:</p>
<pre><code>- name: 'gcr.io/cloud-builders/gsutil'
  entrypoint: 'bash'
  args: ['-c',
         'pip install google-cloud-storage &amp;&amp; python /path_to_script/script.py --arg1 $_ARG1 --arg2 $TAG_NAME --arg3 $BRANCH_NAME --arg4 $_ARG4'
  ]
</code></pre>
<p>But Cloud Build throws a strange <code>SyntaxError: invalid syntax</code> Python error. I can run the same Python script with the same arguments locally <strong>as well as</strong> by using <code>python:3.7-sim</code> inside Cloud Build with that SyntaxError:</p>
<pre><code>- name: 'python:3.7-slim'
  entrypoint: /bin/sh
  args: ['-c',
         'pip install google-cloud-storage &amp;&amp; python python /path_to_script/script.py --arg1 $_ARG1 --arg2 $TAG_NAME --arg3 $BRANCH_NAME --arg4 $_ARG4'
  ]
</code></pre>
<p>The later example is working until <code>gsutil</code> is called, because it is not part of the <code>python:3.7-slim</code> container (therefor expected behavior).</p>
<p>Any helping comments on why <code>gcr.io/cloud-builders/gsutil</code> is throwing that error? Is the entrypoint wrongly specified?</p>","<p>I found an easier solution by invoking the dedicated <code>gsutil</code> image in combination with wildcards to accomplish a similar filtering as in the python script.
See simplified example below:</p>
<pre><code>- name: 'gcr.io/cloud-builders/gsutil'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    gsutil -m cp -r 'gs://bucket/*/' '/workspace'
</code></pre>
<p>This actually does the job for me pretty well.</p>"
"Cloud Build failing to npm install a NodeJS project<p>I'm running the following command in my <code>cloudbuild.yaml</code> file:</p>
<pre><code>  - name: &quot;gcr.io/cloud-builders/npm&quot;
    args: [&quot;install&quot;]
</code></pre>
<p>This command ran correctly on 30th July, but on the 5th October onwards I'm getting this response, followed by a stream of <code>make</code> output until the command fails:</p>
<pre><code>Already have image (with digest): gcr.io/cloud-builders/npm

&gt; grpc@1.24.1 install /workspace/node_modules/grpc
&gt; node-pre-gyp install --fallback-to-build --library=static_library

node-pre-gyp WARN Using request for node-pre-gyp https download 
node-pre-gyp WARN Tried to download(404): https://node-precompiled-binaries.grpc.io/grpc/v1.24.1/node-v83-linux-x64-glibc.tar.gz 
node-pre-gyp WARN Pre-built binaries not found for grpc@1.24.1 and node@14.10.0 (node-v83 ABI, glibc) (falling back to source compile with node-gyp) 
</code></pre>
<p>I get this response even when I rebuild from git commits that previously succeeded</p>
<p>I believe it could be due to Cloud Build attempting to build my NodeJS project as a different version to previously, based on the output from the most recent successful build:</p>
<pre><code>Already have image (with digest): gcr.io/cloud-builders/npm

&gt; grpc@1.24.1 install /workspace/node_modules/grpc
&gt; node-pre-gyp install --fallback-to-build --library=static_library

node-pre-gyp WARN Using request for node-pre-gyp https download 
[grpc] Success: &quot;/workspace/node_modules/grpc/src/node/extension_binary/node-v57-linux-x64-glibc/grpc_node.node&quot; is installed via remote
</code></pre>
<p>I added the following line to my <code>package.json</code> in an attempt to set my NodeJS version to <code>v57</code> (<code>6.13.4</code>), but it had no effect:</p>
<pre><code>  &quot;engines&quot; : { &quot;node&quot; : &quot;6.13.4&quot; },
</code></pre>
<p>Any help would be much appreciated. Preferably how I can prevent my build versions from changing as this unexpected change has cost me more hours than it should have</p>
<p>Thanks!</p>","<p>It is an error with the node version, <a href=""https://gcr.io/cloud-builders/npm"" rel=""nofollow noreferrer"">check the available versions here</a></p>
<p>set version, example:</p>
<pre><code>  - name: &quot;gcr.io/cloud-builders/npm:node-12.18.3&quot;
    args: [&quot;install&quot;]
</code></pre>"
"GCP Cloud Build publishing single event for each Cloud Function deployment<p>We have a microservice deployed using Cloud Build consisting of several Cloud Functions. Till yesterday we used to get a single pub/sub event published into cloud-builds pub/sub topic informing us of the status of the build (SUCCESS/FAIL) but now we see events into cloud-builds pub/sub topic for each Cloud Function and also the final status of the build.</p>
<p>This change produces 10s of email notifications whereas we expect one notification per build.</p>
<p>Not sure what was the change done from GCP's side (Could not find any documentation informing the new change).</p>","<p>It's a bug fix (or a feature deployment). It's normal. In fact, for many months, each time that you deploy a Cloud Functions, a Cloud Build is trigger to build it (I'm in the alpha to see the Cloud Build logs automatically created by the <code>gcloud functions deploy</code> command).</p>
<p>So now, because a Cloud Build is triggered to build the Cloud Functions, it's normal to publish a notification as any Cloud Build jobs. If you deploy an App Engine, you should see the same thing (because <code>gcloud app deploy</code> also trigger a Cloud Build for the deployment).</p>"
"Using bash variables in arguments in Google Cloud Build provider<p>Here is one of my steps in the cloud build trigger.</p>
<pre><code>step {
      name = &quot;gcr.io/cloud-builders/docker&quot;       
entrypoint = &quot;bash&quot;
      args = [&quot;-c&quot;, &quot;docker&quot;, &quot;build&quot;, &quot;-t&quot;, &quot;gcr.io/something:$(date +%m.%d)&quot;, &quot;.&quot;]          
    }
</code></pre>
<p>It applies properly, but when I try to run the trigger, it complains about having a bad tag name.
<code>failed to find one or more images after execution of build steps:</code></p>","<p>The arguments shouldn't be separated.
It should look like:</p>
<pre><code>step {
      name = &quot;gcr.io/cloud-builders/docker&quot;       
entrypoint = &quot;bash&quot;
      args = [&quot;-c&quot;, &quot;docker build -t gcr.io/something:$(date +%m.%d)} .&quot;]         
    }
</code></pre>"
"How can I create an environment file during a cloud build process<p>How can I pass environment variables to a Gatsby build task in a Google Cloud Build CI process? Using the substitution variables I can make variables available in the <code>cloudbuild.json</code> file but these then need to be available in the build task.</p>
<p>Gatsby uses a <code>.env.production</code> file to hold the environment variables which are then available using the <code>dotenv</code> package. At the top of my <code>gatsby-config.js</code> file I set the path to the environment file as follows:</p>
<pre><code>require(&quot;dotenv&quot;).config({
  path: `.env.${process.env.NODE_ENV}`,
})
</code></pre>
<p>Further down the file I use these variables to configure the <code>gatsby-plugin-firebase</code> plugin for Firebase. Given that I need an environment file, I have tried to create one in the <code>cloudbuild.json</code> file before running the build step.</p>
<pre><code>{
  &quot;steps&quot;: [
    {
      &quot;name&quot;: &quot;ubuntu&quot;,
      &quot;args&quot;: [&quot;echo&quot;, &quot;FIREBASE_API_KEY=$_FIREBASE_API_KEY\\nFIREBASE_AUTH_DOMAIN=$_FIREBASE_AUTH_DOMAIN\\nFIREBASE_DATABASE_URL=$_FIREBASE_DATABASE_URL\\nFIREBASE_PROJECT_ID=$_FIREBASE_PROJECT_ID\\nFIREBASE_STORAGE_BUCKET=$_FIREBASE_STORAGE_BUCKET\\nFIREBASE_MESSAGING_SENDER_ID=$_FIREBASE_MESSAGING_SENDER_ID\\nFIREBASE_APP_ID=$_FIREBASE_APP_ID\\nFIREBASE_MEASUREMENT_ID=$_FIREBASE_MEASUREMENT_ID&quot;, &quot;&gt;&quot;, &quot;.env.production&quot;]
    },
    ...More steps here...
    {
      &quot;name&quot;: &quot;node:14.4.0&quot;,
      &quot;entrypoint&quot;: &quot;npm&quot;,
      &quot;args&quot;: [&quot;run&quot;, &quot;build&quot;]
    },
    {
      &quot;name&quot;: &quot;node:14.4.0&quot;,
      &quot;entrypoint&quot;: &quot;./node_modules/.bin/firebase&quot;,
      &quot;args&quot;: [&quot;deploy&quot;, &quot;--project&quot;, &quot;$PROJECT_ID&quot;, &quot;--token&quot;, &quot;$_FIREBASE_TOKEN&quot;]
    }
</code></pre>
<p>The <code>.env.production</code> file does not exist when I get to the build step, which I think is because it has been created in the ubuntu container. How can I create an environment file that can be read by the build step. Or is there a better way of passing the variables?</p>
<p>Thanks,</p>","<p>Your first step is wrong, you only echo the command, not execute it. Change it like this</p>
<pre><code>{
  &quot;steps&quot;: [
    {
      &quot;name&quot;: &quot;ubuntu&quot;,
      &quot;entrypoint&quot;: &quot;bash&quot;,
      &quot;args&quot;: [&quot;-c&quot;, &quot;echo FIREBASE_API_KEY=$_FIREBASE_API_KEY\\nFIREBASE_AUTH_DOMAIN=$_FIREBASE_AUTH_DOMAIN\\nFIREBASE_DATABASE_URL=$_FIREBASE_DATABASE_URL\\nFIREBASE_PROJECT_ID=$_FIREBASE_PROJECT_ID\\nFIREBASE_STORAGE_BUCKET=$_FIREBASE_STORAGE_BUCKET\\nFIREBASE_MESSAGING_SENDER_ID=$_FIREBASE_MESSAGING_SENDER_ID\\nFIREBASE_APP_ID=$_FIREBASE_APP_ID\\nFIREBASE_MEASUREMENT_ID=$_FIREBASE_MEASUREMENT_ID &gt; .env.production&quot;]
    },


</code></pre>"
"Google cloud - cloubduild and app variabile substitution in trigger<p>I'm using cloudbuild to deploy new version of my app when a new commit appears in github.
Everything is working good.</p>
<p>Now I'm trying to setup a variable substitution in the trigger configuration, because I want to put my version number in the trigger once, so that I can find the deployed correct version without modifying cloudbuild configuration file.
Variabile substitution works great in my cloudbuild file, for example:
(cloudbuild.yaml)</p>
<pre><code># TEST: PRINT VARIABLE IN LOG
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args: ['-c', 'echo', '${_VERSION}']

# DEPLOY APP
- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  args: [&quot;app&quot;, &quot;deploy&quot;, &quot;-v&quot;, &quot;${_VERSION}&quot;, &quot;app.yaml&quot;]
  dir: 'frontend'
  timeout: &quot;20m&quot;
</code></pre>
<p>${_VERSION} is correctly replaced with the string I put into my trigger.</p>
<p>Now I want to obtain the same result in app.yaml file, substituting an env variabile, something like:
(app.yaml)</p>
<pre><code>runtime: nodejs
env: flex
service: backend

env_variables:
  VERSION: &quot;${_VERSION}&quot;
  TEST_ENV: &quot;read from google&quot;
</code></pre>
<p>When I read TEST_ENV from my app, it works, but _VERSION is not replaced.</p>
<p>Any suggestion?</p>","<p>When you perform this step</p>
<pre><code># DEPLOY APP
- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  args: [&quot;app&quot;, &quot;deploy&quot;, &quot;-v&quot;, &quot;${_VERSION}&quot;, &quot;app.yaml&quot;]
  dir: 'frontend'
  timeout: &quot;20m&quot;
</code></pre>
<p>The <code>app.yaml</code> is provided as-is to the gcloud command, and it's not evaluated. You have to update it manually. Something like this</p>
<pre><code># REPLACE: PUT THE CORRECT VALUE IN APP.YAML FILE
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args: ['-c', 'sed', &quot;-i&quot;, &quot;sed -i &quot;s/\$${_VERSION}/${_VERSION}/g&quot;, 'app.yaml']
</code></pre>
<p>Of course if you let the</p>
<pre><code>env_variables:
  VERSION: &quot;${_VERSION}&quot;
</code></pre>
<p>as-is in your <code>app.yaml</code> file. You can change this replacement string</p>"
"Issue in deploying NodeJs app on Google Cloud Build<p>I am trying to deploy <code>node.js</code> app on google cloud but getting the following error:</p>
<pre><code>Finished Step #2 - &quot;detector&quot;
Starting Step #3 - &quot;analyzer&quot;
Step #3 - &quot;analyzer&quot;: Already have image (with digest): asia.gcr.io/gae-runtimes/buildpacks/nodejs10/builder:nodejs10_20200913_10_22_0_RC00
Step #3 - &quot;analyzer&quot;: ERROR: failed to initialize cache: failed to create image cache: accessing cache image &quot;asia.gcr.io/[redacted]/app-engine-tmp/build-cache/ttl-7d/default/buildpack-cache:latest&quot;: connect to repo store 'asia.gcr.io/[redacted]/app-engine-tmp/build-cache/ttl-7d/default/buildpack-cache:latest': GET https://asia.gcr.io/v2/[redacted]/app-engine-tmp/build-cache/ttl-7d/default/buildpack-cache/manifests/latest: DENIED: Permission denied for &quot;latest&quot; from request &quot;/v2/[redacted]/app-engine-tmp/build-cache/ttl-7d/default/buildpack-cache/manifests/latest&quot;. 
Finished Step #3 - &quot;analyzer&quot;
ERROR
ERROR: build step 3 &quot;asia.gcr.io/gae-runtimes/buildpacks/nodejs10/builder:nodejs10_20200913_10_22_0_RC00&quot; failed: step exited with non-zero status: 1

</code></pre>
<p>Originally I was facing the issue mentioned in this thread: <a href=""https://stackoverflow.com/questions/64331395/facing-error-while-deploying-node-js-app-on-google-cloud"">Facing error while deploying node.js app on google cloud</a></p>
<p>Then, after using the solution, I am facing this new error.</p>
<p>CloudBuild.yaml:</p>
<pre><code>steps:

- name: node:10.15.1
  entrypoint: npm
  args: ['install']

- name: node:10.15.1
  entrypoint: npm
  args: [ 'run', 'build', '--prod --verbose']

- name: 'gcr.io/cloud-builders/gcloud'
  args: ['beta', 'app', 'deploy', '--version=prod', '--no-cache']


timeout: '4800s'
</code></pre>","<p>Searching for the error message, the most common cause for such errors is billing issues. Please make sure that billing is enabled and there are no issues with previous payments.</p>
<p>If this doesn't solve the issue, please follow Google's documentation on how to <a href=""https://cloud.google.com/container-registry/docs/troubleshooting#permission_issues_when_communicating_with"" rel=""nofollow noreferrer"">troubleshoot common Container Registry and Docker issues</a>.</p>
<p>Let me know how this works for you.</p>"
"Google Cloud Build using volumes on Dockerfile<p>I'm trying to build a build pipeline for my application and share a specific folder between steps using volumes.</p>
<p>The problem is because on my first step (<strong>unit-tests</strong>) I have to install all the libs on the requirements.txt to be able to run my unit tests. And after that I have to build my application running my Dockerfile in other step. I don't want to re-install all the requirements again, so, I thought in copy the requirements already installed and paste them inside the docker build step. Am I able to do that? I followed this thread and tried to replicate to my reality but I still have problems.</p>
<blockquote>
<p><a href=""https://stackoverflow.com/questions/48274288/passing-files-from-google-cloud-container-builder-to-docker-build-task"">Passing files from Google Cloud Container Builder to Docker build task</a></p>
</blockquote>
<p>Here is a sample of what I've done:</p>
<p>My cloudbuild.yaml:</p>
<pre><code>- id: unit-tests
  name: python:3-alpine
  entrypoint: sh
  dir: my-dir
  args:
    - -c
    - |
      apk add --virtual build_dependencies build-base
      apk add --no-cache libstdc++
      pip install --no-cache-dir -r requirements.txt
      apk del build_dependencies
      python -m unittest discover --verbose
      cp -Rv /usr/local/lib/python3.8/site-packages/* /requirements
  volumes:
    - name: 'requirements'
      path: /requirements

- id: docker-build
  name: gcr.io/cloud-builders/docker
  dir: my-dir
  args:
  - build
  - --tag=gcr.io/${PROJECT_ID}/eta:test
  - '.'
  volumes:
    - name: 'requirements'
      path: /requirements
  waitFor: ['unit-tests']

images:
- 'gcr.io/$PROJECT_ID/eta:test'

timeout: 3600s
</code></pre>
<p>And here is my Dockerfile:</p>
<pre><code>from python:3-alpine

RUN set -eux; \
    apk add --virtual build_dependencies build-base; \
    apk add --no-cache libstdc++; \
    apk del build_dependencies; 

COPY /requirements/* /usr/local/lib/python3.8/site-packages/
COPY . /app
RUN [&quot;chmod&quot;, &quot;+x&quot;, &quot;/app/entry.sh&quot;]
WORKDIR /app
ENTRYPOINT [ &quot;/app/entry.sh&quot; ]

</code></pre>
<p>I can't locate the <strong>/requirements</strong> folder inside my dockerfile, here is the error message:</p>
<pre><code>Step #1 - &quot;docker-build&quot;: Step 3/7 : COPY /requirements/* /usr/local/lib/python3.8/site-packages/
Step #1 - &quot;docker-build&quot;: COPY failed: no source files were specified
Finished Step #1 - &quot;docker-build&quot;
ERROR
ERROR: build step 1 &quot;gcr.io/cloud-builders/docker&quot; failed: step exited with non-zero status: 1

</code></pre>
<p>I'm not sure if what I'm doing is the right way.</p>","<p>The Cloud Build (VM) persists <code>/workspace</code> across steps so you may create e.g. <code>/workspace/requirements</code> and use <code>requirements</code> in subsequent steps.</p>
<pre class=""lang-sh prettyprint-override""><code>PROJECT=[[YOUR-PROJECT]]
BILLING=[[YOUR-BILLING]]

gcloud projects create ${PROJECT}

gcloud beta billing projects link ${PROJECT} \
--billing-account=${BILLING}

gcloud services enable cloudbuild.googleapis.com \
--project=${PROJECT}
</code></pre>
<p>Then:</p>
<pre><code>steps:

  - id: also
    name: busybox
    entrypoint: sh
    dir: x
    args:
      - -c
      - |
        mkdir -p /workspace/requirements
        echo &quot;Hello Freddie&quot; &gt; /workspace/requirements/freddie.txt

  - id: after
    name: gcr.io/cloud-builders/docker
    args:
      - build
      - --tag=gcr.io/${PROJECT_ID}/test
      - .
images:
  - gcr.io/${PROJECT_ID}/test
</code></pre>
<p>And <code>Dockerfile</code>:</p>
<pre><code>FROM busybox

WORKDIR /test

COPY /requirements/* .

RUN ls -la

ENTRYPOINT [&quot;more&quot;,&quot;/test/freddie.txt&quot;]
</code></pre>
<p>And:</p>
<pre class=""lang-sh prettyprint-override""><code>gcloud builds submit . \
--config=./cloudbuild.yaml \
--project=${PROJECT}
</code></pre>
<p>Yields:</p>
<pre><code>BUILD
Starting Step #0 - &quot;also&quot;
Step #0 - &quot;also&quot;: Pulling image: busybox
Step #0 - &quot;also&quot;: Using default tag: latest
Step #0 - &quot;also&quot;: latest: Pulling from library/busybox
Step #0 - &quot;also&quot;: Digest: sha256:2ca5e69e
Step #0 - &quot;also&quot;: Status: Downloaded newer image for busybox:latest
Step #0 - &quot;also&quot;: docker.io/library/busybox:latest
Finished Step #0 - &quot;also&quot;
Starting Step #1 - &quot;after&quot;
Step #1 - &quot;after&quot;: Already have image (with digest): gcr.io/cloud-builders/docker
Step #1 - &quot;after&quot;: Sending build context to Docker daemon   7.68kB
Step #1 - &quot;after&quot;: Step 1/5 : FROM busybox
Step #1 - &quot;after&quot;:  ---&gt; 6858809bf669
Step #1 - &quot;after&quot;: Step 2/5 : WORKDIR /test
Step #1 - &quot;after&quot;:  ---&gt; Running in 7e075adce9eb
Step #1 - &quot;after&quot;: Removing intermediate container 7e075adce9eb
Step #1 - &quot;after&quot;:  ---&gt; 78f7b2faec16
Step #1 - &quot;after&quot;: Step 3/5 : COPY /requirements/* .
Step #1 - &quot;after&quot;:  ---&gt; 1ec6a35c1deb
Step #1 - &quot;after&quot;: Step 4/5 : RUN ls -la
Step #1 - &quot;after&quot;:  ---&gt; Running in 943ddad6434c
Step #1 - &quot;after&quot;: total 12
Step #1 - &quot;after&quot;: .
Step #1 - &quot;after&quot;: ..
Step #1 - &quot;after&quot;: freddie.txt
Step #1 - &quot;after&quot;: Removing intermediate container 943ddad6434c
Step #1 - &quot;after&quot;:  ---&gt; 634072b8aec4
Step #1 - &quot;after&quot;: Step 5/5 : ENTRYPOINT [&quot;more&quot;,&quot;/test/freddie.txt&quot;]
Step #1 - &quot;after&quot;:  ---&gt; Running in 13c43b5ca924
Step #1 - &quot;after&quot;: Removing intermediate container 13c43b5ca924
Step #1 - &quot;after&quot;:  ---&gt; f0ebfe1e8e33
Step #1 - &quot;after&quot;: Successfully built f0ebfe1e8e33
Step #1 - &quot;after&quot;: Successfully tagged gcr.io/.../test:latest
Finished Step #1 - &quot;after&quot;
PUSH
</code></pre>
<p>The problem using <code>volumes</code> is that these won't be in the docker build's context. So, if you'd prefer to use volumes, you'd need to copy the volume's content into <code>/workspace</code>.</p>
<p>Then, when you run the <code>gcr.io/cloud-builders/docker</code> step, <code>/workspace</code> is mounted as the root and so you may access <code>/workspace/requirements</code> as <code>/requirements</code>:</p>
<p>Then:</p>
<pre><code>steps:
  - id: before
    name: busybox
    entrypoint: sh
    dir: x
    args:
      - -c
      - |
        echo &quot;Hello Freddie&quot; &gt; /requirements/freddie.txt
    volumes:
      - name: requirements
        path: /requirements

  - id: during
    name: busybox
    entrypoint: sh
    dir: x
    args:
      - -c
      - |
        more /requirements/freddie.txt
    volumes:
      - name: requirements
        path: /requirements

  - id: fix
    name: busybox
    entrypoint: sh
    args:
      - -c
      - |
        ls -la /workspace
        ls -la /requirements
        mkdir -p /workspace/requirements
        cp -r /requirements /workspace/requirements
        ls -la /workspace/requirements
    volumes:
      - name: requirements
        path: /requirements

  - id: after
    name: gcr.io/cloud-builders/docker
    args:
      - build
      - --tag=gcr.io/${PROJECT_ID}/test
      - .

images:
  - gcr.io/${PROJECT_ID}/test

</code></pre>
<p>Yields:</p>
<pre><code>BUILD
Starting Step #0 - &quot;before&quot;
Step #0 - &quot;before&quot;: Pulling image: busybox
Step #0 - &quot;before&quot;: Using default tag: latest
Step #0 - &quot;before&quot;: latest: Pulling from library/busybox
Step #0 - &quot;before&quot;: Digest: sha256:2ca5e69e
Step #0 - &quot;before&quot;: Status: Downloaded newer image for busybox:latest
Step #0 - &quot;before&quot;: docker.io/library/busybox:latest
Finished Step #0 - &quot;before&quot;
Starting Step #1 - &quot;during&quot;
Step #1 - &quot;during&quot;: Already have image: busybox
Step #1 - &quot;during&quot;: Hello Freddie
Finished Step #1 - &quot;during&quot;
Starting Step #2 - &quot;fix&quot;
Step #2 - &quot;fix&quot;: Already have image: busybox
Step #2 - &quot;fix&quot;: total 24
Step #2 - &quot;fix&quot;: .
Step #2 - &quot;fix&quot;: ..
Step #2 - &quot;fix&quot;: Dockerfile
Step #2 - &quot;fix&quot;: cloudbuild.yaml
Step #2 - &quot;fix&quot;: requirements
Finished Step #2 - &quot;fix&quot;
Starting Step #3 - &quot;after&quot;
Step #3 - &quot;after&quot;: Already have image (with digest): gcr.io/cloud-builders/docker
Step #3 - &quot;after&quot;: Sending build context to Docker daemon  8.192kB
Step #3 - &quot;after&quot;: Step 1/5 : FROM busybox
Step #3 - &quot;after&quot;:  ---&gt; 6858809bf669
Step #3 - &quot;after&quot;: Step 2/5 : WORKDIR /test
Step #3 - &quot;after&quot;:  ---&gt; Running in 236aa78f2229
Step #3 - &quot;after&quot;: Removing intermediate container 236aa78f2229
Step #3 - &quot;after&quot;:  ---&gt; cf05164e8175
Step #3 - &quot;after&quot;: Step 3/5 : COPY /requirements/* .
Step #3 - &quot;after&quot;:  ---&gt; f5dee710b4d7
Step #3 - &quot;after&quot;: Step 4/5 : RUN ls -la
Step #3 - &quot;after&quot;:  ---&gt; Running in 8a30309a1536
Step #3 - &quot;after&quot;: total 12
Step #3 - &quot;after&quot;: .
Step #3 - &quot;after&quot;: ..
Step #3 - &quot;after&quot;: freddie.txt
Step #3 - &quot;after&quot;: Removing intermediate container 8a30309a1536
Step #3 - &quot;after&quot;:  ---&gt; 20e2e99c3818
Step #3 - &quot;after&quot;: Step 5/5 : ENTRYPOINT [&quot;more&quot;,&quot;/test/freddie.txt&quot;]
Step #3 - &quot;after&quot;:  ---&gt; Running in cc6a57aaa103
Step #3 - &quot;after&quot;: Removing intermediate container cc6a57aaa103
Step #3 - &quot;after&quot;:  ---&gt; 0a50096e471c
Step #3 - &quot;after&quot;: Successfully built 0a50096e471c
Step #3 - &quot;after&quot;: Successfully tagged gcr.io/.../test:latest
Finished Step #3 - &quot;after&quot;
PUSH
Pushing gcr.io/.../test
...
DONE
</code></pre>"
"How to pass substitution variables on Google Cloud build to run a dockerfile<p>I have set up a Github trigger on Google Cloud build. I already have a dockerfile to do the build steps. Since there is no provision to pass substitution variables I am trying to build it with cloud build configuration file (yaml) and then passing the path to the dockerfile in the configuration file.</p>
<p>This is the cloud build configuration file where I need to pass 5 variables for the Dockerfile to consume and the yaml file goes like this:</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  env:
  - 'ACCESS_TOKEN=$ACCESS_TOKEN'
  - 'BRANCH=$BRANCH'
  - 'UTIL_BRANCH=$UTIL_BRANCH'
  - 'ARG_ENVIRONMENT=$ARG_ENVIRONMENT'
  - 'ARG_PYTHON_SCRIPT=$ARG_PYTHON_SCRIPT'
  args:
  - build
  - &quot;--tag=gcr.io/$PROJECT_ID/quickstart-image&quot;
  - &quot;--file=./twitter/dax/processing_scripts/Dockerfile&quot;
  - .
</code></pre>
<p>When the trigger runs the build, I get an error in one of the build steps in dockerfile saying that the variable is not available. It's clear that the environment variable passed in the yaml file is not being passed to the Dockerfile for consumption.
And this is how the I have filled the substitution variables in on the trigger page
<a href=""https://i.stack.imgur.com/Q5w8B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q5w8B.png"" alt=""enter image description here"" /></a></p>
<p>Pasting the build code from step 5 where the error occurs, before which is just apt-get update commands running:</p>
<pre><code>Step 5/28 : RUN git config --global url.&quot;https://${ACCESS_TOKEN}:@github.com/&quot;.insteadOf &quot;https://github.com/&quot; &amp;&amp; echo $(ACCESS_TOKEN)
 ---&gt; Running in f7b94bc2a0d9

/bin/sh: 1: ACCESS_TOKEN: not found
Removing intermediate container f7b94bc2a0d9
 ---&gt; 30965207dcec
Step 6/28 : ARG BRANCH
 ---&gt; Running in 93e36589ac48
Removing intermediate container 93e36589ac48
 ---&gt; 1d1508b1c1d9
Step 7/28 : RUN git clone https://github.com/my_repo45/twitter.git -b &quot;${BRANCH}&quot;
 ---&gt; Running in fbeb93dbb113
Cloning into 'twitter'...
remote: Repository not found.
fatal: Authentication failed for 'https://github.com/my_repo45/twitter.git/'
The command '/bin/sh -c git clone https://github.com/my_repo45/twitter.git -b &quot;${BRANCH}&quot;' returned a non-zero code: 128
ERROR
ERROR: build step 0 &quot;gcr.io/cloud-builders/docker&quot; failed: step exited with non-zero status: 128```

Could anyone please point out what the issue and validate if the environment declaration is proper?
</code></pre>","<p>You would still need to add the variables to the <code>docker build</code> command. As in your current situation, the substitution variables are only available for you within cloud build and not as a regular environment variable. A simplified example of how to pass those variables to cloud build is outlined below. Note that you can only use this with <code>ARG</code> in your dockerfile, not with <code>ENV</code>.</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  env:
    - 'ACCESS_TOKEN=$_ACCESS_TOKEN'
  args:
    - build
    - &quot;--build-arg=ACCESS_TOKEN=${ACCESS_TOKEN}&quot;
    - &quot;--tag=gcr.io/$PROJECT_ID/quickstart-image&quot;
    - &quot;--file=./twitter/dax/processing_scripts/Dockerfile&quot;
</code></pre>
<p>Another option is to export the environment variable within the same build step your <code>docker build</code> command is in:</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  env:
    - 'ACCESS_TOKEN=$_ACCESS_TOKEN'
  args:
  - '-c'
  - |
    export ACCESS_TOKEN=${ACCESS_TOKEN}
    docker build \
      --tag=gcr.io/$PROJECT_ID/quickstart-image&quot; \
      --file=./twitter/dax/processing_scripts/Dockerfile&quot;
</code></pre>
<p>Of course, you could argue if the <code>env</code> field is still needed in this setup, I'll leave that up to you.</p>"
"How to access a file from Docker image from the host using Google Clould Build<p>I would like to move node_modules.zip which is inside the Docker image to the static folder found within the directory.</p>
<p>Currently, my cloudbuild.yaml looks as follows:</p>
<pre><code>  - id: package_js
    name: gcr.io/numom-57642/test4:latest
    entrypoint: /bin/bash
    waitFor: ['-']
    args:
      - -c
      - |
        mv node_modules.zip static/
</code></pre>
<p>When I run cloud-build-local I get an error:</p>
<pre><code>Step #1 - &quot;package_js&quot;: mv: cannot stat 'node_modules.zip': No such file or directory
</code></pre>
<p>How can I access node_modules.zip which is part of the Docker Image.</p>","<p>Hmmm.... interesting.</p>
<p>I think you're on the right path.</p>
<p>You should be able to copy the file into the default mounted <code>/workspace</code> or a newly mounted volume:</p>
<pre><code>- id: package_js
  name: gcr.io/numom-57642/test4:latest
  entrypoint: /bin/bash
  volumes:
  - name: 'scratch'
    path: '/scratch'
  args:
  - -c
  - |
    cp node_modules.zip /workspace
    cp node_modules.zip /scratch
</code></pre>
<p>And then:</p>
<pre><code>- id: check
  name: busybox
  volumes:
  - name: 'scratch'
    path: '/scratch'
  args:
  - |
    ls -l /workspace
    ls -l /scratch
</code></pre>
<p>Using <code>/workspace</code> is easier as you needn't mount volumes but using <code>/scratch</code> is more explicit.</p>"
"Google Cloud Build fetch Identity token<p>in my scenario, I would like to trigger an Google Cloud Function based on HTTP endpoint during a Google Cloud Build. The HTTP request is done using a step with a python:3.7-slim container.</p>
<p>Based on <a href=""https://cloud.google.com/functions/docs/securing/authenticating?hl=de#functions-bearer-token-example-python"" rel=""nofollow noreferrer"">this</a> and <a href=""https://cloud.google.com/run/docs/authenticating/service-to-service#console-ui"" rel=""nofollow noreferrer"">this</a> examples from the documentation, I wanted to use the following code:</p>
<pre><code>REGION = 'us-central1'
PROJECT_ID = 'name-of-project'
RECEIVING_FUNCTION = 'my-cloud-function'

function_url = f'https://{REGION}-{PROJECT_ID}.cloudfunctions.net/{RECEIVING_FUNCTION}'

metadata_server_url = 'http://metadata/computeMetadata/v1/instance/service-accounts/default/identity?audience='
token_full_url = metadata_server_url + function_url
token_headers = {'Metadata-Flavor': 'Google'}

token_response = requests.get(token_full_url, headers=token_headers)
jwt = token_response.text
print(jwt)

r = requests.post(url=function_url, headers=function_headers, json=payload)
</code></pre>
<p>Surprisingly, the code fails because <code>jwt</code> is <code>Not Found</code> (according to the <code>print</code> statement).
I already tested the code and IAM settings by hard coding a valid identity token and also tested the exact same fetching mechanism on a test VM inside the same project.
The problem seems to be that the meta data fetching some is not working inside cloud build.</p>
<p>Am I missing something?
Thank you for any help!</p>","<p>The solution is to use a <a href=""https://cloud.google.com/iam/docs/reference/credentials/rest/v1/projects.serviceAccounts/generateIdToken"" rel=""noreferrer"">new IAM api to generate an ID_TOKEN</a>, on a service account with an access token, if the requester (this one who generate the access token) has the role Service Account Token Creator on the service account (or widely on the project).</p>
<p>This first example use direct API calls</p>
<pre><code> - name: gcr.io/cloud-builders/gcloud
   entrypoint: &quot;bash&quot;
   args:
    - &quot;-c&quot;
    - |
        curl -X POST -H &quot;content-type: application/json&quot; \
        -H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \
        -d '{&quot;audience&quot;: &quot;YOUR AUDIENCE&quot;}' \
         &quot;https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/YOUR SERVICE ACCOUNT:generateIdToken&quot;
        # Use Cloud Build Service Account
        # service_account_email=$(gcloud config get-value account) 

</code></pre>
<p>And here the Python code version</p>
<pre><code>- name: python:3.7
          entrypoint: &quot;bash&quot;
          args:
            - &quot;-c&quot;
            - |
                    pip3 install google-auth requests
                    python3 extract-token.py
</code></pre>
<p>And <code>extract-token.py</code> content the following code</p>
<pre><code>REGION = 'us-central1'
PROJECT_ID = 'name-of-project'
RECEIVING_FUNCTION = 'my-cloud-function'
function_url = f'https://{REGION}-{PROJECT_ID}.cloudfunctions.net/{RECEIVING_FUNCTION}'

import google.auth
credentials, project_id = google.auth.default(scopes='https://www.googleapis.com/auth/cloud-platform')

# To use the Cloud Build service account email
service_account_email = credentials.service_account_email
#service_account_email = &quot;YOUR OWN SERVICE ACCOUNT&quot;

metadata_server_url = f'https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/{service_account_email}:generateIdToken'
token_headers = {'content-type': 'application/json'}

from google.auth.transport.requests import AuthorizedSession
authed_session = AuthorizedSession(credentials)
import json
body = json.dumps({'audience': function_url})

token_response = authed_session.request('POST',metadata_server_url, data=body, headers=token_headers)
jwt = token_response.json()
print(jwt['token'])
</code></pre>
<p>Don't hesitate if you need more details.</p>
<p><em>I think I will write an article on this on Medium, if you want I name you, let me know</em></p>"
"Cloud build pytest Can not find Ghostscript library<p>I'm trying to create a trigger that test a function before deploying it in <code>cloud function</code>. So far I managed to install <code>requirements.txt</code> and execute <code>pytest</code> but I get the following error:</p>
<pre><code>/usr/local/lib/python3.7/site-packages/ghostscript/__init__.py:35: in &lt;module&gt;
    from . import _gsprint as gs
/usr/local/lib/python3.7/site-packages/ghostscript/_gsprint.py:515: in &lt;module&gt;
    raise RuntimeError('Can not find Ghostscript library (libgs)')
E   RuntimeError: Can not find Ghostscript library (libgs)
</code></pre>
<p>I have <code>ghostscript</code> in my <code>requirements.txt</code> file :</p>
<pre><code>[...]
ghostscript==0.6
[...]
pytest==6.0.1
pytest-mock==3.3.1
</code></pre>
<p>Here is my <code>deploy.yaml</code></p>
<pre><code>steps:
  - name: 'docker.io/library/python:3.7'
    id: Test
    entrypoint: /bin/sh
    dir: 'My_Project/'
    args:
    - -c
    - 'pip install -r requirements.txt &amp;&amp; pytest pytest/test_mainpytest.py -v'
</code></pre>
<p>From the traceback, I understand that I don't have <code>ghostscript</code> installed on the cloud build, which is true.</p>
<p>Is there a way to install <code>ghostscript</code> on a step of my <code>deploy.yaml</code>?</p>
<p>Edit-1:</p>
<p>So I tried to install <code>ghostscript</code> using commands in a step, I tried <code>apt-get gs</code>, <code>apt-get ghostscript</code> but unfortunately it didn't work</p>","<p>The real problem is that you are missing a c-library, the package itself seems installed by pip. You should install that library with your package manager. This is an example for ubuntu-based containers:</p>
<pre><code>  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        apt update
        apt install ghostscript -y
        pip install -r requirements.txt
        pytest pytest/test_mainpytest.py -v
</code></pre>"
"Cloud build can't open requirements.txt<p>I want to setup a cloud build trigger so that each time I modify (commit and push) <code>main.py</code>, it execute <code>test_mainpytest.py</code> with <code>pytest</code></p>
<p>I have a project that look like this :</p>
<pre><code>My_Project\function_one\
                        main.py
                        deploy.yaml
                        requirements.txt
                        dir_pytest\
                                   test_mainpytest.py
</code></pre>
<p>My <code>deploy.yaml</code> contain thoose steps :</p>
<pre><code>steps:
  - name: 'python'
    args: ['pip3', 'install', '-r', 'My_Project/function_one/requirements.txt', '--user']
  - name: 'python'
    args: ['python3', 'pytest', 'My_Project/function_one/dir_pytest/']
</code></pre>
<p>For the moment I just want to try to execute <code>pytest</code> using the trigger. When I execute the cloud build trigger, I get this error :</p>
<p><code>ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'My_Project/function_one/requirements.txt'</code></p>
<p>Also my project is saved in a google cloud repository.</p>
<p>Edit :</p>
<p>I tried to add <code>dir</code> in my step, so it currently look like this :</p>
<pre><code>steps:
  - name: 'python'
    dir: 'MyProject/function_one/'
    args: ['pip3', 'install', '-r', 'My_Project/function_one/requirements.txt', '--user']
  - name: 'python'
    dir: 'MyProject/function_one/'
    args: ['python3', 'pytest', 'My_Project/function_one/dir_pytest/']
</code></pre>
<p>Yet I still get the error, (I also tried to put <code>dir</code> after <code>args</code> but it didn't change much</p>
<p>I also noticed; when executing the trigger in Cloud Build; thoose 2 lines :</p>
<pre><code>Initialized empty Git repository in /workspace/.git/
From https://source.developers.google.com/p/my_id_1234/r/My_Project
</code></pre>
<p>Should I use <code>https://source.developers.google.com/p/my_id_1234/r/My_Project</code> and add the path to my <code>requirement.txt</code> and my <code>py_test</code> directory ?</p>","<p>Could you show your whole cloudbuild.yaml? If you are using a build trigger, the repository is imported directly in /workspace. If you are doing a git clone, then your repository is inside a directory with the name of the repository. The difference is:</p>
<pre><code>/workspace/my-repository/My_Project/function_one/requirements.txt
</code></pre>
<p>versus</p>
<pre><code>/workspace/My_Project/function_one/requirements.txt
</code></pre>
<p>If nothing else works, you can do <code>ls -R</code> to show you the directory structure within the build. Add this as a first build step:</p>
<pre><code> - name: 'list recursively'
   args: ['ls', '-R']
</code></pre>"
"Keras checkpoints not being saved to google cloud bucket<p>I'm using the following code to save checkpoints while a google cloud build runs my model:</p>
<pre><code> cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = &quot;gs://mybucket/checkpoints&quot;, 
                                                   verbose=0,
                                                   save_weights_only=True,
                                                   monitor='val_loss',
                                                   mode='min',
                                                   save_best_only=True)
</code></pre>
<p>I'm getting no errors in my build logs, but the only thing in the bucket after each run is a tf_cloud_train_tar file containing the source directory contents.</p>
<p>I'm using callbacks = [cp_callback] in model.fit.</p>","<p>I was having this problem for several reasons:</p>
<ul>
<li>Dataset was not on the storage bucket, and so the code had no access to it.</li>
<li>Use of generator for dataset without files creates an infinite loop, but no crash.</li>
</ul>
<p>I switched to AI Platform and sourced my data from the GCS Bucket and the problem was fixed.</p>"
"How to reject checking in if tests fail in Google Cloud Build<p>I've created a Docker file and configured it as the trigger to my Google Cloud Source Repository.
There are only a few options available to I chose &quot;Push to a branch&quot;.
Right now my docker image can do a new Cloud Function (written with golang) deployment upon a new push.
I have added a go test step in my Docker file.
I want to reject the commit if it will cause the go test failed, like GitLab.
If the go test fails, the cloud function won't be updated. But the bad code will stay there.
How to implement this &quot;reject failure code&quot; feature to Google Cloud Source Repository?</p>","<p>This is not a complete or sufficient answer, but it's too long for a comment and the snippet needs formatting.</p>
<p>I suspect you have a build file you can edit somewhere called <code>cloudbuild.yaml</code>, in which you can add in a test step.</p>
<p>We use Github for our repos with the GCP plugin. Though different from Google's source repository, we typically control this by adding a step in our <code>cloudbuild.yaml</code> file, e.g.</p>
<pre><code># build and run the test suite
  - name: 'python:3.8-slim' # add a go container here, see below
    id: 'Run Unit Tests'
    entrypoint: '/bin/bash'
    args: 
      - &quot;-c&quot;
      - &quot;\
      whattevercommandsyouwant &amp;&amp; \
      morecommands&quot;
</code></pre>
<p>That's not complete but hopefully it helps you solve the issue. And for your reference, the <a href=""https://cloud.google.com/cloud-build/docs/building/build-go"" rel=""nofollow noreferrer"">Google Cloud Build Go docs</a>.</p>"
"How to start a GCP VM in a pipeline?<p>I am kinda having this weird not meant to use for case with GCP. So there is a few things I need to do with Google Cloud Platform. We use an much stronger than at the office Ubuntu VM to build a yocto build. I can somehow not figure out what the proper .yaml is to turn on a VM in google cloud. The pipeline should run from bitbucket and is supposed to the following things</p>
<p>(pseudo code)</p>
<pre><code>start up the vm in gcp &amp;&amp; ssh builder@server 
cd  ./repo/build
start build &amp;&amp; push build image to repo server
push logs to pipeline
shutdown
</code></pre>
<p>I am aware of google cloud build but we have some dependencies that would likely make this more or less inefficient, now I have a general idea how my yaml is suppose to look like but I could use some better pointers in this. As in I am sure this is wrong.</p>
<pre><code>steps:
  - name: 'gcloud compute instances start build-server-turnoff-when-unused' 

  - name: buildstep
    script: /bin/bash build.sh

  - name: 'send logs'
    script: /bin/bash sendlogs.sh
    

  - name: gcloud compute instances stop build-server-turnoff-when-unused'
</code></pre>
<p>I was wondering if someone has done something like this before and could help me out?</p>","<p>I had a bit of a wrong understanding how pipelines in bitbucket exactly work, I figured out I could just download the google-cloud sdk and give commands through the pipeline.</p>
<pre><code>image: google/cloud-sdk:latest 

pipelines:
  default:
    - step:
        name: &quot;Start build instance&quot;
        script:
          - echo ${GCLOUD_JSON_KEY} &gt; client-secret.json
          - gcloud auth activate-service-account --key-file client-secret.json
          - gcloud config set project $project-name
          - 'gcloud compute instances start build-serve --zone=america-west4-b'
</code></pre>"
"Cloudbuild trigger that builds when tag is pushed is not invoked automatically<p>I've created a cloudbuild trigger that should fire a when a tag matching the regex is pushed to the repository.</p>
<p>The regex is <code>^production/widget_(.+)$</code> and the UI is showing that multiple tags are matched. When pushing a new tag that matches the regex, I would expect it the cloudbuild trigger to start a new build.</p>
<p>The tags are formatted like this for instance: <code>production/widget_1.0_1745 </code></p>
<p><a href=""https://i.stack.imgur.com/63CCU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/63CCU.png"" alt=""enter image description here"" /></a></p>","<p>Still not sure what exactly the problem was but after I changed the path structure to <code>production/widget/1.0.1</code> and adjusted the regex accordingly, cloudbuild is picking up newly pushed tags</p>
<h2>Update</h2>
<p>After some time I figured out that the push tag invocation can get prevented by file filters if these don't allow a build to trigger.</p>"
"Why does redirecting stderr make my App Engine deployment failing consistently?<p>I am deploying a trivial App Engine Standard Environment app. (Literally the shortest possible, a Python 3 &quot;hello world&quot;.)  I am using Macbook with zshell.</p>
<p>If I redirect standard error to file, I get an error (below) every time.</p>
<pre><code>  gcloud app deploy -q  2&gt;&gt;err.log 
</code></pre>
<p>If I omit the redirection, it succeeds every time.</p>
<p>There is no difference between using <code>&gt;</code> or <code>&gt;&gt;</code>. Redirecting with a pipe, e.g. to <code>grep</code>, does not cause the problem.</p>
<p>So this is a &quot;solution&quot; (by sending output through a passthrough grep) that does what I need and does not trigger the problem, but this is very roundabout.</p>
<pre><code>  gcloud app deploy -q 2&gt;&amp;1 &gt;/dev/null |egrep &quot;.&quot; &gt;&gt; err.txt 
</code></pre>
<p>Note that I use <code>-q</code>, so waiting for my <code>Y</code> for approval is not the issue.</p>
<p>The error is this. (Identifiers were anonymized.)</p>
<pre><code>..................failed.
ERROR: (gcloud.app.deploy) Error Response: [9] Cloud build BUILD_ID status: FAILURE
Build error details: Failed to download at least one file. Cannot continue.

Full build logs: https://console.cloud.google.com/cloud-build/builds/BUILD_ID?project=PROJECT_ID
</code></pre>
<p>Looking at the logs, I see this.</p>
<pre><code>starting build &quot;BUILD_ID&quot;
FETCHSOURCE
BUILD
Starting Step #0 - &quot;fetcher&quot;
Step #0 - &quot;fetcher&quot;: Already have image (with digest): gcr.io/cloud-builders/gcs-fetcher
Step #0 - &quot;fetcher&quot;: Fetching manifest gs://staging.joshua-playground.appspot.com/ae/BUILD_ID/manifest.json.
Step #0 - &quot;fetcher&quot;: Processing 728 files.
Step #0 - &quot;fetcher&quot;: Failed to fetch gs://staging.my-project.appspot.com/BUILD_ID, will no longer retry: fetching &quot;gs://staging.my-project.appspot.com/BUILD_ID&quot; with timeout 1h0m0s to temp file &quot;/workspace/.download/staging.joshua-playground.appspot.com-BUILD_ID&quot;: err SHA mismatch, got &quot;SHA_VALUE&quot;, want &quot;SHA_VALUE&quot;
Step #0 - &quot;fetcher&quot;: Failed to download at least one file. Cannot continue.
Finished Step #0 - &quot;fetcher&quot;
ERROR
ERROR: build step 0 &quot;gcr.io/cloud-builders/gcs-fetcher&quot; failed: step exited with non-zero status: 1
</code></pre>","<p>Google has confirmed this issue. Please track it <a href=""https://issuetracker.google.com/issues/175803946"" rel=""nofollow noreferrer"">here</a>.</p>"
"How can I build a Docker Image in Google Cloud Build and use in later Build Steps?<p>In my Rails project, I have a Docker Image in a repo which is used for DB migration and unit tests.  Prior to running migrations/testing, I may need to update gems on the Image.  However, it seems that even after updating Gems, the updated image (which is not pushed to the repo, but which is in a build step just prior to migration/testing) is not available to future build steps.</p>
<p>My cloudbuild.yaml looks like this:</p>
<pre><code>steps:
  - id: update_gems
    name: 'gcr.io/cloud-builders/docker'
    args: [ 'build', '-t', &quot;us-central1-docker.pkg.dev/$PROJECT_ID/myregistry/myimage:deploy&quot;,
            '--build-arg', 'PROJECT=${PROJECT_ID}', '-f', 'docker/bundled.Dockerfile', '.' ]
  - id: db_migration
    name: &quot;gcr.io/google-appengine/exec-wrapper&quot;
    args: [&quot;-i&quot;, &quot;us-central1-docker.pkg.dev/$PROJECT_ID/myregistry/myimage:deploy&quot;,
           &quot;-e&quot;, &quot;RAILS_ENV=${_RAILS_ENV}&quot;,
           &quot;-e&quot;, &quot;INSTANCE_CONNECTION_NAME=${_INSTANCE_CONNECTION_NAME}&quot;,
           &quot;-s&quot;, &quot;${_INSTANCE_CONNECTION_NAME}&quot;,
           &quot;--&quot;, &quot;./bin/rake&quot;, &quot;db:migrate&quot;]
  - id: unit_test
    name: &quot;gcr.io/google-appengine/exec-wrapper&quot;
    args: [&quot;-i&quot;, &quot;us-central1-docker.pkg.dev/$PROJECT_ID/myregistry/myimage:deploy&quot;,
           &quot;-e&quot;, &quot;RAILS_ENV=test&quot;, 
           &quot;-e&quot;, &quot;INSTANCE_CONNECTION_NAME=${_INSTANCE_CONNECTION_NAME}&quot;,
           &quot;-s&quot;, &quot;${_INSTANCE_CONNECTION_NAME}&quot;,
           &quot;--&quot;, &quot;./bin/rspec&quot;]
  - id: deploy_to_GAE
    name: gcr.io/cloud-builders/gcloud
    args: ['app', 'deploy', '--project', '${PROJECT_ID}', 'app.yaml']
</code></pre>
<p>The Dockerfile referred to in the 1st step looks like this:</p>
<pre><code>ARG PROJECT
FROM us-central1-docker.pkg.dev/${PROJECT}/myregistry/myimage:deploy
WORKDIR /workspace
ADD Gemfile* ./
RUN bundle update
RUN bundle install

</code></pre>
<p>During a triggered Cloud Build, I see it update Gems and create a new hash like so:
<a href=""https://i.stack.imgur.com/muxZh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/muxZh.png"" alt=""updated gems"" /></a></p>
<p>And then during the <code>db_migration</code> step, I see it pulling the old image before the Gems were updated:
<a href=""https://i.stack.imgur.com/BCKiE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BCKiE.png"" alt=""db migration with old image"" /></a></p>
<p>This can be verified in the <code>update_gems</code> step logs where the pre-updated image hash matches (ie the image hash which is freshly pulled, but not yet had its Gems updated):
<a href=""https://i.stack.imgur.com/6KqUP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6KqUP.png"" alt=""pre-gem update"" /></a></p>
<p>I realize a work-around is to push the updated image after building it, which does in fact work.  For example, I could add this step after <code>update_gems</code> step:</p>
<pre><code>  - id: update_image
    name: 'gcr.io/cloud-builders/docker'
    args: [ 'push',  'us-central1-docker.pkg.dev/$PROJECT_ID/myregistry/myimage:deploy' ]
</code></pre>
<p>However, it begs the question why the new <code>udate_image</code> build step has access to the image built by the <code>update_gems</code> step while other future steps don't.</p>","<p>The image are stored locally, in a local docker registry, that Docker can access. That's why you can push it with Docker.</p>
<p>But, when you use another step, such as <code>gcr.io/google-appengine/exec-wrapper</code>, Docker is no longer loaded in the runtime context and thus the local docker registry is unknown/not active.</p>
<p>So, the solution is:</p>
<ul>
<li>Either to push externally the image and then use it. Like this, it's not a local registry but an external registry which is used, and it works in any steps.</li>
<li>Or install docker on your current runtime step image (or use Docker as step image and install what you need on this image) -&gt; it will be difficult, I don't recommend this way.</li>
</ul>"
"Cloud build not working for different environment angular<p>Below command, I am trying to run with cloud build trigger but it still picks up the enviroment.ts file</p>
<p>cloudbuild.yaml</p>
<pre><code>- name: gcr.io/cloud-builders/npm
  args: [ run, build, --configuration=Staging ]
</code></pre>
<p>Below is the Package.json script code</p>
<pre><code>  &quot;scripts&quot;: {
    &quot;ng&quot;: &quot;ng&quot;,
    &quot;start&quot;: &quot;ng serve&quot;,
    &quot;build&quot;: &quot;ng build&quot;,
    &quot;test&quot;: &quot;ng test&quot;,
    &quot;lint&quot;: &quot;ng lint&quot;,
    &quot;e2e&quot;: &quot;ng e2e&quot;}
</code></pre>
<p>File replacement code from angular.json</p>
<pre><code>{
  &quot;$schema&quot;: &quot;./node_modules/@angular/cli/lib/config/schema.json&quot;,
  &quot;version&quot;: 1,
  &quot;newProjectRoot&quot;: &quot;projects&quot;,
  &quot;projects&quot;: {
    &quot;projectname&quot;: {
      &quot;root&quot;: &quot;&quot;,
      &quot;sourceRoot&quot;: &quot;src&quot;,
      &quot;projectType&quot;: &quot;application&quot;,
      &quot;prefix&quot;: &quot;app&quot;,
      &quot;schematics&quot;: {
        &quot;@schematics/angular:component&quot;: {
          &quot;styleext&quot;: &quot;scss&quot;
        }
      },
      &quot;architect&quot;: {
        &quot;build&quot;: {
          &quot;builder&quot;: &quot;@angular-devkit/build-angular:browser&quot;,
          &quot;options&quot;: {
            &quot;outputPath&quot;: &quot;dist/projectname&quot;,
            &quot;index&quot;: &quot;src/index.html&quot;,
            &quot;main&quot;: &quot;src/main.ts&quot;,
            &quot;polyfills&quot;: &quot;src/polyfills.ts&quot;,
            &quot;tsConfig&quot;: &quot;src/tsconfig.app.json&quot;,
            &quot;assets&quot;: [
              &quot;src/favicon.ico&quot;,
              &quot;src/assets&quot;
            ],
            &quot;styles&quot;: [
              &quot;node_modules/bootstrap/dist/css/bootstrap.min.css&quot;,
              &quot;src/styles.scss&quot;,
              &quot;src/assets/css/style.css&quot;
            ],
            &quot;scripts&quot;: [
              &quot;src/assets/js/jquery.min.js&quot;,
              &quot;node_modules/bootstrap/dist/js/bootstrap.min.js&quot;,
              &quot;node_modules/chart.js/dist/Chart.bundle.min.js&quot;
            ]
          },
          &quot;configurations&quot;: {
            &quot;production&quot;: {
              &quot;fileReplacements&quot;: [
                {
                  &quot;replace&quot;: &quot;src/environments/environment.ts&quot;,
                  &quot;with&quot;: &quot;src/environments/environment.prod.ts&quot;
                }
              ],
              &quot;optimization&quot;: true,
              &quot;outputHashing&quot;: &quot;all&quot;,
              &quot;sourceMap&quot;: false,
              &quot;extractCss&quot;: true,
              &quot;namedChunks&quot;: false,
              &quot;aot&quot;: true,
              &quot;extractLicenses&quot;: true,
              &quot;vendorChunk&quot;: false,
              &quot;buildOptimizer&quot;: true,
              &quot;budgets&quot;: [
                {
                  &quot;type&quot;: &quot;initial&quot;,
                  &quot;maximumWarning&quot;: &quot;5mb&quot;,
                  &quot;maximumError&quot;: &quot;8mb&quot;
                }
              ]
            },
            &quot;staging&quot;: {
              &quot;fileReplacements&quot;: [
                {
                  &quot;replace&quot;: &quot;src/environments/environment.ts&quot;,
                  &quot;with&quot;: &quot;src/environments/environment.staging.ts&quot;
                }
              ],
              &quot;optimization&quot;: true,
              &quot;outputHashing&quot;: &quot;all&quot;,
              &quot;sourceMap&quot;: false,
              &quot;extractCss&quot;: true,
              &quot;namedChunks&quot;: false,
              &quot;aot&quot;: true,
              &quot;extractLicenses&quot;: true,
              &quot;vendorChunk&quot;: false,
              &quot;buildOptimizer&quot;: true,
              &quot;budgets&quot;: [
                {
                  &quot;type&quot;: &quot;initial&quot;,
                  &quot;maximumWarning&quot;: &quot;5mb&quot;,
                  &quot;maximumError&quot;: &quot;8mb&quot;
                }
              ]
            },
            &quot;dev&quot;: {
              &quot;fileReplacements&quot;: [
                {
                  &quot;replace&quot;: &quot;src/environments/environment.ts&quot;,
                  &quot;with&quot;: &quot;src/environments/environment.dev.ts&quot;
                }
              ],
              &quot;optimization&quot;: true,
              &quot;outputHashing&quot;: &quot;all&quot;,
              &quot;sourceMap&quot;: false,
              &quot;extractCss&quot;: true,
              &quot;namedChunks&quot;: false,
              &quot;aot&quot;: true,
              &quot;extractLicenses&quot;: true,
              &quot;vendorChunk&quot;: false,
              &quot;buildOptimizer&quot;: true,
              &quot;budgets&quot;: [
                {
                  &quot;type&quot;: &quot;initial&quot;,
                  &quot;maximumWarning&quot;: &quot;5mb&quot;,
                  &quot;maximumError&quot;: &quot;8mb&quot;
                }
              ]
            }
          }
        },
        &quot;serve&quot;: {
          &quot;builder&quot;: &quot;@angular-devkit/build-angular:dev-server&quot;,
          &quot;options&quot;: {
            &quot;browserTarget&quot;: &quot;projectname:build&quot;
          },
          &quot;configurations&quot;: {
            &quot;production&quot;: {
              &quot;browserTarget&quot;: &quot;projectname:build:production&quot;
            }
          }
        },
        &quot;extract-i18n&quot;: {
          &quot;builder&quot;: &quot;@angular-devkit/build-angular:extract-i18n&quot;,
          &quot;options&quot;: {
            &quot;browserTarget&quot;: &quot;projectname:build&quot;
          }
        },
        &quot;test&quot;: {
          &quot;builder&quot;: &quot;@angular-devkit/build-angular:karma&quot;,
          &quot;options&quot;: {
            &quot;main&quot;: &quot;src/test.ts&quot;,
            &quot;polyfills&quot;: &quot;src/polyfills.ts&quot;,
            &quot;tsConfig&quot;: &quot;src/tsconfig.spec.json&quot;,
            &quot;karmaConfig&quot;: &quot;src/karma.conf.js&quot;,
            &quot;styles&quot;: [
              &quot;src/styles.scss&quot;
            ],
            &quot;scripts&quot;: [],
            &quot;assets&quot;: [
              &quot;src/favicon.ico&quot;,
              &quot;src/assets&quot;
            ]
          }
        },
        &quot;lint&quot;: {
          &quot;builder&quot;: &quot;@angular-devkit/build-angular:tslint&quot;,
          &quot;options&quot;: {
            &quot;tsConfig&quot;: [
              &quot;src/tsconfig.app.json&quot;,
              &quot;src/tsconfig.spec.json&quot;
            ],
            &quot;exclude&quot;: [
              &quot;**/node_modules/**&quot;
            ]
          }
        }
      }
    },
    &quot;projectname-e2e&quot;: {
      &quot;root&quot;: &quot;e2e/&quot;,
      &quot;projectType&quot;: &quot;application&quot;,
      &quot;prefix&quot;: &quot;&quot;,
      &quot;architect&quot;: {
        &quot;e2e&quot;: {
          &quot;builder&quot;: &quot;@angular-devkit/build-angular:protractor&quot;,
          &quot;options&quot;: {
            &quot;protractorConfig&quot;: &quot;e2e/protractor.conf.js&quot;,
            &quot;devServerTarget&quot;: &quot;projectname:serve&quot;
          },
          &quot;configurations&quot;: {
            &quot;production&quot;: {
              &quot;devServerTarget&quot;: &quot;projectname:serve:production&quot;
            }
          }
        },
        &quot;lint&quot;: {
          &quot;builder&quot;: &quot;@angular-devkit/build-angular:tslint&quot;,
          &quot;options&quot;: {
            &quot;tsConfig&quot;: &quot;e2e/tsconfig.e2e.json&quot;,
            &quot;exclude&quot;: [
              &quot;**/node_modules/**&quot;
            ]
          }
        }
      }
    }
  },
  &quot;defaultProject&quot;: &quot;projectname&quot;,
  &quot;cli&quot;: {
    &quot;analytics&quot;: &quot;51d20bdf-c925-4ff9-b30c-d7fe27412dc1&quot;
  }
}
</code></pre>
<p>Can you please help me where I am getting this wrong? I tried it with --prod as well but does not work, it always takes the evironment.ts file. However, it works fine when I do it locally. I see cloud build has the substitution variable concept not sure the best way of using it.</p>
<p><strong>EDIT</strong></p>
<p>Even If I have the below-staging command in cloud build it just takes the <em>ng build</em> always. refer to the image.
I do not see any errors everything is successful.</p>
<pre><code>- name: gcr.io/cloud-builders/npm
  args: [ run, build, --configuration=staging ]
</code></pre>
<p><a href=""https://i.stack.imgur.com/ME4p3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ME4p3.png"" alt=""enter image description here"" /></a></p>","<p>Change cloudbuild.yaml to below</p>
<pre><code>steps:

- name: gcr.io/cloud-builders/npm
  args: [ install ]


- name: gcr.io/cloud-builders/npm
  args: ['run', 'build','--','--prod']

- name: gcr.io/cloud-builders/gcloud
  args: ['app', 'deploy', '--version=$SHORT_SHA']
</code></pre>
<p>Replace <code>--prod</code> to <code>--configuration=staging</code> if you want it for environment other than prod</p>"
"What is the imageurl in Cloud Build?<p>Here <a href=""https://cloud.google.com/run/docs/building/containers"" rel=""nofollow noreferrer"">https://cloud.google.com/run/docs/building/containers</a> it says</p>
<blockquote>
<p>Run the command: <code>gcloud builds submit --tag IMAGE_URL</code></p>
<p>Replace IMAGE_URL with a reference to the container image, for
example, gcr.io/myproject/my-image:latest.</p>
</blockquote>
<p>What is IMAGE_URL, is it something that I make up or is it something I needed to have created in a previous step?</p>","<p>When you run this command, you ask Cloud Build to build a container based on the Dockerfile of your current environment.</p>
<p>This container needs to be store somewhere. The easiest solution is to use <a href=""https://cloud.google.com/container-registry"" rel=""nofollow noreferrer"">GCR (Google Container Registry)</a>, a managed service where you can store your container image. You have nothing to create or build. You only have <a href=""https://cloud.google.com/container-registry/docs/pushing-and-pulling#tag_the_local_image_with_the_registry_name"" rel=""nofollow noreferrer"">several base URLs according with your location</a>, then, you need to add your projectID and your image name to have this pattern</p>
<pre><code>&lt;optional region.&gt;gcr.io/&lt;ProjectID&gt;/&lt;ImageName&gt;
</code></pre>
<p>If you want to use <a href=""https://cloud.google.com/artifact-registry"" rel=""nofollow noreferrer"">artifact registry</a>, the new container registry of Google Cloud, with more feature and capacity, you have to <a href=""https://cloud.google.com/artifact-registry/docs/manage-repos"" rel=""nofollow noreferrer"">create it</a> before being able to <a href=""https://cloud.google.com/artifact-registry/docs/docker/pushing-and-pulling"" rel=""nofollow noreferrer"">use it</a>. The naming is longer, but allow you to have several repository in the same project, and to set permission on each repository.</p>
<hr />
<p>So, to answer your question</p>
<blockquote>
<p>is it something I needed to have created in a previous step?</p>
</blockquote>
<p>It depends what you want to use!</p>"
"Build times out, can't increase time out<p>I'm deploying to Kubernettes via Cloud Build. Every now and then the build times out because it exceeds the build-in time out of ten minutes. I can't figure out how to increase this time out. I'm using in-line build config in my trigger. It looks like this</p>
<pre><code>    steps:
  - name: gcr.io/cloud-builders/docker
    args:
      - build
      - '-t'
      - '$_IMAGE_NAME:$COMMIT_SHA'
      - .
      - '-f'
      - $_DOCKERFILE_NAME
    dir: $_DOCKERFILE_DIR
    id: Build
  - name: gcr.io/cloud-builders/docker
    args:
      - push
      - '$_IMAGE_NAME:$COMMIT_SHA'
    id: Push
  - name: gcr.io/cloud-builders/gke-deploy
    args:
      - prepare
      - '--filename=$_K8S_YAML_PATH'
      - '--image=$_IMAGE_NAME:$COMMIT_SHA'
      - '--app=$_K8S_APP_NAME'
      - '--version=$COMMIT_SHA'
      - '--namespace=$_K8S_NAMESPACE'
      - '--label=$_K8S_LABELS'
      - '--annotation=$_K8S_ANNOTATIONS,gcb-build-id=$BUILD_ID'
      - '--create-application-cr'
      - &gt;-
        --links=&quot;Build
        details=https://console.cloud.google.com/cloud-build/builds/$BUILD_ID?project=$PROJECT_ID&quot;
      - '--output=output'
    id: Prepare deploy
  - name: gcr.io/cloud-builders/gsutil
    args:
      - '-c'
      - |-
        if [ &quot;$_OUTPUT_BUCKET_PATH&quot; != &quot;&quot; ]
        then
          gsutil cp -r output/suggested gs://$_OUTPUT_BUCKET_PATH/config/$_K8S_APP_NAME/$BUILD_ID/suggested
          gsutil cp -r output/expanded gs://$_OUTPUT_BUCKET_PATH/config/$_K8S_APP_NAME/$BUILD_ID/expanded
        fi
    id: Save configs
    entrypoint: sh
  - name: gcr.io/cloud-builders/gke-deploy
    args:
      - apply
      - '--filename=output/expanded'
      - '--cluster=$_GKE_CLUSTER'
      - '--location=$_GKE_LOCATION'
      - '--namespace=$_K8S_NAMESPACE'
    id: Apply deploy
    timeout: 900s
images:
  - '$_IMAGE_NAME:$COMMIT_SHA'
options:
  substitutionOption: ALLOW_LOOSE
substitutions:
  _K8S_NAMESPACE: default
  _OUTPUT_BUCKET_PATH: xxxxx-xxxxx-xxxxx_cloudbuild/deploy
  _K8S_YAML_PATH: kubernetes/
  _DOCKERFILE_DIR: ''
  _IMAGE_NAME: xxxxxxxxxxx
  _K8S_ANNOTATIONS: gcb-trigger-id=xxxxxxxx-xxxxxxx
  _GKE_CLUSTER: xxxxx
  _K8S_APP_NAME: xxxxx
  _DOCKERFILE_NAME: Dockerfile
  _K8S_LABELS: ''
  _GKE_LOCATION: xxxxxxxx
tags:
  - gcp-cloud-build-deploy
  - $_K8S_APP_NAME
</code></pre>
<p>I've tried sticking the <code>timeout: 900</code> arg in in various places with no luck.</p>","<p>The timeout of 10 minutes is the default for the <a href=""https://cloud.google.com/cloud-build/docs/build-config#timeout_2"" rel=""nofollow noreferrer"">whole build</a>, therefore if you add the <code>timeout: 900s</code> option in any of the <a href=""https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.builds#buildstep"" rel=""nofollow noreferrer"">steps</a>, it will only apply to the step that it has been added to. You can make a step have a larger timeout than the overall build timeout, but the whole build process will fail if the sum of all the steps exceeds the build timeout. This example shows this behavior:</p>
<pre><code>steps:
- name: 'ubuntu'
  args: ['sleep', '600']
  timeout: 800s # Step timeout -&gt; Allows the step to run up to 800s, but as the overall timeout is 600s, it will fail after that time has been passed, so the effective timeout value is 600s.
timeout: 600s # Overall build timeout
</code></pre>
<p>That said, the solution is to expand the overall build timeout by adding it outside of any step, and then you can have a build with up to 24h to finish before it fails with a timeout error.</p>
<p>Something like the following example should work out for you:</p>
<pre><code>steps:
- name: 'ubuntu'
  args: ['sleep', '600']
timeout: 3600s
</code></pre>"
"Is it possible to deploy kustomize command with cloud build on GKE of GCP?<p>If deploy with filename on GKE, <a href=""https://cloud.google.com/cloud-build/docs/cloud-builders#supported_builder_images_provided_by"" rel=""nofollow noreferrer"">Supported builder images provided by Cloud Build</a> can be found from official.</p>
<p>Also can found <a href=""https://github.com/GoogleCloudPlatform/cloud-builders#google-cloud-build-official-builder-images"" rel=""nofollow noreferrer"">full list</a>.</p>
<p>But both <code>gke-deploy</code> or <code>kubectl</code> can't suite my requirement. I want to run a command like</p>
<pre><code>kustomize build ./overlays/production | kubectl apply -f -
</code></pre>
<p>Because I need to build a whole file with kustomize first, then use kubectl.</p>
<p>From <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/run-bash-scripts#running_inline_bash_scripts"" rel=""nofollow noreferrer"">Running inline bash scripts</a>, I can try</p>
<pre><code>steps:
...
- name: gcr.io/cloud-builders/gcloud
  entrypoint: bash
  args:
    - kustomize
    - build
    - ./overlays/production
    - |
    - kubectl
    - apply
    - -f
    - -
</code></pre>
<p>But I think it can't find which cluster on GKE to use. So how to use it in this case with Cloud Build instead of run it on local shell?</p>",<p>One approach would be to install kustomize on the machine that has kubectl installed. Another option is to run kustomize outside the cluster and and then store the resulting yaml somewhere in an object store like S3 in case of AWS and then in the machine download the yaml files from the object store and run kubectl apply.</p>
"CI/CD integration problem when using google-cloud-build with github push as trigger for Cloud Run<p>I am trying to set up a CI/CD pipeline using one of my public GitHub repositories as the source for Cloud Run (fully-managed) service using Cloud Build. I am using a Dockerfile initialized in root folder of the repository with source configuration parameter initialized as <code>/Dockerfile</code> when setting up the cloud build trigger. (to continuously deploy new revisions from source repository)</p>
<p>When, I initialize the cloud run instance, I face the following error:</p>
<p><a href=""https://i.stack.imgur.com/ymaHb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ymaHb.png"" alt=""Error after initializing the instance"" /></a></p>
<p>Moreover, when I try to run my cloud build trigger manually, it shows the following error:</p>
<p><a href=""https://i.stack.imgur.com/wRoRa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wRoRa.png"" alt=""When using run option of cloud build trigger"" /></a></p>
<p>I also tried editing continuous deployment settings by setting it to <strong>automatically detect Dockerfile/cloudbuild.yaml</strong>. After that, build process becomes successful but the revision are not getting updated. I've also tried deploying a new revision and then triggering cloud build trigger but it isn't still able to pick the latest build from container registry.</p>
<p>I am positive that my Dockerfile and application code are working properly since I've previously submitted the build on Container registry using Google Cloud Shell and have tested it manually after deploying it to cloud run.</p>
<p>Need help to fix the issue.</p>",<p>UPPERCASE letters in the image path aren't allowed. Chnage <code>Toxicity-Detector</code> to <code>toxicity-detector</code></p>
"Can I run a Docker container to use in another Google Cloud Build step?<p>I'd like to run a fresh MySQL instance in a Docker Container as a Cloud Build, and then access that MySQL DB in a later step to run Unit Tests against.   Is this possible?  <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/docker#run-a-docker-image"" rel=""nofollow noreferrer"">It appears as if I can run a Docker Container in a build step, but the step doesn't complete until the Container exists</a>.  I'd like this MySQL container to remain running until after the final build step completes.</p>
<p>FWIW I'd like to use this on a Ruby on Rails project to run rspec tests.  I currently use a CloudSQL instance to run tests against, but it's pretty slow, even though the same tests run quickly locally.  Changing the machine-type for the Cloud Builder to something powerful didn't help, so I assume latency is my biggest killer, which is why I want to try a peer Container MySQL instance instead.</p>","<p>It turns out there are at least 2 ways to skin this cat:</p>
<ol>
<li>Use <code>docker-compose</code> cloud builder to spin up multiple containers in 1 step: MySQL and a test runner.   The downside here is the step will never complete unless since MySQL will run in the background and never exit.  I suppose one could write a wrapper to cause it to die after a few minutes.</li>
<li>You can actually start a container with <code>-d</code> in an early build step and ensure it's on the <code>cloudbuild</code> docker network, and then later steps can connect to it if they're also on the <code>cloudbuild</code> network.  Essentially the Mysql step will &quot;complete&quot; quickly as it just starts the server in daemon mode then continues to next build step.  Later, the test runner will run tests against the fresh DB and its build step completes when tests are actually done.</li>
</ol>
<p>I went with option 2, and my 16-min unit tests (run against CloudSQL in same region) shrunk down to 1.5mins using the dockerized MySQL server.</p>"
"Problem with Installing python requirements in GCP cloudbuild<p>I am trying to run a  apache beam pipeline with DirectRunner in cloudbuild and by doing that I need to install the requirements for the python script, but I am facing some errors.<br />
This is part of my cloudbuild.yaml</p>
<pre><code>    steps:
- name: gcr.io/cloud-builders/gcloud
  entrypoint: 'bash'
  args: [ '-c', &quot;gcloud secrets versions access latest --secret=env --format='get(payload.data)' | tr '_-' '/+' | base64 -d &gt; .env&quot; ]
  id: GetSecretEnv
# - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#   entrypoint: 'bash'
#   args: ['-c', 'gcloud config set app/cloud_build_timeout 1600 &amp;&amp; gcloud app deploy --quiet tweepy-to-pubsub/app.yaml']

- name: gcr.io/cloud-builders/gcloud
  id: Access id_github
  entrypoint: 'bash'
  args: [ '-c', 'gcloud secrets versions access latest --secret=id_github&gt; /root/.ssh/id_github' ]
  volumes:
  - name: 'ssh'
    path: /root/.ssh
# Set up git with key and domain
- name: 'gcr.io/cloud-builders/git'
  id: Set up git with key and domain
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    chmod 600 /root/.ssh/id_github
    cat &lt;&lt;EOF &gt;/root/.ssh/config
    Hostname github.com
    IdentityFile /root/.ssh/id_github
    EOF
    ssh-keyscan -t rsa github.com &gt; /root/.ssh/known_hosts
  volumes:
  - name: 'ssh'
    path: /root/.ssh
- name: 'gcr.io/cloud-builders/git'
# Connect to the repository
  id: Connect and clone repository
  dir: workspace
  args:
  - clone
  - --recurse-submodules
  - git@github.com:x/repo.git
  volumes:
  - name: 'ssh'
    path: /root/.ssh

- name: 'gcr.io/$PROJECT_ID/dataflow-python3'
  entrypoint: '/bin/bash'
  args: [ '-c',
          'source /venv/bin/activate' ]
- name: 'gcr.io/$PROJECT_ID/dataflow-python3'  
  entrypoint: '/bin/bash' 
  dir: workspace
  args: ['pip', 'install','-r', '/dir1/dir2/requirements.txt']
- name: 'gcr.io/$PROJECT_ID/dataflow-python3'
  entrypoint: 'python'
  dir: workspace
  args: [ 'dir1/dir2/script.py', 
         '--runner=DirectRunner' ]
timeout: &quot;1600s&quot;
</code></pre>
<p>Without the step where I install the requirements this works but I need the libs, because  I have python error for missing libs, and on the second step (5th actually in the original form of cloud build) the cloud build fails with this error</p>
<pre><code>Step #5: Already have image (with digest): gcr.io/x/dataflow-python3
Step #5: import-im6.q16: unable to open X server `' @ error/import.c/ImportImageCommand/360.
Step #5: import-im6.q16: unable to open X server `' @ error/import.c/ImportImageCommand/360.
Step #5: /usr/local/bin/pip: line 5: from: command not found
Step #5: /usr/local/bin/pip: pip: line 7: syntax error near unexpected token `('
Step #5: /usr/local/bin/pip: pip: line 7: `    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])'
</code></pre>
<p>How do I fix this? I also tried some examples on the internet and it doesn't work</p>
<p>Edit: First I deploy on app engine and then I download the repo in cloud build vm, install requirements and try to run it the python script</p>","<p>I think that the issue comes from your path definition</p>
<pre><code>'source /venv/bin/activate'
</code></pre>
<p>and</p>
<pre><code>'pip', 'install','-r', '/dir1/dir2/requirements.txt'
</code></pre>
<p>You use the full path definition and it doesn't work on Cloud Build. The current working directory is <code>/workspace/</code>. If you use relative path, add simply a dot <code>.</code> before the path, it should works better.</p>
<p>Or not... Indeed, you have the venv activation in a step, and the pip install in the following step. From one step to another, the runtime environment is offloaded and reloaded with the other container. Thus, your <code>source</code> command that set up environment variable, disappear in the <code>pip</code> step.</p>
<hr />
<p>In addition, your cloud build environment is built for the build and destroy then. You don't need to use venv in this case and you can simplify the 3 last steps like this</p>
<pre><code>
- name: 'gcr.io/$PROJECT_ID/dataflow-python3'
  entrypoint: '/bin/bash'
  args:
   - '-c'
   - |
       pip install -r ./dir1/dir2/requirements.txt
       python ./dir1/dir2/script.py --runner=DirectRunner

</code></pre>"
"How to increase gcloud app deploy timeout in 2021<p><a href=""https://stackoverflow.com/questions/60070274/why-cant-i-override-the-timeout-on-my-google-cloud-build"">There</a> <a href=""https://stackoverflow.com/questions/64344278/how-can-i-extend-cloudbuild-timeout"">are</a> <a href=""https://stackoverflow.com/questions/63729971/google-cloud-build-not-using-my-timeout-settings"">many</a> <a href=""https://stackoverflow.com/questions/60431732/gcp-cloud-build-ignores-timeout-settings"">answers</a> to this question already, but they no longer work here in January 2021.  All those answers come in 3 flavors:</p>
<ol>
<li>Set local machine timeout with something like <code>gcloud config set app/cloud_build_timeout 1600</code> then deploy with <code>gcloud app deploy ...</code></li>
<li>Use a <code>cloudbuild.yaml</code> file instead with <code>timeout: 1600s</code> bits in the <code>gcloud app deploy</code> buildstep and the global configuration, then deploy with <code>gcloud builds submit ...</code></li>
<li><a href=""https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-appengine#configuring_the_deployment"" rel=""nofollow noreferrer"">Per google's own docs</a>, don't set <code>timeout: 1600s</code> in the cloudbuild file, but rather do a mashup of the previous 2 flavors with a build step including <code>args: ['-c', 'gcloud config set app/cloud_build_timeout 1600 &amp;&amp; gcloud app deploy']</code></li>
</ol>
<p>None of them have any impact on the <code>app deploy</code> build - it's stuck at 10mins.  When using <code>gcloud builds submit</code>, it results in 2 Cloud Builds being kicked off: one for the cloudbuild.yaml, and one for the app engine deployment using buildpack.  The above solutions can impact the first build, but once that first build kicks off the second build (<code>gcloud app deploy</code>, you can see at <a href=""https://console.cloud.google.com/cloud-build"" rel=""nofollow noreferrer"">https://console.cloud.google.com/cloud-build</a> in the Execution Details tab that the Timeout is still 10m.</p>
<p>IMO, solutions 2-3 are hacks since 1 doesn't work, but now that 2-3 don't work either, I'm looking for another hack.  Does anyone have a solution which works in 2021?  Since my app is using GAE Standard Environment, I can't prebuild an image - I'm stuck with Buildpack building my ruby app and pulling all the Gems every time, and this runs out the seemingly immutable 10m clock.</p>","<p>You <strong>cannot</strong> change the timeout property in App Engine standard and it's always 10min. The workaround is to use App Engine flex and this way you can use the <code>gcloud config set app/cloud_build_timeout TIME_SECONDS</code>.</p>
<p>There is a <a href=""https://issuetracker.google.com/131889237"" rel=""noreferrer"">feature request</a> to enable timeout edit for App Engine standard but seems still in progress.</p>"
"Can I share persistent volumes between builds in Google Cloud Build?<p>My build process is somewhat slow and it would be very handy if I could speed it up by reusing some assets from previous builds.</p>
<p>I've found that one can define <a href=""https://cloud.google.com/cloud-build/docs/build-config#volumes"" rel=""nofollow noreferrer"">volumes</a> to share volumes between steps but is it possible to share folders between <strong>builds</strong>?</p>","<p>If it's layer of your container, you can use <a href=""https://cloud.google.com/cloud-build/docs/kaniko-cache"" rel=""nofollow noreferrer"">Kaniko cache feature</a>. Else, you need to export the content somewhere (on Cloud Storage, it's a great place for this) and then import it on the next Build.</p>
<p>Tips: You can create <a href=""https://cloud.google.com/cloud-build/docs/cloud-builders#writing_your_own_custom_builder"" rel=""nofollow noreferrer"">custom builder</a>. Define yours which saves the assets, and another one which retrieves the assets. Like this, add them simple at the end and the beginning of your pipeline to achieve this easily and in a more reusable manner.</p>"
"URL not found appears when try to deploy with cloud build (gcp)<p>Application - Angular</p>
<p>I am trying to automate the deployment process for my angular app in GCP. When I deploy manually from the cloud shell things works fine but when I try to build-deploy with cloudbuild.yaml, cloud build, triggers it says deployed successfully. When I hit the URL it says 404 not found.</p>
<p>Manual deployment commands</p>
<pre><code>gsutil rsync -r gs://v2-appname.appspot.com ./deploytest
cd deploytest
gcloud app deploy
</code></pre>
<p>I am not much familiar with cloud build.</p>
<p>Possibly, the issue might be in the cloudbuild.yaml file given below.</p>
<pre><code>steps:

      # Install node packages
      - name: &quot;gcr.io/cloud-builders/npm:latest&quot;
        args: [&quot;install&quot;]
    
      # Build production package
      - name: &quot;gcr.io/cloud-builders/npm&quot;
        args: [&quot;build&quot;, &quot;--configuration=staging&quot;]
    
      # Deploy to google cloud app engine
      - name: &quot;gcr.io/cloud-builders/gcloud&quot;
        args: [&quot;app&quot;, &quot;deploy&quot;, &quot;app.yaml&quot;]
</code></pre>
<p>What I understood is when we deploy manually we build and upload files to &quot;dist&quot; folder in storage. then we sync up the directory for deployment and then deploy with gcloud app deploy.</p>
<p>But while doing this with cloud build -
I have GitHub repo that is connected to the trigger any push happens there to some branch it picks up the cloudbuild.yaml file and process. But cloudbuild.yaml does not have any directory where to deploy or sync Is this something I am missing? How to add it? If not please correct me!</p>
<p>Thanks,</p>
<p><strong>EDIT</strong></p>
<pre><code>EA_Website -&gt;
         src/
         cloudbuild.yaml
         app.yaml
         angular.json
         package.json 
</code></pre>
<p>app.yaml</p>
<pre><code>runtime: python27
threadsafe: yes
api_version: 1

# Google App Engine's cache default expiration time is 10 minutes. It's suitable for most Production
# scenarios, but a shorter TTL may be desired for Development and QA, as it allows us to see a fresh
# code in action just a minute after the deployment.
default_expiration: 60s

handlers:

# To enhance security, all http requests are redirected to their equivalent https addresses (secure: always).

# Assets are retrieved directly from their parent folder.
- url: /assets
  static_dir: dist/projectname/assets
  secure: always

# Static files located in the root folder are retrieved directly from there, but their suffixes need to be
# mapped individually in order to avoid them from being hit by the most general (catch-all) rule.
- url: /(.*\.css)
  static_files: dist/projectname/\1
  upload: dist/projectname/(.*\.css)
  secure: always

- url: /(.*\.html)
  static_files: dist/projectname/\1
  upload: dist/projectname/(.*\.html)
  secure: always

- url: /(.*\.ico)
  static_files: dist/projectname/\1
  upload: dist/projectname/(.*\.ico)
  secure: always

- url: /(.*\.js)
  static_files: dist/projectname/\1
  upload: dist/projectname/(.*\.js)
  secure: always

- url: /(.*\.txt)
  static_files: dist/projectname/\1
  upload: dist/projectname/(.*\.txt)
  secure: always

# Site root.
- url: /
  static_files: dist/projectname/index.html
  upload: dist/projectname/index.html
  secure: always

# Catch-all rule, responsible from handling Angular application routes (deeplinks).
- url: /.*
  static_files: dist/projectname/index.html
  upload: dist/projectname/index.html
  secure: always

skip_files:
- ^(?!dist)
</code></pre>
<p>When I update cloudbuild.yaml to below I get below error</p>
<pre><code>steps:

- name: &quot;gcr.io/cloud-builders/npm:node-12.18.3&quot;
  entrypoint: npm
  args: ['install']

- name: gcr.io/cloud-builders/npm
  args: [run, build, --prod]

- name: gcr.io/cloud-builders/gcloud
  args: [ app, deploy, --version=$SHORT_SHA ]


ERROR in ./src/styles.scss (./node_modules/@angular-devkit/build-angular/src/angular-cli-files/plugins/raw-css-loader.js!./node_modules/postcss-loader/src??embedded!./node_modules/sass-loader/lib/loader.js??ref--14-3!./src/styles.scss)
Module build failed (from ./node_modules/sass-loader/lib/loader.js):
Error: Node Sass does not yet support your current environment: Linux 64-bit with Unsupported runtime (83)
For more information on which environments are supported please see:
https://github.com/sass/node-sass/releases/tag/v4.12.0
</code></pre>","<p>Using below does not throw an error but not even build.</p>
<pre><code>  args: [&quot;build&quot;, &quot;--prod&quot;]
</code></pre>
<p>Replacing any of the below works</p>
<pre><code>  args: [&quot;run&quot;, &quot;build&quot;, &quot;--prod&quot;]
</code></pre>
<p>or</p>
<pre><code>  args: [run, build, --prod]
</code></pre>
<p>My final cloudbuild.yaml</p>
<pre><code>steps:

- name: gcr.io/cloud-builders/npm
  args: [ install ]

- name: gcr.io/cloud-builders/npm
  args: [ run, build, --prod]

- name: gcr.io/cloud-builders/gcloud
  args: [ app, deploy, --version=$SHORT_SHA ]
  
</code></pre>
<p>app.yaml</p>
<pre><code>runtime: python27
threadsafe: yes
api_version: 1

# Google App Engine's cache default expiration time is 10 minutes. It's suitable for most Production
# scenarios, but a shorter TTL may be desired for Development and QA, as it allows us to see a fresh
# code in action just a minute after the deployment.
default_expiration: 60s

handlers:
    # Static files located in the root folder are retrieved directly from there, but their suffixes need to be
    # mapped individually in order to avoid them from being hit by the most general (catch-all) rule.
    - url: /(.*\.css)
      static_files: dist/projectname/\1
      upload: dist/projectname/(.*\.css)
      secure: always
    
    - url: /(.*\.html)
      static_files: dist/projectname/\1
      upload: dist/projectname/(.*\.html)
      secure: always
    
    - url: /(.*\.ico)
      static_files: dist/projectname/\1
      upload: dist/projectname/(.*\.ico)
      secure: always
    
    - url: /(.*\.js)
      static_files: dist/projectname/\1
      upload: dist/projectname/(.*\.js)
      secure: always
    
    - url: /(.*\.txt)
      static_files: dist/projectname/\1
      upload: dist/projectname/(.*\.txt)
      secure: always
    
    # Site root.
    - url: /
      static_files: dist/projectname/index.html
      upload: dist/projectname/index.html
      secure: always
    
    # Catch-all rule, responsible from handling Angular application routes (deeplinks).
    - url: /.*
      static_files: dist/projectname/index.html
      upload: dist/projectname/index.html
      secure: always
    
    skip_files:
    - ^(?!dist)
</code></pre>
<p>I faced some errors related to package -
which I fixed by updating a suitable version</p>
<p>I faced the error for angular
cloud build An unhandled exception occurred: Cannot find module '@angular/compiler-cli/src/tooling'</p>
<p>This is due to cache, so you should reinstall/update-modules.
<a href=""https://stackoverflow.com/questions/55606102/error-cannot-find-module-angular-compiler-cli"">Error: Cannot find module &#39;@angular/compiler-cli</a>
If nothing works try creating a new branch and trigger it from there (just hit and trial).</p>"
"Google Cloud Build does not ignore file when using Github App<p>I'm using Google Cloud Build with Kaniko (for docker layer caching).</p>
<p>I wanted to add a first step before running kaniko and realised that my <code>.gcloudignore</code> is not taken into account when Github app send the trigger. It works when I call the command myself <code>gcloud builds submit</code>.</p>
<p>.gcloudignore</p>
<pre><code># If you would like to upload your .git directory, .gitignore file or
# files from your .gitignore file, remove the corresponding line below:
.git
.gitignore
.gcloudignore
.dockerignore
#!include:.gitignore
#!include:.dockerignore
</code></pre>
<p>cloudbuild.yaml</p>
<pre><code>steps:
  - name: 'gcr.io/cloud-builders/docker'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        docker -v &amp;&amp; ls -lah
  - name: 'gcr.io/kaniko-project/executor:latest'
    id: 'image-base'
    args:
      - --build-arg=git_hash=$SHORT_SHA
</code></pre>
<p>When using the CLI it works well, the ls is showing only what should be there.
But when gcb receive a github trigger the <code>ls -lah</code> will print a lot of files that should be ignored with the include:.dockerignore</p>
<p>I don't understand and it drives me insane 
Can someone help me please?</p>","<p>As defined <a href=""https://cloud.google.com/sdk/gcloud/reference/topic/gcloudignore"" rel=""nofollow noreferrer"">here</a>, <code>gcloud</code> cli can update files in different commands. using a <code>.gcloudignore</code> file allows <code>gcloud</code> cli to not upload these files.</p>
<p>That's all, it's only for <code>gcloud</code>, not for other code upload process.</p>"
"Migration to Google Cloud Run times out on grpcio<p>I had a Python program running in Google Cloud Run to which I just added Google Stackdriver logging capabilities. The program runs fine on my local machine and properly logs to Stackdriver. However, when I try to deploy the program to Google Cloud Run, the build times-out when installing grpcio. Here are some of the messages I see...</p>
<blockquote>
<p>Step #0 - &quot;Build&quot;:     Running setup.py install for grpcio: started</p>
<p>Step #0 - &quot;Build&quot;:     Running setup.py install for grpcio: still running...</p>
</blockquote>
<p>The second message just keeps repeating until the build times-out after about 10 minutes.
My requirements.txt file is as follows...</p>
<pre><code>SQLAlchemy~=1.3.2
Werkzeug~=1.0.1
MarkupSafe~=1.1.1
WTForms~=2.3.1
python-dateutil~=2.8.1
Flask~=1.1.2

grpcio~=1.34.0
google-cloud-logging~=2.0.2
</code></pre>
<p>Any thoughts on what's happening and how I can fix it? Thanks!</p>","<p>OK, it turns out that sometimes the best answer is the easiest one. Cloud Build was, indeed, timing out so all we had to do was to increase the amount of time it runs before timing out. This is done in the Build Configuration trigger YAML file by adding the following parameter (Cloud Build -&gt; Triggers -&gt; Build Configuration -&gt; Inline)...</p>
<pre><code>timeout: 1200s
</code></pre>
<p>I also added a line to my Dockerfile that may or may not be part of the solution (I haven't had a chance to see what happens if I try to run without this line)</p>
<pre><code>RUN apk add --no-cache linux-headers
</code></pre>"
Storing Secrets as cloud build environment variables<p>The recommended way of using secrets during builds on Cloud Build is by loading them in from Secret Manager. What would be the dangers of saving them as environment variables on the build trigger?</p>,<p>Anyone with project viewer or higher permissions will be able to see them. Anyone who can invoke your build can easily print them out in build logs. There’s no auditing or logging when a secret is accessed or by who.</p>
"How to use cloud build to deploy cloud run with cloud sql on google cloud?<p>My cloudbuild.yaml file
(I have built a docker image and pushed it to gcr)</p>
<p>This application using mysql on Cloud SQL. So needs to connect to it.</p>
<pre><code>steps:
  - id: cloud-run
    name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - 'run'
      - 'deploy'
      - 'my-service'
      - '--image'
      - 'asia.gcr.io/$_PROJECT_ID/my-service:$_COMMIT_SHA'
      - '--region'
      - 'asia-northeast1'
      - '--platform'
      - 'managed'
      - '--service-account'
      - '$_CLOUD_RUN_PUBSUB_INVOKER'
      - '--add-cloudsql-instances'
      - '$_MYSQL_MAIN_INSTANCE_NAME'
      - '--set-env-vars'
      - 'MYSQL_MAIN_CONNECTIONS=$_MYSQL_MAIN_CONNECTIONS'
      - '--set-env-vars'
      - 'MYSQL_MAIN_INSTANCE_NAME=$_MYSQL_MAIN_INSTANCE_NAME'
      - '--set-env-vars'
      - 'MYSQL_MAIN_DB=$_MYSQL_MAIN_DB'
      - '--set-env-vars'
      - 'MYSQL_MAIN_USER=$_MYSQL_MAIN_USER'
      - '--set-env-vars'
      - 'MYSQL_MAIN_PASSWORD_SECRET_ID=$_MYSQL_MAIN_PASSWORD_SECRET_ID'
      - '--set-env-vars'
</code></pre>
<p>When ran build to submit, got Cloud SQL API not activated error</p>
<pre><code>$ gcloud builds submit
Creating temporary tarball archive of 5 file(s) totalling 47.4 KiB before compression.
Uploading tarball of [.] to [gs://my-project_cloudbuild/source/1610067564.911628-8d7f3de581ca4b8faa57bd5a8ea75ef1.tgz]
Created [https://cloudbuild.googleapis.com/v1/projects/my-project/locations/global/builds/b4e1bf9c-bc06-4ce8-b252-3b34f164719d].
Logs are available at [https://console.cloud.google.com/cloud-build/builds/b4e1bf9c-bc06-4ce8-b252-3b34f164719d?project=421686839359].
---------------------------------------------------------------------------------------------- REMOTE BUILD OUTPUT -----------------------------------------------------------------------------------------------
starting build &quot;b4e1bf9c-bc06-4ce8-b252-3b34f164719d&quot;

FETCHSOURCE
Fetching storage object: gs://my-project_cloudbuild/source/1610067564.911628-8d7f3de581ca4b8faa57bd5a8ea75ef1.tgz#1610067566084932
Copying gs://my-project_cloudbuild/source/1610067564.911628-8d7f3de581ca4b8faa57bd5a8ea75ef1.tgz#1610067566084932...
/ [1 files][ 17.1 KiB/ 17.1 KiB]
Operation completed over 1 objects/17.1 KiB.
BUILD
Pulling image: gcr.io/google.com/cloudsdktool/cloud-sdk
Using default tag: latest
latest: Pulling from google.com/cloudsdktool/cloud-sdk
6c33745f49b4: Already exists
...
ffa0764d79dc: Pull complete
Digest: sha256:3f32cb39cdfe8902bc85e31111a9f1bc7cbd9d37f31c6164f2b41cfdaa66284f
Status: Downloaded newer image for gcr.io/google.com/cloudsdktool/cloud-sdk:latest
gcr.io/google.com/cloudsdktool/cloud-sdk:latest
Skipped validating Cloud SQL API and Cloud SQL Admin API enablement due to an issue contacting the Service Usage  API. Please ensure the Cloud SQL API and Cloud SQL Admin API are activated (see https://console.cloud.google.com/apis/dashboard).
ERROR: (gcloud.run.deploy) PERMISSION_DENIED: The caller does not have permission
ERROR
ERROR: build step 0 &quot;gcr.io/google.com/cloudsdktool/cloud-sdk&quot; failed: step exited with non-zero status: 1
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

ERROR: (gcloud.builds.submit) build b4e1bf9c-bc06-4ce8-b252-3b34f164719d completed with status &quot;FAILURE&quot;
</code></pre>
<p>I have checked dashboard <a href=""https://console.cloud.google.com/apis/dashboard"" rel=""nofollow noreferrer"">https://console.cloud.google.com/apis/dashboard</a>, both <code>Cloud SQL API and Cloud SQL Admin API</code> are activated.</p>
<p>I also ran permission setting by <a href=""https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-cloud-run#continuous-iam"" rel=""nofollow noreferrer"">https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-cloud-run#continuous-iam</a></p>
<pre><code>gcloud iam service-accounts add-iam-policy-binding \
  PROJECT_NUMBER-compute@developer.gserviceaccount.com \
  --member=&quot;serviceAccount:PROJECT_NUMBER@cloudbuild.gserviceaccount.com&quot; \
  --role=&quot;roles/iam.serviceAccountUser&quot;
</code></pre>
<p>But still the same error.</p>","<p>It seems the error is about the IAM permission.</p>
<blockquote>
<p>PERMISSION_DENIED: The caller does not have permission</p>
</blockquote>
<p>You also need to follow the Required IAM permission steps in this <a href=""https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-cloud-run#required_iam_permissions"" rel=""nofollow noreferrer"">document</a>:</p>
<blockquote>
<p>To deploy to Cloud Run (fully managed) grant the Cloud Run Admin and Service Account User roles to the Cloud Build service account:</p>
<ol>
<li><p>In the Cloud Console, go to the Cloud Build Settings page:</p>
</li>
<li><p>Open the Settings page</p>
</li>
<li><p>In the Service account permissions panel, set the status of the Cloud Run Admin role to ENABLED:</p>
</li>
<li><p>In the Additional steps may be required pop-up, you click Skip or click GRANT ACCESS TO ALL SERVICE ACCOUNTS.</p>
</li>
</ol>
</blockquote>"
"Where to store environment variables in App Engine for CI/CD Pipeline?<p>I am deploying my first application on the cloud and I'm trying to setup my env vars.</p>
<p>From what I understand, they are set in the <code>app.yaml</code> file. But if that file is pushed to the repo, it would then contain the secret API keys which is bad..</p>
<p>I could treat the <code>app.yaml</code> the same way I treat the <code>.env</code> but the problem is, how can I set env vars for prod in a CI/CD pipeline?</p>
<p>I am using Cloud Build to run my build pipeline. I am coming from Bitbucket &amp; Heroku and there doesn't seem to be a way to &quot;set&quot; the env vars for the build environment like on those two platforms.</p>
<p>So then, how can I make my .env variables available in my app without taking risks of pushing it on my repo?</p>
<p>Thank you for your help</p>","<p>For those looking, here is how I solved this problem.</p>
<p>I followed the steps outlined in <a href=""https://medium.com/@brian.young.pro/how-to-add-environmental-variables-to-google-app-engine-node-js-using-cloud-build-5ce31ee63d7"" rel=""nofollow noreferrer"">this blog post</a>.</p>
<p>Basically we set variables in the <code>.yaml</code> file, which we then compile into an <code>.env</code> file during the build process. We can set what the value of those variables is via Cloud Build configuration so we can restrict access to them and have them hidden.</p>"
"Google Cloud Build and App Engine enviroment variables<p>I have a secret token on my App Engine <code>app.yaml</code></p>
<pre><code>env_variables:
  TOKEN: super-secret-token
</code></pre>
<p>And obviously this token is out of git. Using Google Cloud Build, how can I set this parameter <code>TOKEN</code> at build time or before?</p>","<p>You can use <a href=""https://cloud.google.com/cloud-build/docs/securing-builds/use-encrypted-secrets-credentials"" rel=""nofollow noreferrer"">Secret Manager within Cloud Build</a> to get the actual secret and replace the <code>super-secret-token</code> placeholder value in app.yaml prior to deploying your app to App Engine. That would look something like this:</p>
<pre><code>steps:
- name: gcr.io/cloud-builders/gcloud
  entrypoint: 'bash'
  args: [ '-c', &quot;gcloud secrets versions access latest --secret=secret-name --format='get(payload.data)' | tr '_-' '/+' | base64 -d &gt; decrypted-data.txt&quot; ]
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: /bin/sh
  args:
  - '-c'
  - |
     sed &quot;s/super-secret-token/g&quot; $(cat decrypted-data.txt)
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'bash'
  args: ['-c', 'gcloud config set app/cloud_build_timeout 1600 &amp;&amp; gcloud app deploy']
timeout: '1600s'
</code></pre>
<p>Having said that, your secret token will still be available unencrypted in your App Engine's environment variable which is not optimal security-wise. Instead you may want to query Secret Manager from within your App Engine code directly. You'll find code samples to do so <a href=""https://cloud.google.com/secret-manager/docs/samples/secretmanager-get-secret"" rel=""nofollow noreferrer"">here</a>.</p>"
"Deploy Docker images to google cloud run with docker-compose<p>I'm trying to deploy an app from a docker-compose file with two images in it: an Angular app (frontend) and a very small nestjs app (backend). I'm using GCB, their triggers and a cloudbuild.json.</p>
<p>This is its current state:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;steps&quot;: [
    {
      &quot;name&quot;: &quot;gcr.io/$PROJECT_ID/docker-compose&quot;,
      &quot;args&quot;: [&quot;-f&quot;, &quot;./docker-compose.${_ENVIRONMENT}.yml&quot;, &quot;up&quot;, &quot;-d&quot;]
    },
    {
      &quot;name&quot;: &quot;gcr.io/cloud-builders/docker&quot;,
      &quot;args&quot;: [&quot;tag&quot;, &quot;configurator:latest&quot;, &quot;gcr.io/$PROJECT_ID/${_IMAGE_ID}&quot;]
    },
    {
      &quot;name&quot;: &quot;gcr.io/cloud-builders/gcloud&quot;,
      &quot;args&quot;: [
        &quot;run&quot;, &quot;deploy&quot;,
        &quot;--allow-unauthenticated&quot;,
        &quot;${_IMAGE_ID}&quot;,
        &quot;--image&quot;, &quot;gcr.io/$PROJECT_ID/${_IMAGE_ID}&quot;,
        &quot;--region&quot;, &quot;europe-west4&quot;,
        &quot;--platform&quot;, &quot;managed&quot;
      ]
    }
  ],
  &quot;images&quot;: [
    &quot;gcr.io/$PROJECT_ID/${_IMAGE_ID}&quot;
  ],
  &quot;timeout&quot;: &quot;1200s&quot;
}
</code></pre>
<p>The build always fails on step 2, when trying to push the docker image to the registry. I'm not really sure what the images name could be or how this should even work, since there is two docker images that need to be pushed and deployed. Is it even possible with GCB or do I need a GKE Cluster for running two docker images?</p>
<p>Should I maybe build the two Docker images separately, push them each to the registry and deploy them to separate GCR Services?</p>
<p>Thanks in advance.</p>","<p>Docker compose build nothing, it only run the images according with the yaml configuration. Your step 2, that tag something, tag what? That's why it fails.</p>
<p>So, forget docker compose. Deploy your backend on Cloud Run, only the container. For your frontend, you have several solution:</p>
<ul>
<li>Deploy it on Cloud Run also (not the best solution)</li>
<li>Deploy the static files on App Engine (need to create a app.yaml file)</li>
<li>Deploy the static files on Google Cloud Storage.</li>
</ul>
<p>then, create a load balancer with 2 backends:</p>
<ol>
<li>Your web site backend</li>
<li>Your nestjs backend.</li>
</ol>
<p>And then, you have a production ready deployment. (Let me know if you need more help on some parts)</p>
<hr />
<p><strong>EDIT 1</strong></p>
<p>With a load balancer, you can define the path to serve your resources (URL map). For instances:</p>
<ul>
<li>/nest/* -&gt; redirect the traffic to your nestjs backend</li>
<li>/* -&gt; by default, redirect the traffic to your bucket backend.</li>
</ul>
<p>Both, static and nestjs backend, are accessible through the same IP (and domain name, after the set up of your DNS registrar. Google can also automatically provision the SSL certificate for you).</p>
<p>There is 2 advantages to use a Load balancer:</p>
<ul>
<li>The front and the back have the same base URL, you won't have to manage CORS on your backend.</li>
<li>The traffic of both back and front is served in HTTP (not recommended) or HTTPS, that prevent the errors for mixed content in your browser.</li>
</ul>
<p><em>Note: without a load balancer, you can serve static source only in HTTP mode, HTTPS isn't supported without additional layer, such as a load balancer.</em></p>
<p>The &quot;issue&quot; with a load balancer, is a minimal cost of $15 per month.</p>"
"GCP cloud build VIEW RAW logs link<p>I have written a small cloud function in GCP which is subscribed to Pub/Sub event. When any cloud builds triggered function post message into the slack channel over webook.</p>
<p>In response, we get lots of details to trigger name, branch name, variables details but i am more interested in <strong>Build logs URL</strong>.</p>
<p>Currently getting build logs URL in response is like : <strong>logUrl</strong>: <code>https://console.cloud.google.com/cloud-build/builds/899-08sdf-4412b-e3-bd52872?project=125205252525252</code></p>
<p>which requires GCP console access to check logs.</p>
<p>While in the console there an option <strong>View Raw</strong>. Is it possible to get that direct URL in the event response? so that i can directly sent it to slack and anyone can access direct logs without having GCP console access.</p>","<p>In your Cloud Build event message, you need to extract 2 values from the JSON message:</p>
<ul>
<li>logsBucket</li>
<li>id</li>
</ul>
<p>The raw file is stored here</p>
<pre><code>&lt;logsBucket&gt;/log-&lt;id&gt;.txt
</code></pre>
<p>So, you can get it easily in your function with Cloud Storage client library (preferred solution) or with a simple HTTP Get call to the storage API.</p>
<p>If you need more guidance, let me know your dev language, I will send you a piece of code.</p>"
"add small bash script to cloudbuild.yaml<p>I have a <code>cloudbuild.yaml</code> file for google cloud (GCP). I would like to grab the <code>version</code> from <code>package.json</code> using a simple bash script <code>$(node -p -e &quot;require('./package.json').version&quot;)</code> (or any other way). How can I add this to my <code>cloudbuild.yaml</code> file?</p>
<p>I tried putting the script in <code>substitution</code> but it didn't work.</p>
<pre><code># gcloud submit   --substitutions=_VERSION=&quot;1.1.0&quot;

steps:
  # build the container image
  - name: &quot;gcr.io/cloud-builders/docker&quot;
    args: [&quot;build&quot;, &quot;-t&quot;, &quot;gcr.io/${_PROJECT_ID}/${_IMAGE}:${_VERSION}&quot;, &quot;.&quot;]
  # push the container image to Container Registry
  - name: &quot;gcr.io/cloud-builders/docker&quot;
    args: [&quot;push&quot;, &quot;gcr.io/${_PROJECT_ID}/${_IMAGE}:${_VERSION}&quot;]
  # build the container image
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      [
        &quot;run&quot;,
        &quot;deploy&quot;,
        &quot;${_SERVICE_NAME}&quot;,
        &quot;--project&quot;,
        &quot;${_PROJECT_ID}&quot;,
        &quot;--image&quot;,
        &quot;gcr.io/${_PROJECT_ID}/${_IMAGE}:${_VERSION}&quot;,
        &quot;--platform&quot;,
        &quot;managed&quot;,
        &quot;--allow-unauthenticated&quot;,
        &quot;--region&quot;,
        &quot;${_REGION}&quot;,
        &quot;--set-env-vars&quot;,
        &quot;${_ENV_VARS}&quot;,
        &quot;--ingress&quot;,
        &quot;internal-and-cloud-load-balancing&quot;,
        &quot;--quiet&quot;,
      ]
images:
  - gcr.io/${_PROJECT_ID}/${_IMAGE}

substitutions:
  _REGION: us-east1
  _PROJECT_ID: my-dev
  _SERVICE_NAME: my-client
  _IMAGE: my-client
  _VERSION: $(node -p -e &quot;require('./package.json').version&quot;)
  _ENV_VARS: &quot;APP_ENV=dev&quot;
</code></pre>","<p>Based on Guillaume's answer, you can use a bash script with two <code>$$</code> instead of 1 <code>$</code>, like so <code>$$(node -p -e &quot;require('./package.json').version&quot;)</code>. <strong>However</strong>, if the command you're attempting to use isn't available (<code>node</code> will not be available), it's best to pull it from a file that you can make in the step above like in Guillaume's answer:</p>
<pre class=""lang-yaml prettyprint-override""><code>- name: &quot;gcr.io/cloud-builders/docker&quot;
    entrypoint: bash
    args: 
      - -c
      - docker build -t gcr.io/${_PROJECT_ID}/${_IMAGE}:$$(cat ./package_version) .
</code></pre>"
"How to identify cloudbuild child tasks<p>I have a CloudBuild <code>cloudbuild.yaml</code> file which defines a task to export a GCE Image to a Bucket in <code>.vmdk</code> format.</p>
<pre class=""lang-sh prettyprint-override""><code>    gcloud compute images export \
    --image=$IMAGE_NAME \
    --destination-uri=$DESTINATION_BUCKET/$VMDK_NAME \
    --export-format=vmdk \
    --network=$NETWORK \
    --subnet=$SUBNET \
    --project=$PROJECT_ID \
    --async
</code></pre>
<p>The <code>gcloud compute images export</code> is working fine; the command triggers a child CloudBuild to convert and to upload to gcs.</p>
<p>There are no fields in the new build that identifies the &quot;parent&quot; caller, these fields are empty:
<code>Provider</code>, <code>Source</code>, <code>Ref Commit</code>, <code>Trigger Id</code>, <code>Trigger Type</code>, <code>Trigger Name</code>, <code>Trigger Description</code></p>
<p>How can I identify these child process ?</p>","<p>Interesting question.</p>
<p>I think you may be unable to do this (see Hack below) because there's no user-definable metadata that you can pass from the parent Cloud Build task through the <code>gcloud compute images export ...</code> to the child Cloud Build task.</p>
<blockquote>
<p><strong>Hack</strong> I've not tried this! It may (!?) be possible to inject a trace token from Cloud Build into the step that invokes <code>gcloud compute images export</code> command using the <a href=""https://cloud.google.com/sdk/gcloud/reference#--trace-token"" rel=""nofollow noreferrer""><code>--trace-token</code></a> flag. You could try (I've not tried this) setting this flag on <code>gcloud compute images export ... --trace-token=${SOMETHING}</code> perhaps using the value of the parent's Cloud Build ID? This <strong>should</strong> be passed through subsequent API calls and hopefully at least reaches as far the child Cloud Build.</p>
</blockquote>
<p>I think it's a useful feature request to be able to pass arbitrary metadata (labels?) through Cloud SDK commands. You should consider submitting a request to Google's <a href=""https://issuetracker.google.com/issues/new?component=187143"" rel=""nofollow noreferrer"">Issue Tracker for Cloud SDK</a></p>"
"Error when deploying to App Engine through Cloud Build<p>When I manually run <code>gcloud app deploy</code> from Cloud Shell, I am able to deploy the application to App Engine and it works.</p>
<p>But when I try to do the same via Cloud Build, the build succeeds, but the application URL returns a 500 Error.</p>
<p><a href=""https://i.stack.imgur.com/yDb3g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yDb3g.png"" alt=""App Engine Application"" /></a></p>
<p><a href=""https://i.stack.imgur.com/7T9TS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7T9TS.png"" alt=""cloudbuild.yaml"" /></a></p>
<p><a href=""https://i.stack.imgur.com/Htk7F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Htk7F.png"" alt=""Cloudbuild Service Account Permissions"" /></a></p>
<p>The Winston library tries to create a logs directory when the npm start script it run. Seems like the Service Account is unable to create that directory according to the Logs Explorer. This error does not appear when I deploy it through Cloud Shell.</p>
<p><a href=""https://i.stack.imgur.com/L5eYO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L5eYO.png"" alt=""enter image description here"" /></a></p>
<p>What does the Cloud Build Service Account need?</p>",<p>is your logs directory part of gitignore? It may be possible that deploying manually works because you have a logs directory in your local but not on cloud build</p>
"Cloudbuild can't find my package.json from my Vue app<p>I am attempting to deploy a Vue JS app to GCP App Engine via GCP Cloud Build. I've followed the instructions and have created a cloudbuild.yaml file in a separate directory from the app.yaml file. The build errors with</p>
<p><code>error Couldn't find a package.json file in &quot;/workspace&quot;</code></p>
<p>It looks like the first two steps of the cloudbuild.yaml file execute successfully, but it fails when it tries to run the build.</p>
<p>The directory is like so:</p>
<pre><code>root/
├─ config/
│  ├─ cloudbuild.yaml
app.yaml
package.json
</code></pre>
<p>Here is my app.yaml file</p>
<pre><code>runtime: nodejs10
service: icx-ui

handlers:
# Serve all static files with urls ending with a file extension
- url: /(.*\..+)$ 
  static_files: dist/\1
  upload: dist/(.*\..+)$
# catch all handler to index.html
- url: /.*
  static_files: dist/index.html
  upload: dist/index.html
</code></pre>
<p>My cloudbuild.yaml is as follows:</p>
<pre><code>steps:
  - name: node
    entrypoint: yarn
    args: [&quot;install&quot;]
  - name: node
    entrypoint: yarn
    args: ['global', 'add', '@vue/cli']
  - name: node
    entrypoint: yarn
    args: [&quot;run&quot;, &quot;build&quot;]
  - name: &quot;gcr.io/cloud-builders/gcloud&quot;
    args: [&quot;app&quot;, &quot;deploy&quot;, &quot;./app.yaml&quot;]
    timeout: &quot;1600s&quot;
</code></pre>
<p>As you can see, I add a file path to my app.yaml file</p>","<p>The defined source directory of your <code>gcloud builds submit</code> command is <code>dist/</code> which is the built, production version of your app. This directory does not contain both package.json and app.yaml by default.</p>
<p>Running <code>yarn run build</code> on Cloud Build requires files such as <code>src/</code> and <code>package.json</code> to build the
files (dist) that you will serve via App Engine. The solution is to specify the root project as the source directory of the build:</p>
<pre><code>gcloud builds submit --config ./config/cloudbuild.yaml .
</code></pre>
<p>If at some case this brought you concerns where some unnecessary files are included on deployment such as <code>node_modules</code>, you can ignore such files by specifying them on <code>.gcloudignore</code>.</p>"
"Parallelizing google cloudbuild steps without mangling the build logs<p>Following guidance here: <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/configure-build-step-order"" rel=""nofollow noreferrer"">https://cloud.google.com/cloud-build/docs/configuring-builds/configure-build-step-order</a></p>

<p>We have split up our build into a multilayer docker image where layers are</p>

<ol>
<li>Install OS and third party deps</li>
<li>Install our sources and build (in debug or release depending on docker ARG)</li>
<li>Run ci / code coverage</li>
</ol>

<p>We have tagged the steps in cloudbuild.yaml with docker id's and are using <code>waitfor</code> to try to make it so that the <code>debug</code> and <code>release</code> versions can run <em>in parallel</em>.</p>

<p>However, when we do this, the build log is mixed up -- the build logs for release and debug are are jumbled together making it much harder to read. e.g.</p>

<pre><code>Step #2 - ""build-debug"": �[0m�[91m  Downloaded colored v1.7.0
Step #5 - ""build-release"": �[0m�[91merror: couldn't read /tmp/mobilenode/src/attest/src/ias/../data/AttestationReportSigningCACert.pem: No such file or directory (os error 2)
Step #5 - ""build-release"":   --&gt; /tmp/mobilenode/src/attest/src/ias/verify.rs:35:7
Step #5 - ""build-release"":    |
Step #5 - ""build-release"": 35 |     &amp;[include_str!(""../data/AttestationReportSigningCACert.pem"")];
Step #5 - ""build-release"":    |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Step #5 - ""build-release"": 
Step #2 - ""build-debug"": �[0m�[91m  Downloaded toml v0.4.10
</code></pre>

<pre><code>[0m�[91m  Downloaded safemem v0.3.0
Step #2 - ""build-debug"": �[0m�[91m  Downloaded crunchy v0.1.6
Step #2 - ""build-debug"": �[0m�[91m  Downloaded grpcio-sys v0.4.4
Step #5 - ""build-release"": �[0m�[91merror: aborting due to previous error
Step #5 - ""build-release"": 
Step #5 - ""build-release"": �[0m�[91merror: Could not compile `attest`.
Step #5 - ""build-release"": warning: build failed, waiting for other jobs to finish...
Step #5 - ""build-release"": �[0m�[91merror: build failed
Step #5 - ""build-release"": �[0m�[91mmake: *** [src/enclave/target/release/libenclave.so] Error 101
Step #5 - ""build-release"": �[0mMakefile:90: recipe for target 'src/enclave/target/release/libenclave.so' failed
Step #2 - ""build-debug"": �[0m�[91m  Downloaded term v0.5.1
Step #2 - ""build-debug"": �[0m�[91m  Downloaded tiny_http v0.6.2
Step #2 - ""build-debug"": �[0m�[91m  Downloaded regex v0.1.80
</code></pre>

<p>Is there a way to configure cloudbuild so that there is a separate log file for each build step? Is the best answer to use a different cloudbuild.yaml for the parallel steps, and skip all this <code>wait_for</code> stuff?</p>","<p>I was having this same issue, but then I discovered that viewing a build via the <code>History</code> under <code>Cloud Build</code> in the <a href=""https://console.cloud.google.com/cloud-build/builds"" rel=""nofollow noreferrer"">Console</a> will let you view each step individually, or view the entire job, like you're currently seeing (click <code>Build Summary</code>, the top entry in the sidebar).
Click on the name of the step in sidebar, each step should be comprised of a number followed by a colon and then the name of the step. The number appears to be based upon the order in which the job appears in your <code>cloudbuild.yaml</code></p>
<p>What you're seeing is a result of the parallelization of your build, as each step that is marked as being able to run concurrently is at a different step in its execution.</p>
<p><a href=""https://i.stack.imgur.com/MqLxQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MqLxQ.png"" alt=""Example Build"" /></a>
You can read more about viewing the Build History using the console <a href=""https://cloud.google.com/build/docs/view-build-results#console"" rel=""nofollow noreferrer"">here</a>.</p>"
"How do I access a git tag in a google cloud build?<p>I have a Cloud Source Repository where I maintain the code of my python package. I have set up two triggers:</p>
<ul>
<li>A trigger that runs on every commit on every branch (this one installs my python package and tests the code.</li>
<li>A trigger that runs on a pushed git tag (install the package, test, build artifacts, and deploy them to my private pypi repo).</li>
</ul>
<p>During the second trigger, I want to verify that my Version number matches the git tag. In the setup.py file, I have added the code:</p>
<pre><code>#!/usr/bin/env python
import sys
import os
from setuptools import setup
from setuptools.command.install import install

VERSION = &quot;v0.1.5&quot;


class VerifyVersionCommand(install):
    &quot;&quot;&quot;Custom command to verify that the git tag matches our version&quot;&quot;&quot;
    description = 'verify that the git tag matches our version'

    def run(self):
        tag = os.getenv('TAG_NAME')

        if tag != VERSION:
            info = &quot;Git tag: {0} does not match the version of this app: {1}&quot;.format(
                tag, VERSION
            )
            sys.exit(info)


setup(
    name=&quot;name&quot;,
    version=VERSION,
    classifiers=[&quot;Programming Language :: Python :: 3 :: Only&quot;],
    py_modules=[&quot;name&quot;],
    install_requires=[
        [...]
    ],
    packages=[&quot;name&quot;],
    cmdclass={
        'verify': VerifyVersionCommand,
    }
)
</code></pre>
<p>The beginning of my cloudbuild.yaml looks like this:</p>
<pre><code>steps:

  - name: 'docker.io/library/python:3.8.6'
    id: Install
    entrypoint: /bin/sh
    args:
      - -c
      - |
        python3 -m venv /workspace/venv &amp;&amp;
        . /workspace/venv/bin/activate &amp;&amp;
        pip install -e .

  - name: 'docker.io/library/python:3.8.6'
    id: Verify
    entrypoint: /bin/sh
    args:
      - -c
      - |
        . /workspace/venv/bin/activate &amp;&amp;
        python setup.py verify
</code></pre>
<p>This works flawlessly on CircleCi, but on Cloud Build I get the error message:</p>
<pre><code>Finished Step #0 - &quot;Install&quot;
Starting Step #1 - &quot;Verify&quot;
Step #1 - &quot;Verify&quot;: Already have image: docker.io/library/python:3.8.6
Step #1 - &quot;Verify&quot;: running verify
Step #1 - &quot;Verify&quot;: /workspace/venv/lib/python3.8/site-packages/setuptools/dist.py:458: UserWarning: Normalizing 'v0.1.5' to '0.1.5'
Step #1 - &quot;Verify&quot;:   warnings.warn(tmpl.format(**locals()))
Step #1 - &quot;Verify&quot;: Git tag: None does not match the version of this app: v0.1.5
Finished Step #1 - &quot;Verify&quot;
ERROR
ERROR: build step 1 &quot;docker.io/library/python:3.8.6&quot; failed: step exited with non-zero status: 1
</code></pre>
<p>Therefore, the <code>TAG_NAME</code> variable as specified in the <a href=""https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values"" rel=""nofollow noreferrer"">Cloud Build documentation</a> seems to not contain the git tag.</p>
<p>How can I access the git tag to verify it?</p>","<p>The <code>TAG_NAME</code> is set as substitution variables but not as environment variables</p>
<p>You can do that</p>
<pre><code>  - name: 'docker.io/library/python:3.8.6'
    id: Verify
    entrypoint: /bin/sh
    env:
    - &quot;TAG_NAME=$TAG_NAME&quot;
    args:
      - -c
      - |
        . /workspace/venv/bin/activate &amp;&amp;
        python setup.py verify
</code></pre>"
"How come file is not excluded with gsutil rsync -x by the Google Cloud Builder?<p>I am currently running the gsutil rsync cloud build command:</p>
<pre><code>gcr.io/cloud-builders/gsutil
-m rsync -r -c -d -x &quot;\.gitignore&quot; . gs://mybucket/ 
</code></pre>
<p>I am using the <code>-x &quot;\.gitignore&quot;</code> argument here to try and not copy over the <code>.gitignore</code> file, as mentioned here:</p>
<p><a href=""https://cloud.google.com/storage/docs/gsutil/commands/rsync"" rel=""nofollow noreferrer"">https://cloud.google.com/storage/docs/gsutil/commands/rsync</a></p>
<p>However, when looking in the bucket and the logs, it still says:</p>
<p><code>2021-04-23T13:29:37.870382893Z Step #1: Copying file://./.gitignore [Content-Type=application/octet-stream]...</code></p>
<p>So <code>rsync</code> is still copying over the file despite the <code>-x &quot;\.gitignore&quot;</code> argument.</p>
<p>According to the docs <code>-x</code> is a Python regexp, so <code>//./.gitignore</code> should be captured by <code>\.gitignore</code></p>
<p><a href=""https://i.stack.imgur.com/9O7TU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9O7TU.png"" alt=""enter image description here"" /></a></p>
<p>Does anyone know why this isn't working and why the file is still being copied?</p>","<p>See the <a href=""https://github.com/GoogleCloudPlatform/gsutil/blob/master/gslib/commands/rsync.py#L755"" rel=""nofollow noreferrer""><code>rsync.py</code></a> source code:</p>
<blockquote>
<p><code>if cls.exclude_pattern.match(str_to_check):</code></p>
</blockquote>
<p>In Python, <code>re.match</code> only returns a match if it occurs at the start of string.</p>
<p>So, in order to find a match anywhere using the <code>-x</code> parameter, you need to prepend the pattern you need to find with <code>.*</code> or with <code>(?s).*</code>:</p>
<pre class=""lang-py prettyprint-override""><code>gcr.io/cloud-builders/gsutil
-m rsync -r -c -d -x &quot;.*\.gitignore&quot; . gs://mybucket/ 
</code></pre>
<p>Note that to make sure <code>.gitignore</code> appears at the end of string, you need to append <code>$</code>, <code>-x &quot;.*\.gitignore$&quot;</code>.</p>"
"How to build quarkus native image on google cloudbuild<p>Does anyone has an idea how to build quarkus native image on cloudbuild?
I use following command to do so:</p>
<pre><code>- name: maven:3-jdk-11
    entrypoint: mvn
    args: [&quot;package&quot;, &quot;-Dmaven.test.skip=true&quot;, &quot;-Pnative&quot;, &quot;-Dquarkus.native.container-build=true&quot;, &quot;-Dquarkus.container-image.build=true&quot;]
</code></pre>
<p>Locally everything works fine, but when I try to do it on Google Cloud it throws an error:</p>
<pre><code>[ERROR] Failed to execute goal io.quarkus:quarkus-maven-plugin:1.12.2.Final:build (default) on project fishki: Failed to build quarkus application: io.quarkus.builder.BuildException: Build failure: Build failed due to errors
[ERROR]     [error]: Build step io.quarkus.deployment.pkg.steps.NativeImageBuildStep#build threw an exception: java.lang.IllegalStateException: No container runtime was found to run the native image builder
[ERROR]     at io.quarkus.deployment.pkg.steps.NativeImageBuildContainerRunner.detectContainerRuntime(NativeImageBuildContainerRunner.java:114)
</code></pre>
<p>My idea is to try to provide the container runtime to run the native image builder, but I have no idea how to do it.</p>
<p>I will appreciate any help, thanks!</p>
<p><strong>EDIT</strong>:</p>
<p>I use following cloudbuild.yaml</p>
<pre><code>steps:
  - name: maven:3-jdk-11
    entrypoint: mvn
    args: [&quot;quarkus:add-extension&quot;, &quot;-Dextensions=container-image-docker&quot;]
  - name: docker:latest
  - name: maven:3-jdk-11
    entrypoint: mvn
    args: [&quot;package&quot;, &quot;-Pnative&quot;, &quot;-Dmaven.test.skip=true&quot;, &quot;-Dquarkus.container-image.build=true&quot;, &quot;-Dquarkus.native.container-build=true&quot;, &quot;-Dquarkus.native.container-runtime=docker&quot;]
  - name: 'gcr.io/cloud-builders/docker'
    args: [ 'build', '-t', 'gcr.io/XXX-XX-XXX/XX-XXX', '.' ]
  - name: &quot;gcr.io/cloud-builders/docker&quot;
    args: [&quot;push&quot;, &quot;gcr.io/XXXX/XXX-XXXX&quot;]
  - name: &quot;gcr.io/cloud-builders/gke-deploy&quot;
    args:
      - run
      - --filename=./deployment.yaml
      - --image=gcr.io/XXX/XXX:latest
      - --location=europe-west1-b
      - --cluster=XX-XXX-XXX-1

</code></pre>
<p>Now I have a new problem - when I try to install docker container runtime, I get the following error:</p>
<pre><code>[ERROR] Failed to execute goal io.quarkus:quarkus-maven-plugin:1.12.2.Final:build (default) on project fishki: Failed to build quarkus application: io.quarkus.builder.BuildException: Build failure: Build failed due to errors
[ERROR]     [error]: Build step io.quarkus.deployment.pkg.steps.NativeImageBuildStep#build threw an exception: java.lang.RuntimeException: Failed to pull builder image quay.io/quarkus/ubi-quarkus-native-image:21.0.0-java11
</code></pre>","<p>I was struggling with this for a while, but the info in the question you asked helped me a lot, so thank you for the ideas which lead me to resolve this issue.</p>
<p>The way I ended up doing it was to build a custom build image like so (<code>Dockerfile</code>):</p>
<pre><code>FROM maven:3-jdk-11
run apt-get update
run apt-get install docker.io -y
run docker --version
</code></pre>
<p>You can find my build image here (note: mine works with gradle, not maven):
<a href=""https://hub.docker.com/repository/docker/lackrobin/quarkus-gradle-build-image"" rel=""nofollow noreferrer"">https://hub.docker.com/repository/docker/lackrobin/quarkus-gradle-build-image</a></p>
<p>Push this image to the gcr, or any other registry that you can access from google cloud build.</p>
<p>In the <code>cloudbuild.yaml</code> file the following configuration should do the trick:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: gcr.io/[link to the builder image above]
    args:
      - addExtension
      - '--extensions=container-image-docker'
      - build
      - '-Dquarkus.package.type=native'
      - '-Dquarkus.native.native-image-xmx=16g'
      - '-Dorg.gradle.jvmargs=-Xmx3g -XX:MaxPermSize=2048m'
    entrypoint: ./gradlew
  - name: gcr.io/cloud-builders/docker
    args:
      - build
      - '-t'
      - gcr.io/[project name]/[image name]
      - '-f'
      - src/main/docker/Dockerfile.native
      - .
images:
timeout: 2000s
  - gcr.io/gcr.io/[project name]/[image name]
options:
  machineType: E2_HIGHCPU_32
</code></pre>
<p>quarkus, or GraalVM uses a lot of resources to build. I had to use a vm with more resources for the build. The command <code>machineType: E2_HIGHCPU_32</code> under <code>options</code> is responsible for that.</p>"
"Cloud build with JFrog Artifactory<p>I am using google cloud build to build my maven projects and I use JFrog antifactory registry to store maven artifacts. In cloud build need these artifacts. I tried with several documentations [1], [2]. But time to time it given many errors. Can I take proper latest updated guide to integrate cloud build and JFrog antifactory. Proper authentication method need to use other than user name password. API key method can be used.</p>
<p>[1]. <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/jfrog"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/jfrog</a></p>
<p>[2]. <a href=""https://cloud.google.com/blog/products/application-development/integrating-google-cloud-build-with-jfrog-artifactory"" rel=""nofollow noreferrer"">https://cloud.google.com/blog/products/application-development/integrating-google-cloud-build-with-jfrog-artifactory</a></p>
<p><strong>EDIT 1</strong></p>
<p>I set M2_HOME as MAVEN_HOME. Then that issue was fixed. But new error given as Unsupported major.minor version 52.0. This is common issue with java version mismatch.</p>
<p>Error message :</p>
<pre><code>Step #1: [Info] Running Mvn...
Step #1: [Info] The build-info-extractor jar is not cached locally. Downloading it now...
Step #1: You can set the repository from which this jar is downloaded. Read more about it at https://www.jfrog.com/confluence/display/CLI/CLI+for+JFrog+Artifactory#CLIforJFrogArtifactory-DownloadingtheMavenandGradleExtractorJARs
Step #1: [Info] Downloading build-info-extractor from https://oss.jfrog.org/artifactory/oss-release-local/org/jfrog/buildinfo/build-info-extractor-maven3/2.26.1/build-info-extractor-maven3-2.26.1-uber.jar
Step #1: [main] WARN Sisu - Error injecting: org.jfrog.build.extractor.maven.DependencyResolutionSpy
Step #1: java.lang.TypeNotPresentException: Type org.jfrog.build.extractor.maven.DependencyResolutionSpy not present
Step #1: at org.eclipse.sisu.space.URLClassSpace.loadClass(URLClassSpace.java:115)
Step #1: at org.eclipse.sisu.space.NamedClass.load(NamedClass.java:46)
Step #1: at org.eclipse.sisu.space.AbstractDeferredClass.get(AbstractDeferredClass.java:48)
Step #1: at com.google.inject.internal.ProviderInternalFactory.provision(ProviderInternalFactory.java:86)
Step #1: at com.google.inject.internal.InternalFactoryToInitializableAdapter.provision(InternalFactoryToInitializableAdapter.java:54)
Step #1: at com.google.inject.internal.ProviderInternalFactory$1.call(ProviderInternalFactory.java:70)
Step #1: at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:113)
Step #1: at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:129)
Step #1: at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:68)
Step #1: at com.google.inject.internal.ProviderInternalFactory.circularGet(ProviderInternalFactory.java:68)
Step #1: at com.google.inject.internal.InternalFactoryToInitializableAdapter.get(InternalFactoryToInitializableAdapter.java:46)
Step #1: at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)
Step #1: at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1066)
Step #1: at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)
Step #1: at com.google.inject.Scopes$1$1.get(Scopes.java:59)
Step #1: at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)
Step #1: at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1009)
Step #1: at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1066)
Step #1: at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1005)
Step #1: at org.eclipse.sisu.inject.LazyBeanEntry.getValue(LazyBeanEntry.java:82)
Step #1: at org.eclipse.sisu.plexus.LazyPlexusBean.getValue(LazyPlexusBean.java:51)
Step #1: at org.eclipse.sisu.wire.EntryListAdapter$ValueIterator.next(EntryListAdapter.java:111)
Step #1: at java.util.AbstractCollection.toArray(AbstractCollection.java:141)
Step #1: at java.util.ArrayList.(ArrayList.java:164)
Step #1: at org.apache.maven.eventspy.internal.EventSpyDispatcher.setEventSpies(EventSpyDispatcher.java:49)
Step #1: at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Step #1: at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
Step #1: at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Step #1: at java.lang.reflect.Method.invoke(Method.java:606)
Step #1: at org.eclipse.sisu.bean.BeanPropertySetter.set(BeanPropertySetter.java:76)
Step #1: at org.eclipse.sisu.plexus.ProvidedPropertyBinding.injectProperty(ProvidedPropertyBinding.java:48)
Step #1: at org.eclipse.sisu.bean.BeanInjector.injectMembers(BeanInjector.java:52)
Step #1: at com.google.inject.internal.MembersInjectorImpl.injectMembers(MembersInjectorImpl.java:140)
Step #1: at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:117)
Step #1: at com.google.inject.internal.ConstructorInjector.access$000(ConstructorInjector.java:32)
Step #1: at com.google.inject.internal.ConstructorInjector$1.call(ConstructorInjector.java:92)
Step #1: at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:113)
Step #1: at org.eclipse.sisu.bean.BeanScheduler$Activator.onProvision(BeanScheduler.java:176)
Step #1: at com.google.inject.internal.ProvisionListenerStackCallback$Provision.provision(ProvisionListenerStackCallback.java:122)
Step #1: at com.google.inject.internal.ProvisionListenerStackCallback.provision(ProvisionListenerStackCallback.java:68)
Step #1: at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:90)
Step #1: at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:269)
Step #1: at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)
Step #1: at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1066)
Step #1: at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)
Step #1: at com.google.inject.Scopes$1$1.get(Scopes.java:59)
Step #1: at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)
Step #1: at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1009)
Step #1: at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1059)
Step #1: at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1005)
Step #1: at org.eclipse.sisu.inject.LazyBeanEntry.getValue(LazyBeanEntry.java:82)
Step #1: at org.eclipse.sisu.plexus.LazyPlexusBean.getValue(LazyPlexusBean.java:51)
Step #1: at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:263)
Step #1: at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:255)
Step #1: at org.codehaus.plexus.DefaultPlexusContainer.lookup(DefaultPlexusContainer.java:249)
Step #1: at org.apache.maven.cli.MavenCli.container(MavenCli.java:419)
Step #1: at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:210)
Step #1: at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
Step #1: at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Step #1: at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
Step #1: at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Step #1: at java.lang.reflect.Method.invoke(Method.java:606)
Step #1: at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
Step #1: at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
Step #1: at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
Step #1: at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Step #1: Caused by: java.lang.UnsupportedClassVersionError: org/jfrog/build/extractor/maven/DependencyResolutionSpy : Unsupported major.minor version 52.0
Step #1: at java.lang.ClassLoader.defineClass1(Native Method)
Step #1: at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
Step #1: at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
Step #1: at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
Step #1: at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
Step #1: at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
Step #1: at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
Step #1: at java.security.AccessController.doPrivileged(Native Method)
Step #1: at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
Step #1: at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClassFromSelf(ClassRealm.java:401)
Step #1: at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass(SelfFirstStrategy.java:42)
Step #1: at org.codehaus.plexus.classworlds.realm.ClassRealm.unsynchronizedLoadClass(ClassRealm.java:271)
Step #1: at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:247)
Step #1: at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:239)
Step #1: at org.eclipse.sisu.space.URLClassSpace.loadClass(URLClassSpace.java:107)
Step #1: ... 65 more
Step #1: [main] ERROR org.apache.maven.cli.MavenCli - Error executing Maven.
Step #1: [main] ERROR org.apache.maven.cli.MavenCli - com.google.inject.ProvisionException: Unable to provision, see the following errors:
Step #1:
Step #1: 1) Error injecting: public void org.apache.maven.eventspy.internal.EventSpyDispatcher.setEventSpies(java.util.List)
Step #1: at ClassRealm[plexus.core, parent: null] (via modules: org.eclipse.sisu.wire.WireModule -&gt; org.eclipse.sisu.plexus.PlexusBindingModule)
Step #1: while locating org.apache.maven.eventspy.internal.EventSpyDispatcher
Step #1:
Step #1: 1 error
Step #1: role: org.apache.maven.eventspy.internal.EventSpyDispatcher
Step #1: roleHint:
Step #1: [main] ERROR org.apache.maven.cli.MavenCli - Caused by: Unable to provision, see the following errors:
Step #1:
Step #1: 1) Error injecting: public void org.apache.maven.eventspy.internal.EventSpyDispatcher.setEventSpies(java.util.List)
Step #1: at ClassRealm[plexus.core, parent: null] (via modules: org.eclipse.sisu.wire.WireModule -&gt; org.eclipse.sisu.plexus.PlexusBindingModule)
Step #1: while locating org.apache.maven.eventspy.internal.EventSpyDispatcher
Step #1:
Step #1: 1 error
Step #1: [main] ERROR org.apache.maven.cli.MavenCli - Caused by: Unable to provision, see the following errors:
Step #1:
Step #1: 1) Error in custom provider, java.lang.TypeNotPresentException: Type org.jfrog.build.extractor.maven.DependencyResolutionSpy not present
Step #1: at ClassRealm[plexus.core, parent: null] (via modules: org.eclipse.sisu.wire.WireModule -&gt; org.eclipse.sisu.plexus.PlexusBindingModule)
Step #1: at ClassRealm[plexus.core, parent: null] (via modules: org.eclipse.sisu.wire.WireModule -&gt; org.eclipse.sisu.plexus.PlexusBindingModule)
Step #1: while locating org.apache.maven.eventspy.EventSpy
Step #1:
Step #1: 1 error
Step #1: [main] ERROR org.apache.maven.cli.MavenCli - Caused by: Type org.jfrog.build.extractor.maven.DependencyResolutionSpy not present
Step #1: [main] ERROR org.apache.maven.cli.MavenCli - Caused by: org/jfrog/build/extractor/maven/DependencyResolutionSpy : Unsupported major.minor version 52.0
Step #1: [Error] exit status 1
</code></pre>
<p>But my</p>
<p>project java version : 1.8</p>
<p>Maven version : 3.5.0</p>
<p>Maven docker image : gcr.io/cloud-builders/mvn:3.5.0-jdk-8</p>
<p>JFrog CLI version: 1.48.0</p>
<p>Is this error due to <a href=""https://oss.jfrog.org/artifactory/oss-release-local/org/jfrog/buildinfo/build-info-extractor-maven3/2.26.1/build-info-extractor-maven3-2.26.1-uber.jar"" rel=""nofollow noreferrer"">https://oss.jfrog.org/artifactory/oss-release-local/org/jfrog/buildinfo/build-info-extractor-maven3/2.26.1/build-info-extractor-maven3-2.26.1-uber.jar</a> or any other reason?</p>
<p>How I solve this isuue?</p>","<p>I solved this issue using maven settings xml file. I followed below steps.</p>
<ol>
<li><p>Create maven settings.xml in root directory.</p>
 
<p></p>
<pre><code> &lt;servers&gt;
     &lt;server&gt;
         &lt;id&gt;repo&lt;/id&gt;
         &lt;username&gt;${server.username}&lt;/username&gt;
         &lt;password&gt;${server.password}&lt;/password&gt;
     &lt;/server&gt;
 &lt;/servers&gt;
</code></pre>
 
</li>
<li><p>Create keyring and keys in google cloud.</p>
<p><code>gcloud kms keyrings create [KEYRING-NAME] --location=global</code></p>
<p><code>gcloud kms keys create [KEY-NAME] --location=global --keyring=[KEYRING-NAME] --purpose=encryption</code></p>
</li>
<li><p>Encrypt JFrog username and password using above keyring and key</p>
<p><code>USERNAME=aaa</code></p>
<p><code>echo $USERNAME | gcloud kms encrypt --plaintext-file=- --ciphertext-file=- --location=global --keyring=[KEYRING-NAME] --key=[KEY-NAME] | base64</code></p>
</li>
<li><p>Create cloud build file to build maven</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/mvn:3.5.0-jdk-8'
  entrypoint: 'bash'
  args: ['-c', 'mvn clean package -DskipTests=true -Dserver.username=$$USERNAME -Dserver.password=$$PASSWORD -s settings.xml -q']
  secretEnv: ['USERNAME', 'PASSWORD']

secrets:
- kmsKeyName: projects/[PROJECT]/locations/global/keyRings/jfrog/cryptoKeys/jfrog
  secretEnv:
    USERNAME: [ENCRYPTED-USERNAME]

    PASSWORD: [ENCRYPTED-PASSWORD]


</code></pre>
</li>
</ol>"
"Cloudbuild - build docker image with custom variable from a different step<p>I want to achieve the following build process:</p>
<ul>
<li>decide the value of <code>environment</code> var depending on the build branch</li>
<li>persist this value through diff build steps</li>
<li>use this var to pass it as <code>build-arg</code> to <code>docker build</code></li>
</ul>
<p>Here is some of the cloudbuild config I've got:</p>
<pre><code>  - id: 'Get env from branch'
    name: bash
    args:
      - '-c'
      - |-
        environment=&quot;dev&quot;
        if [[ &quot;${BRANCH_NAME}&quot; == &quot;staging&quot; ]]; then
          environment=&quot;stg&quot;
        elif [[ &quot;${BRANCH_NAME}&quot; == &quot;master&quot; ]]; then
          environment=&quot;prd&quot;
        fi
        echo $environment &gt; /workspace/environment.txt

  - id: 'Build Docker image'
    name: bash
    dir: $_SERVICE_DIR
    args:
      - '-c'
      - |-
        environment=$(cat /workspace/environment.txt)
        echo &quot;===== ENV: $environment&quot;
        docker build --build-arg ENVIRONMENT=$environment -t gcr.io/${_GCR_PROJECT_ID}/${_SERVICE_NAME}/${COMMIT_SHA} .
</code></pre>
<p>The problem lies in the 2nd step. If I use <code>bash</code> step image, then I've got no <code>docker</code> executable in order to build my custom image.</p>
<p>And if I use <code>gcr.io/cloud-builders/docker</code> step image, then I can't execute bash scripts. In the <code>args</code> field I can only pass arguments for the <code>docker</code> executable. And this way I cannot extract the value of <code>environment</code> that I've persisted through the steps of the build.</p>
<p>The way I managed to accomplish both is to use my own, custom, pre-built image, which contains both <code>bash</code> and <code>docker</code> executables. I have that image in the container registry and I use it as the build step image. But this requires some custom work from my side. I was wondering if there is a better, more standardized way with built-in tools from <code>cloudbuild</code>.</p>
<p>Sources:</p>
<ul>
<li><a href=""https://cloud.google.com/build/docs/configuring-builds/run-bash-scripts#running_inline_bash_scripts"" rel=""nofollow noreferrer"">how to run inline bash scripts</a></li>
<li><a href=""https://medium.com/google-cloud/how-to-pass-data-between-cloud-build-steps-de5c9ebc4cdd"" rel=""nofollow noreferrer"">how to persist values through build steps</a></li>
</ul>","<p>You can change the default entrypoint by adding <code>  entrypoint:</code> parameter</p>
<pre><code>  - name: 'gcr.io/cloud-builders/docker'
    entrypoint: 'bash'
    args:
      - -c
      - |
        echo $PROJECT_ID
        environment=$(cat /workspace/environment.txt)
        echo &quot;===== ENV: $environment&quot;
        docker build --build-arg ENVIRONMENT=$environment -t gcr.io/${_GCR_PROJECT_ID}/${_SERVICE_NAME}/${COMMIT_SHA} .
</code></pre>"
"Using a database password stored in Google Secrets Manager from within a pod running in a GKE cluster<p>I am running an application in a Google Kubernetes Cluster. One pod in it needs access to an externally managed database (Google cloud SQL), and up until now has had a hard coded password for testing purposes. It's come time to move to a more secure solution, so a generated password is now in my Google Secrets Manager and has replaced my user password in my database.</p>
<p>What is the most efficient yet secure way for me to allow the pod to access the database?</p>
<p>My deployment process is as follows:</p>
<ol>
<li>Source changes pushed to source repo</li>
<li>Changes trigger Cloud Build</li>
<li>Cloud Build builds, tags, and pushes images.</li>
<li>Cloud Build commits new image version to source repo</li>
<li>Changes to Kubernetes base manifest trigger Cloud Build</li>
<li>Cloud Build uses Kustomize to generate new Kubernetes manifest and pushes it to my GKE</li>
</ol>","<p>There are might be some options that avoid having the password pass through some of the systems that you mentioned (specifically Cloud Build and Kustomize).</p>
<p>I think that the most secure way would be using a configuration described here: <a href=""https://cloud.google.com/sql/docs/mysql/connect-kubernetes-engine"" rel=""nofollow noreferrer"">https://cloud.google.com/sql/docs/mysql/connect-kubernetes-engine</a> that way you can avoid the password all together and rely on IAM controls.</p>
<p>If this setup doesn't work for you-- Would using workload identity from your GKE app to access the Secret Manager API directly an option? This would allow your workload to access Secret Manager directly through IAM controls. <a href=""https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity"" rel=""nofollow noreferrer"">https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity</a></p>
<p>If your app can't be modified to make API calls to Secret Manager there is also an CSI driver so that you can use the file system or environment variables: <a href=""https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp</a></p>"
"GCP: how to access proj-1 secret manager from proj-2 CloudBuild?<p>All my GCP containers use <code>CloudBuild</code> on <code>my-dev-project</code> and access <code>secret manager</code> on the same project (<code>my-dev-project</code>). However, <em>one project requires access to <code>secrets manger</code> on <code>my-prod-project</code></em>. I imagine I need to add a service account, but I'm not sure how I can go about doing that for <code>CloudBuild</code> when it already has it's <code>my-dev-project</code> service account.</p>
<p><strong>Question</strong>: How can I access <code>my-prod-project</code> <code>secret manger</code> from the <code>my-dev-project</code> <code>CloudBuild</code>?</p>","<p>You can grant the Cloud Build service account from <code>my-dev-project</code> permissions on the secret in <code>my-prod-project</code>. Get the service account's email address from <code>my-dev-project</code> from the IAM console; it will be in the format:</p>
<pre><code>project-number@@cloudbuild.gserviceaccount.com
</code></pre>
<p>In <code>my-prod-project</code>, find the secret you wish to grant access, add that email with Secret Accessor permissions.</p>"
"How to store docker images in Container Registry with different tags with a config file using Cloudbuild?<p>I want to store in the Google Container Registry the image with two different tags <code>$BRANCH_NAME-$REVISION_ID</code> and <code>latest</code></p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/$PROJECT_ID/myapp:$BRANCH_NAME-$REVISION_ID', '.']
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/$PROJECT_ID/myapp:$BRANCH_NAME-$REVISION_ID']
images: ['gcr.io/$PROJECT_ID/myapp:$BRANCH_NAME-$REVISION_ID']
</code></pre>
<p>I am able to save it with a single tag, but it fails when I try to add a second tag. I get the following error</p>
<pre><code>Finished Step #1
Starting Step #2
Step #2: Already have image (with digest): gcr.io/cloud-builders/docker
Step #2: The push refers to repository [gcr.io/myproject/myapp]
Step #2: tag does not exist: gcr.io/myproject/myapp:latest
</code></pre>
<p>I want to do this to be sure that my k8s deployment file is pointing to the latest image.</p>
<hr />
<p>UPDATE</p>
<p>I was able to do it</p>
<pre><code>substitutions:
  _IMG_NAME: &quot;myapp&quot;

steps:

- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/$PROJECT_ID/${_IMG_NAME}:$BRANCH_NAME-$REVISION_ID', '-t' , 'gcr.io/$PROJECT_ID/${_IMG_NAME}:latest', '.']
images: ['gcr.io/$PROJECT_ID/${_IMG_NAME}']
</code></pre>","<p>I think it may be possible to use/set a few tags when the docker image is created.
For example, something similar can be used:</p>
<pre><code>- name: 'gcr.io/cloud-builders/docker'
  env:
    - 'DOCKER_BUILDKIT=1'
  args:
    - build
    - --tag
    - gcr.io/$PROJECT_ID/myapp:$BRANCH_NAME-$REVISION_ID'
    - --tag
    - gcr.io/$PROJECT_ID/myapp:latest
    - .
</code></pre>
<p>I also would guess that one of either <code>docker push</code> or <code>image</code> should be enough in your case.</p>"
"Google Cloud Build - Summary status shows FAILURE, all steps succeded<p>I've setup a Google Cloud Build pipeline that'll build a docker image from a Dockerfile, test the image and push the image into Google Container Registry.</p>
<p>Upon running the pipeline I noticed that all defined steps passed with <code>SUCCESS</code> status but the build summary itself returned with <code>FAILURE</code> status even though I can see the image being produced into Google Container Registry.</p>
<p>I used following command to build the image</p>
<pre><code>gcloud builds submit --config cloudbuild.yml --gcs-log-dir 'gs://&lt;bucket&gt;' .
</code></pre>
<p>and below is the error message returned:</p>
<pre><code>ERROR: (gcloud.builds.submit) build www-xxxx-yyyy-zzzz completed with status &quot;FAILURE&quot;
 Error: The command exited with status 1
</code></pre>
<p>Is there any reason for the <code>gcloud builds submit</code> command to exit with code <code>1</code> as above if all the steps were marked as <code>SUCCESS</code>?</p>
<p>Below is some filtered log data taken from <code>gcloud builds describe</code> command for that specific build.</p>
<pre><code>steps:
- args:
  - build
  - -t
  - &lt;host&gt;/&lt;project/&lt;image&gt;:&lt;tag&gt;
  - .
  name: gcr.io/cloud-builders/docker
  status: SUCCESS
- args:
  - test
  - --image
  - &lt;host&gt;/&lt;project/&lt;image&gt;:&lt;tag&gt;
  - --config
  - test_config.yml
  - -o
  - json
  name: gcr.io/gcp-runtimes/container-structure-test
  status: SUCCESS
</code></pre>
<p>Below is Google Cloud Build setup:</p>
<pre><code># cloudbuild.yml

steps:
# Build the image
- name: 'gcr.io/cloud-builders/docker'
  args: [ 'build', '-t', '&lt;host&gt;/&lt;project/&lt;image&gt;:&lt;tag&gt;', '.' ]
# Test the image
- name: 'gcr.io/gcp-runtimes/container-structure-test'
  args: [ 
    'test',
    '--image',
    '&lt;host&gt;/&lt;project/&lt;image&gt;:&lt;tag&gt;',
    '--config',
    'test_config.yml',
    '-o',
    'json'
  ]

# Push the image
images: [ '&lt;host&gt;/&lt;project/&lt;image&gt;:&lt;tag&gt;' ]
</code></pre>","<p>I've finally resolved this issue with the assistance of Google Cloud support team.</p>
<p>They found out a <code>403 Permission Denied</code> error as the Cloud Build container trying to access Google Cloud Storage to delete a certain log object stored in the bucket, this error message is found at the back system of Cloud Build where users/clients have no access to. The <code>403 Permission Denied</code> error is the result of the object retention policy applied to the bucket.</p>
<p>In my case, I've replaced retention policy with lifecycle policy to resolve this issue and it worked. We do this as we consider keeping Cloud Build log size under control is our primary objective and, to prevent any accidental deletion/modification to the log file, we ended up with setting read-only access to the resources in the log bucket except for the service account used by Cloud Build.</p>"
"What is correct context for getting workspace maven build output into container build task?<p>This is similar to <a href=""https://stackoverflow.com/questions/48274288/passing-files-from-google-cloud-container-builder-to-docker-build-task"">Passing files from Google Cloud Container Builder to Docker build task</a> but I can't seem to figure out what the difference is.</p>
<p>I am attempting to build a simple Java program and package it into a Container using Google Cloud Build. I am following mostly along with <a href=""https://cloud.google.com/build/docs/building/build-java"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/building/build-java</a> but using my own repo which is a fork of <a href=""https://github.com/jchraibi/cloud-native-workshop"" rel=""nofollow noreferrer"">https://github.com/jchraibi/cloud-native-workshop</a></p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: 'maven:3-openjdk-11'
    args:
      - test
      - '-f=code/inventory-quarkus/pom.xml'
    entrypoint: mvn
  - name: 'maven:3-openjdk-11'
    args:
      - package
      - '-f=code/inventory-quarkus/pom.xml'
      - '-Dmaven.test.skip=true'
    entrypoint: mvn
  - name: 'gcr.io/cloud-builders/docker'
    args: 
      - 'build'
      - '-t'
      - 'gcr.io/$PROJECT_ID/inventory-quarkus'
      - '--build-arg=JAR_FILE=workspace/code/inventory-quarkus/target/$_BUILD_ARTIFACT'
      - '-f' 
      - 'code/inventory-quarkus/gcp/Dockerfile'
      - '/workspace'
</code></pre>
<p>The above are the build steps. When the maven package completes, the output is as follows:</p>
<pre><code>Step #1: [INFO] [io.quarkus.deployment.pkg.steps.JarResultBuildStep] Building thin jar: /workspace/code/inventory-quarkus/target/inventory-quarkus-1.0.0-SNAPSHOT-runner.jar
</code></pre>
<p>However, no matter what combination of copying things and contexts I attempt, I can't seem to find <code>inventory-quarkus-1.0.0-SNAPSHOT-runner.jar</code> anywhere in the filesystem of the Docker build. Note that the Dockerfile lives in a different place than where the target of the build seems to end up.</p>
<pre><code>FROM openjdk:11
ARG JAR_FILE=JAR_FILE_MUST_BE_SPECIFIED_AS_BUILD_ARG
RUN find .
COPY ${JAR_FILE} app.jar
ENTRYPOINT [&quot;java&quot;, &quot;-Djava.security.edg=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;]
</code></pre>
<p>In the logs, interestingly, the output of the <code>find</code> command doesn't have a workspace folder at all. I'm sure this has something to do with the fact that the <code>Dockerfile</code> is not in the root of the repo, but I just can't find the right invocation.</p>
<p>The link at the beginning of this post to the other thread ended up using an extra step with the <code>git</code> image to do some kind of copy, but I don't see why that would be required given all of the Google examples don't have to go through any hoops like that.</p>
<p>The <a href=""https://cloud.google.com/build/docs/building/build-containers"" rel=""nofollow noreferrer"">build containers</a> documentation has the following statement:</p>
<blockquote>
<p>If your Dockerfile and source code are in different directories, add -f and the path to the Dockerfile to the list of arguments in the args field:</p>
</blockquote>
<p>This is definitely what I'm doing in my build step. It's clearly finding the Dockerfile, because it's processing the find and the copy. I just can't seem to find where/how to get the Maven package step's workspace folder into the builder, or what the source path would be to copy the file from it.</p>","<p>Thank you for your question!  I cloned your repo and added a <code>cloudbuild.yaml</code> at the root and added a <code>Dockerfile</code> in the <code>inventory-quarkus/src/main/docker</code> directory.  I'm sure this isn't exactly the repo structure you're working with, but the concept should carry over.</p>
<p>Essentially, you want to use the <a href=""https://cloud.google.com/build/docs/build-config#dir"" rel=""nofollow noreferrer""><code>dir</code> field</a> to set your working directory between the steps to more easily pass the data around.  This cloudbuild.yaml worked for me:</p>
<pre><code>steps:
  # Test
  - name: 'maven:3-openjdk-11'
    entrypoint: mvn
    args:
      - test
      - '-f=labs/inventory-quarkus/pom.xml'

  # Package
  - name: 'maven:3-openjdk-11'
    entrypoint: mvn
    dir: 'labs/inventory-quarkus'
    args:
      - package 
      - -f=pom.xml 
      - -Dmaven.test.skip=true

  # Docker Build
  - name: 'gcr.io/cloud-builders/docker'
    dir: 'labs/inventory-quarkus'
    args: 
      - 'build'
      - '-t'
      - 'gcr.io/$PROJECT_ID/inventory-quarkus'
      - '--build-arg=JAR_FILE=target/inventory-quarkus-1.0.0-SNAPSHOT-runner.jar'
      - '-f' 
      - 'src/main/docker/Dockerfile'
      - '.'
</code></pre>
<p>I also want to draw your attention to the fact that we now have <a href=""https://cloud.google.com/artifact-registry/docs"" rel=""nofollow noreferrer"">Artifact Registry</a>, if you wanted to store the JAR there and pull it down from the cloud.  You can also use it to store you Docker image.</p>"
"Gsutil not found with Terraform and Cloudbuild<p>I have a problem with my plan terraform in using cloud build. I cannot use gsutil command in a module terraform, I have an error :</p>
<pre><code>Error: Error running command 'gsutil -m rsync -d -r ../../../sources/composer gs://toto/dags/': exit status 127. Output: /bin/sh: gsutil: not found
</code></pre>
<p>My cloudbuild.yaml :</p>
<pre><code>steps:
- id: 'branch name'
  name: 'alpine'
  entrypoint: 'sh'
  args:
    - '-c'
    - |
      echo &quot;***********************&quot;
      echo &quot;$BRANCH_NAME&quot;
      echo &quot;***********************&quot;
...
# [START tf-apply]
- id: 'tf apply'
  name: 'hashicorp/terraform:0.15.0'
  entrypoint: 'sh'
  args:
    - '-c'
    - |
      if [ -d &quot;terraform/environments/$BRANCH_NAME/&quot; ]; then
        cd terraform/environments/$BRANCH_NAME      
        terraform apply -auto-approve
      else
        echo &quot;***************************** SKIPPING APPLYING *******************************&quot;
        echo &quot;Branch '$BRANCH_NAME' does not represent an oficial environment.&quot;
        echo &quot;*******************************************************************************&quot;
      fi
# [END tf-apply]
timeout: 3600s
</code></pre>
<p>My module to put files in gcs :</p>
<pre><code>resource &quot;null_resource&quot; &quot;upload_folder_content&quot; {
  provisioner &quot;local-exec&quot; {
    command = &quot;gsutil -m rsync -d -r ${var.dag_folder_path} ${var.composer_dag_gcs}/&quot;
  }
}
</code></pre>","<p>As you are using the Hashicorp's Terraform image in your step, it is to be expected that <code>gsutil</code> it's not included by default and as such you're unable to run that command that your null_resource is defining opposed to what you'd be able to do on your local environment.</p>
<p>In order to overcome that, you could build your own custom image and <a href=""https://cloud.google.com/container-registry/docs/pushing-and-pulling"" rel=""nofollow noreferrer"">push</a> it to Google Container Registry so you're able to use it afterwards. With that option you will also have more flexibility as you could install whatever dependency your Terraform code has.</p>"
"Cannot pull golang image in CloudBuild<p>I've created a simple go server and am following <a href=""https://cloud.google.com/go/getting-started/getting-started-on-compute-engine#use-cloud-build-to-build-the-app"" rel=""nofollow noreferrer"">documentation</a> to deploy the server on GCE. But I am getting the following error on my build. What am I missing? I've also tried using a specific version number (i.e. &quot;1.16&quot;), but still fails with a similar error message.</p>
<pre><code>Starting Step #0
Step #0: Pulling image: mirror.gcr.io/library/golang
Step #0: Using default tag: latest
Step #0: Error response from daemon: manifest for mirror.gcr.io/library/golang:latest not found: manifest unknown: Failed to fetch &quot;latest&quot; from request &quot;/v2/library/golang/manifests/latest&quot;.
...
Step #0: Error response from daemon: manifest for mirror.gcr.io/library/golang:latest not found: manifest unknown: Failed to fetch &quot;latest&quot; from request &quot;/v2/library/golang/manifests/latest&quot;.
ERROR: failed to pull because we ran out of retries.
ERROR
ERROR: build step 0 &quot;mirror.gcr.io/library/golang&quot; failed: error pulling build step 0 &quot;mirror.gcr.io/library/golang&quot;: generic::unknown: retry budget exhausted (10 attempts): step exited with non-zero status: 1
</code></pre>","<p>For some reason the <code>golang</code> image isn't available at the moment via this registry. It could be an intermittent issue ‍♂️</p>
<pre><code>$ docker pull mirror.gcr.io/library/golang
Using default tag: latest
Error response from daemon: manifest for mirror.gcr.io/library/golang:latest not found: manifest unknown: Failed to fetch &quot;latest&quot; from request &quot;/v2/library/golang/manifests/latest&quot;.

// But weirdly this works
gcloud container images list --repository=mirror.gcr.io/library
// And this
docker pull mirror.gcr.io/library/alpine
</code></pre>
<p><strong>So I would swap that line with the Golang Docker Hub image in your yaml file.</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>## Where it says:
- name: 'mirror.gcr.io/library/golang'
## Change to
- name: 'registry.hub.docker.com/library/golang'
</code></pre>"
"Automatically trigger a Cloud Build once it is created<p>I am deploying a series of Cloud Build Triggers through Terraform, but I also want Terraform to trigger once every deployed Cloud Build so that it can do the initial deployment.</p>
<p>The Cloud Build Triggers are used to deploy Cloud Functions (and also Cloud Run and maybe Workflows). We could deploy the functions in the Terraform but we want to keep the command easy to modify so we don't want to duplicate it on both Terraform and the Cloud Build config.</p>","<p>It's important for the clarity and the evolutivity/maintainability of your pipeline to separate clearly the concern of each step.</p>
<ul>
<li>You have a (set of) step to deploy the infrastructure of your project (here, your terraform)</li>
<li>You have a (set of) step that run process on your project (can be an Ansible script on VM, trigger Cloud Functions, Cloud Run, or a Cloud Build trigger).</li>
</ul>
<p>I'm pretty sure that you can add this trigger in Terraform, but I strongly don't recommend you to do this.</p>
<hr />
<p><strong>Edit 1</strong></p>
<p>I wasn't clear. You have to run your trigger by API after the terraform deployment, in your main pipeline. Then, the subsequent trigger will be done by Push to the Git repository.</p>"
"Cloud Build -> Google Cloud Storage: Question about downtimes at deployment time<pre><code>- name: 'google/cloud-sdk:alpine'
  entrypoint: 'gsutil'
  args: ['-m', 'rsync', '-r', '-d', '-p', 'dist/', 'gs://my-site-frontend']
</code></pre>
<p>Good morning, the snippet above is the command that, via Google Cloud Build, copies the build of my VueJS frontend to a Google Cloud Storage bucket, where the website will be hosted.</p>
<p>My question is simple and short: <strong>If any user is browsing at the time of this deployment (the execution of the command above), will he notice any inconsistencies, downtime or something like that when Cloud Build is copying/syncing the new files via rsync?</strong> Is this task seamless enough? Maybe the user can feel some inconsistence when accessing some file that is being copied? Should I use Cloud Run instead?</p>","<p>Yes, you can have inconsistency for a while (files outdated or not found). The best solution is to use a product that package in a consistent manner the sources. You can use Cloud Run, but you can also use App Engine standard for that.</p>
<p>The main advantage of this 2 solutions is that each version is unitary, package in the same container. Like that, you can easily perform rollback, traffic splitting, canary release, A/B testing,.... All these things are impossible with Cloud Storage.</p>"
"How to Create a Slim Docker Image for Cloud Build with Bazel, Kubectl and Gcloud Installed?<p>I want to run a <a href=""https://github.com/bazelbuild/rules_k8s"" rel=""nofollow noreferrer"">k8s_objects</a> Bazel rule from a Google Cloud Build step, which deploys new configurations to my cluster. Hence I need a Docker image with</p>

<ul>
<li><strong>bazel</strong> (obviously for running bazel targets) </li>
<li><strong>kubectl</strong> (for applying new configuration to my cluster)</li>
<li><strong>gcloud</strong> (for authenticating to my Google Kubernetes Engine cluster)</li>
</ul>

<p>installed. Currently I have <a href=""https://github.com/flolude/cents-ideas/blob/9f0344b4ba03b8f66205618122efe5a2fede2f84/docker/slim-bazel-kubectl.Dockerfile"" rel=""nofollow noreferrer"">this huge docker image</a> which is 1GB in size and probably cluttered with stuff I don't need. Therefore my build times skyrocket to 10 minutes and more.</p>

<p>This is how my <code>cloudbuild.yaml</code> looks like:</p>

<pre><code>steps:
  - name: eu.gcr.io/cents-ideas/slim-bazel-kubectl
    entrypoint: /bin/sh
    args:
      - -c
      - |
        gcloud container clusters get-credentials cents-ideas --zone europe-west3-a --project cents-ideas
</code></pre>

<h3>What would a small docker image with only the packages I need look like?</h3>","<p>Currently I use the image below. However it doesn't have <code>kubectl</code> installed</p>

<pre><code>FROM ubuntu:18.04

RUN apt-get update
RUN apt-get -y install curl gnupg unzip python python3 git build-essential

# nodejs
RUN apt-get -y install nodejs

# yarn
RUN curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add - &amp;&amp; \
  echo ""deb https://dl.yarnpkg.com/debian/ stable main"" | tee /etc/apt/sources.list.d/yarn.list &amp;&amp; \
  apt-get update &amp;&amp; apt-get -y install yarn

# bazelisk
RUN yarn global add @bazel/bazelisk --prefix /usr/local &amp;&amp; bazelisk version

WORKDIR /app

ENTRYPOINT [ ""bazelisk"" ]
</code></pre>"
"Cloud Build trigger changes file permissions and breaks docker caching<p>I am using <code>google-cloud-build</code> as CI to test if a PR breaks the build or not.</p>

<p>The build is basically creating a Docker image.
To reduce the build time, I am trying to use dockers <code>--cache-from</code> feature, but it fails for me on a <code>COPY ...</code> because when using a <code>Github App trigger</code>, most file permissions are changed for some reason.
When using a <code>Github trigger</code>, this issue does not happen, but I cannot trigger it on a PR as stated <a href=""https://cloud.google.com/cloud-build/docs/automating-builds/create-github-app-triggers"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Is there a way to prevent from cloud build to change file permissions when using a <code>Github App trigger</code>? is there another way to solve this?</p>","<p>For now, we decided that this is fine and if we want specific file permissions on a file, we will manually set them in the docker file using <code>RUN chmod ...</code></p>"
"How to create trigger with multiple substitutions variable in Google Cloud build with Python<p>I am working on Python code to create Google Cloud trigger, I am not able to add substitutions variable.</p>

<p>Currently I have below code</p>

<pre><code>from google.cloud.devtools import cloudbuild_v1

client = cloudbuild_v1.CloudBuildClient()

build_trigger_template = cloudbuild_v1.types.BuildTrigger()

build_trigger_template.description = 'test to create trigger'
build_trigger_template.name = 'github-cloudbuild-trigger1'
build_trigger_template.github.name = 'github-cloudbuild'
build_trigger_template.github.pull_request.branch = 'master'
build_trigger_template.filename = 'cloudbuild.yaml'

response = client.create_build_trigger('dev',
                                       build_trigger_template)
</code></pre>

<p>I want to add two substitutions variables _ENV and _PROJECT, I tried below mentioned way but not working.</p>

<pre><code>build_trigger_template.substitutions = {'_ENV': 'test',
                                        '_PROJECT': 'pro-test'}
</code></pre>

<p><strong>Error: AttributeError: Assignment not allowed to repeated field ""substitutions"" in protocol message object.</strong></p>

<p>Thanks,</p>

<p>Raghunath.</p>","<p>This is an issue with assigning protobuf object. </p>

<p>If you look on the object using <code>dir(build_trigger_template.substitutions)</code></p>

<p>you'll find an <code>.update</code> method that will accept a dictionary.</p>

<p>so try the below, it should return <code>None</code> but your structure will be updated.</p>

<p><code>build_trigger_template.substitutions.update({'_ENV': 'test',
                                        '_PROJECT': 'pro-test'})</code></p>"
"How to add custom deploy script in Google App Engine?<p>I would like to deploy a php application to GAE. During the deployment though, I need to run custom scripts (Or add few commands). Is there a way to define scripts in app.yaml?</p>","<p>If you need to run custom scripts or commands before deploying to GAE, then the best approach would be using a custom runtime.</p>

<p>You will have to build a Dockerfile with a PHP base image and then run all your needed steps there. With this approach you will also be able to change the default entrypoint for the docker container.</p>

<p>The related documentation can be found in this <a href=""https://cloud.google.com/appengine/docs/flexible/custom-runtimes/build"" rel=""nofollow noreferrer"">link</a>.</p>

<p>Google also offers several <a href=""https://cloud.google.com/appengine/docs/flexible/custom-runtimes/build#base"" rel=""nofollow noreferrer"">images</a>, included a PHP one.</p>"
"How do i set up a cloud function for clouldbuild in vue?<p>I have set up a Vue project and initialized firebase functions (using Firebase CLI). Created a function that works fine when deployed from my local machine to the cloud (both with ""firebase Deploy"" and ""firebase deploy --only functions""). The issue arises during cloud build (during CI/CD pipeline). I get a  ""sh: 1: eslint: not found"" error in the build log. The Vue project structure looks like this;</p>

<p><a href=""https://i.stack.imgur.com/Ll06S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ll06S.png"" alt=""view file structure""></a><a href=""https://i.stack.imgur.com/bhd3x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bhd3x.png"" alt=""enter image description here""></a></p>

<p>The build is triggered by commits to the master... the build config is as follows;</p>

<pre><code>steps:
# Install
- name: 'gcr.io/cloud-builders/npm'
  args: ['install']
# Build
- name: 'gcr.io/cloud-builders/npm'
  args: ['run', 'build', '--prod']
# Deploy
- name: 'gcr.io/$PROJECT_ID/firebase'
  args: ['deploy']
</code></pre>

<p>The error occurs at the deploy step... the full build log is as follows;</p>

<pre><code>Finished Step #1
Starting Step #2
Step #2: Pulling image: gcr.io/covid-info-bw/firebase
Step #2: Using default tag: latest
Step #2: latest: Pulling from covid-info-bw/firebase
Step #2: c0c53f743a40: Already exists
Step #2: 66997431d390: Already exists 
Step #2: 0ea865e2909f: Already exists 
Step #2: 584bf23912b7: Already exists
Step #2: 3c4c73959f29: Already exists
Step #2: 63e05266fc4b: Already exists
Step #2: 7b37ba8cd979: Already exists
Step #2: 3a18f94fe18a: Already exists
Step #2: a000f3263f8b: Already exists
Step #2: 3a5d0859c8ef: Pulling fs layer
Step #2: 575701571da4: Pulling fs layer
Step #2: 8e3be3979b6a: Pulling fs layer
Step #2: 8e3be3979b6a: Verifying Checksum
Step #2: 8e3be3979b6a: Download complete
Step #2: 575701571da4: Verifying Checksum
Step #2: 575701571da4: Download complete
Step #2: 3a5d0859c8ef: Verifying Checksum
Step #2: 3a5d0859c8ef: Download complete
Step #2: 3a5d0859c8ef: Pull complete
Step #2: 575701571da4: Pull complete
Step #2: 8e3be3979b6a: Pull complete
Step #2: Digest: sha256:35d71d1c92b972de31f223e63fd25f1be6c419f28b24c106187139c9aa3e6cfa
Step #2: Status: Downloaded newer image for gcr.io/covid-info-bw/firebase:latest
Step #2: gcr.io/covid-info-bw/firebase:latest
Step #2: 
Step #2: [1m[37m===[39m Deploying to 'covid-info-bw'...[22m
Step #2: 
Step #2: [1m[36mi [39m[22m deploying [1mfunctions, hosting[22m
Step #2: Running command: npm --prefix ./functions run lint
Step #2: 
Step #2: &gt; functions@ lint /workspace/functions
Step #2: &gt; eslint .
Step #2: 
Step #2: sh: 1: eslint: not found
Step #2: npm ERR! code ELIFECYCLE
Step #2: npm ERR! syscall spawn
Step #2: npm ERR! file sh
Step #2: npm ERR! errno ENOENT 
Step #2: npm ERR! functions@ lint: `eslint .`
Step #2: npm ERR! spawn ENOENT
Step #2: npm ERR! 
Step #2: npm ERR! Failed at the functions@ lint script.
Step #2: npm ERR! This is probably not a problem with npm. There is likely additional logging output 
above.
Step #2: npm WARN Local package.json exists, but node_modules missing, did you mean to install? 
Step #2: 
Step #2: npm ERR! A complete log of this run can be found in:
Step #2: npm ERR!     /builder/home/.npm/_logs/2020-04-16T23_28_19_649Z-debug.log
Step #2: 
Step #2: [1m[31mError:[39m[22m functions predeploy error: Command terminated with non-zero exit 
code1
Finished Step #2
ERROR
ERROR: build step 2 ""gcr.io/covid-info-bw/firebase"" failed: step exited with non-zero status: 1
</code></pre>

<p>Firebase.json snippet is as follows;</p>

<pre><code> ...
 ""functions"": {
  ""predeploy"": [
    ""npm --prefix ./functions run lint""
   ]
 }
 ...
</code></pre>

<p>This is a link to my <a href=""https://github.com/Abel-Moremi/covid-info-bw"" rel=""nofollow noreferrer"">Repo</a> just for reference</p>","<p>The error was caused by missing scripts because the cloud function dependencies were not being installed in cloud build. Basically, the cloud build's step to install cloud function dependencies were missing. Below is a corrected CloudBuild.yaml (take note of step 2)</p>

<pre><code>steps:
# Install the vue-app dependencies
- name: 'gcr.io/cloud-builders/npm'
  args: ['install']
# Install the function dependencies
- name: 'gcr.io/cloud-builders/npm'
  dir: 'functions'
  args: ['install']
# Build
- name: 'gcr.io/cloud-builders/npm'
  args: ['run', 'build', '--prod']
# Deploy
- name: 'gcr.io/$PROJECT_ID/firebase'
  args: ['deploy']
</code></pre>

<p>The second (Install the function dependencies) step is the one that's been added, <code>dir: 'functions'</code> is how you access functions directory to install the dependencies. This <a href=""https://itnext.io/vue-firebase-google-cloud-how-to-set-up-your-ci-cd-pipeline-3b309ef37015"" rel=""nofollow noreferrer"">article</a> shows how to set up a CI/CD pipeline using Google Cloud Build. It helped me realize my mistake in not placing that step. The article is accompanied by this <a href=""https://github.com/llewellyncollins/vue_gcp_ci_app"" rel=""nofollow noreferrer"">repo</a>.</p>"
"how to build a yaml file to run my python code<p>Hi everyone and sorry for the silly question but its my day 2 with Yaml. </p>

<ul>
<li>Problem statement: 
I have a python code which runs for 12 mins, (so I cant use Cloud Function to automate it), hence using cloud build as a hack. </li>
<li>Steps done so far: 
I have my code in google cloud repository and I used cloud build to build an image and created a google cloud build trigger. Now I want to run the main.py python code each time I trigger the build ( which I will do using a Cloud Scheduler as described <a href=""https://stackoverflow.com/questions/61682555/how-to-trigger-a-google-cloud-build-job-periodically-from-google-cloud-scheduler"">here</a></li>
<li>Folder structure (as shown in the image below)</li>
</ul>

<p><a href=""https://i.stack.imgur.com/MirWX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MirWX.png"" alt=""Folder structure in cloud repo""></a></p>

<ul>
<li>cloudbuild.yaml which I managed to write so far </li>
</ul>

<pre><code>steps:
- name: 'gcr.io/$PROJECT_ID/p2p-cloudbuild' 
  entrypoint: '/bin/bash'
  args: ['-c','virtualenv /workspace/venv' ]
  # Create a Python virtualenv stored in /workspace/venv that will persist across container runs.

- name: 'gcr.io/$PROJECT_ID/p2p-cloudbuild' 
  entrypoint: 'venv/bin/pip'
  args: ['install', '-V', '-r', 'requirements.txt']
  # Installs any dependencies listed in the project's requirements.txt.
</code></pre>

<p>Question : How do I add the step to call/execute 'my_function' inside main.py file ?</p>

<p>appreciate your help. </p>","<pre><code>steps:
- name: 'gcr.io/$PROJECT_ID/p2p-cloudbuild' 
  entrypoint: '/bin/bash'
  args: ['-c','virtualenv /workspace/venv' ]
  # Create a Python virtualenv stored in /workspace/venv that will persist across container runs.

- name: 'gcr.io/$PROJECT_ID/p2p-cloudbuild' 
  entrypoint: 'venv/bin/pip'
  args: ['install', '-V', '-r', 'requirements.txt']
  # Installs any dependencies listed in the project's requirements.txt.
</code></pre>

<p>Let's say I have a file <code>main.py</code>:</p>

<pre><code>def foo():
  return ""bar""
</code></pre>

<p>This actually can be simplified into:</p>

<pre><code>- name: 'gcr.io/$PROJECT_ID/p2p-cloudbuild' 
  entrypoint: '/bin/bash'
  args:
    - '-c'
    - |
      virtualenv /workspace/venv
      source /workspace/venv/bin/activate
      pip install -V -r requirements.txt
      python -c 'from main import foo; print (foo())'
</code></pre>"
"Could you please verify my cloudbuild.yaml file?<p>I am new to setup CI CD pipeline for my nodejs application using GCP cloud build. Below are the steps that I have done:</p>
<ol>
<li>Writing the cloudbuild.yaml file. Done</li>
<li>Mirrored the bitbucket repository in my GCP cloud source repository. Done</li>
<li>Configured the Webhook triggers in my GCP. Done</li>
<li>When I push the code, it gets triggered. Done</li>
</ol>
<p>Cloudbuild.yaml</p>
<pre><code>steps:
- name: &quot;node:8.13&quot;
  args: [&quot;npm&quot;, &quot;install&quot;]
- name: &quot;node:8.13&quot;
  args: [&quot;npm&quot;, &quot;test&quot;]
- name: &quot;node:8.13&quot;
  args: [&quot;npm&quot;, &quot;run&quot;, &quot;build&quot;]
- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  args: ['functions', 'deploy', '[job_scheduling]', '--source=.', '--runtime', '[nodejs8.17]', '--trigger-http', '--allow-unauthenticated', '--entry-point=App']
</code></pre>
<p>package.json:</p>
<pre><code>{
  &quot;name&quot;: &quot;groups&quot;,
  &quot;version&quot;: &quot;1.0.0&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;main&quot;: &quot;src/index.js&quot;,
  &quot;scripts&quot;: {
    &quot;test&quot;: &quot;mocha test/GET.js test/POST.js test/PUT.js test/DELETE.js&quot;,
    &quot;coverage&quot;:&quot;nyc --reporter=text --reporter=lcov mocha test/GET.js test/POST.js test/PUT.js test/DELETE.js&quot;
  },
  &quot;author&quot;: &quot;&quot;,
  &quot;license&quot;: &quot;ISC&quot;,
  &quot;dependencies&quot;: {
    &quot;cors&quot;: &quot;^2.8.5&quot;,
    &quot;express&quot;: &quot;^4.17.1&quot;,
    &quot;firebase-admin&quot;: &quot;^8.9.1&quot;,
    &quot;is-my-json-valid&quot;: &quot;^2.20.0&quot;
  },
  &quot;devDependencies&quot;: {
    &quot;chai&quot;: &quot;^4.2.0&quot;,
    &quot;mocha&quot;: &quot;^7.0.1&quot;,
    &quot;nyc&quot;: &quot;^15.0.0&quot;,
    &quot;supertest&quot;: &quot;^4.0.2&quot;
  }
}
</code></pre>
<p>When I push the code in my bitbucket repository, the build gets triggered and I get the below logs:</p>
<pre><code>starting build &quot;43d640c7-159e-45d2-9d69-af1c03892f8e&quot;

FETCHSOURCE
Initialized empty Git repository in /workspace/.git/
 * branch            5fa3b874c8929f410f3249eedeab8 -&gt; FETCH_HEAD
HEAD is now at 5f57724 cloudbuild.yaml edited online with Bitbucket
BUILD
Starting Step #0
Step #0: Pulling image: node:8.13
Step #0: 8.13: Pulling from library/node
Step #0: 54f7e8ac135a: Pulling fs layer
Step #0: d6341e30912f: Pulling fs layer
Step #0: 087a57faf949: Pulling fs layer
Step #0: 5d71636fb824: Pulling fs layer
Step #0: 0c1db9598990: Pulling fs layer
Step #0: 89669bc2deb2: Pulling fs layer
Step #0: 4c84cd37194c: Pulling fs layer
Step #0: a8061553ef43: Pulling fs layer
Step #0: 5d71636fb824: Waiting
Step #0: 0c1db9598990: Waiting
Step #0: 89669bc2deb2: Waiting
Step #0: 4c84cd37194c: Waiting
Step #0: a8061553ef43: Waiting
Step #0: 087a57faf949: Verifying Checksum
Step #0: 087a57faf949: Download complete
Step #0: d6341e30912f: Verifying Checksum
Step #0: d6341e30912f: Download complete
Step #0: 54f7e8ac135a: Verifying Checksum
Step #0: 54f7e8ac135a: Download complete
Step #0: 89669bc2deb2: Verifying Checksum
Step #0: 89669bc2deb2: Download complete
Step #0: 5d71636fb824: Verifying Checksum
Step #0: 5d71636fb824: Download complete
Step #0: a8061553ef43: Verifying Checksum
Step #0: a8061553ef43: Download complete
Step #0: 4c84cd37194c: Verifying Checksum
Step #0: 4c84cd37194c: Download complete
Step #0: 0c1db9598990: Verifying Checksum
Step #0: 0c1db9598990: Download complete
Step #0: 54f7e8ac135a: Pull complete
Step #0: d6341e30912f: Pull complete
Step #0: 087a57faf949: Pull complete
Step #0: 5d71636fb824: Pull complete
Step #0: 0c1db9598990: Pull complete
Step #0: 89669bc2deb2: Pull complete
Step #0: 4c84cd37194c: Pull complete
Step #0: a8061553ef43: Pull complete
Step #0: Digest: sha256:3ecf259bf23f8a75555cf0b92ac5ad3c746e7e2262e937a9bed9ca1685f8e1c7
Step #0: Status: Downloaded newer image for node:8.13
Step #0: docker.io/library/node:8.13
Step #0: npm WARN saveError ENOENT: no such file or directory, open '/workspace/package.json'
Step #0: npm notice created a lockfile as package-lock.json. You should commit this file.
Step #0: npm WARN enoent ENOENT: no such file or directory, open '/workspace/package.json'
Step #0: npm WARN workspace No description
Step #0: npm WARN workspace No repository field.
Step #0: npm WARN workspace No README data
Step #0: npm WARN workspace No license field.
Step #0: 
Step #0: up to date in 0.447s
Step #0: found 0 vulnerabilities
Step #0: 
Finished Step #0
Starting Step #1
Step #1: Already have image: node:8.13
Step #1: npm ERR! path /workspace/package.json
Step #1: npm ERR! code ENOENT
Step #1: npm ERR! errno -2
Step #1: npm ERR! syscall open
Step #1: npm ERR! enoent ENOENT: no such file or directory, open '/workspace/package.json'
Step #1: npm ERR! enoent This is related to npm not being able to find a file.
Step #1: npm ERR! enoent 
Step #1: 
Step #1: npm ERR! A complete log of this run can be found in:
Step #1: npm ERR!     /builder/home/.npm/_logs/2020-04-15T18_49_03_616Z-debug.log
Finished Step #1
ERROR
ERROR: build step 1 &quot;node:8.13&quot; failed: step exited with non-zero status: 254
</code></pre>
<p>Please help me solve this issue.</p>
<p>Update:
When I run your code:</p>
<pre><code>steps:
- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  entrypoint: &quot;ls&quot;
  args: [&quot;-la&quot;, &quot;/workspace&quot;]
</code></pre>
<p>Below are the logs:</p>
<pre><code>[![starting build &quot;363526b2-37f7-4b5a-9ee6-599db1442933&quot;

FETCHSOURCE
Initialized empty Git repository in /workspace/.git/
 * branch            98a8af215a8e683f94558a6351af60512b6fa1d7 -&gt; FETCH_HEAD
HEAD is now at 98a8af2 cloudbuild.yaml edited online with Bitbucket
BUILD
Already have image (with digest): gcr.io/cloud-builders/gcloud
total 20
drwxr-xr-x  5 root root 4096 Apr 16 14:17 .
drwxr-xr-x  1 root root 4096 Apr 16 14:17 ..
drwxr-xr-x  8 root root 4096 Apr 16 14:17 .git
drwxr-xr-x 15 root root 4096 Apr 16 14:17 API
drwxr-xr-x  4 root root 4096 Apr 16 14:17 UI
PUSH
DONE][1]][1]
</code></pre>
  <img src=""https://i.stack.imgur.com/1ChZd.png"">
<p>I have attached the screenshot of my project structure.</p>","<p>You should have configured your trigger to use the <code>/api/groups/cloudbuild.yaml</code> file but the root of your git repository is <code>/</code></p>

<p>Update your <code>cloudbuild.yaml</code> steps by adding the <code>dir</code> param for setting the correct working directory</p>

<pre><code>- name: ""node:8.13""
  args: [""npm"", ""install""]
  dir: 'API/groups'
...
</code></pre>

<p>Full doc <a href=""https://cloud.google.com/cloud-build/docs/build-config"" rel=""nofollow noreferrer"">here</a></p>"
"How to set the environment variable in cloudbuild.yaml file?<p>I am trying to set GOOGLE_APPLICATION_CREDENTIALS. Is this correct way to set environment variable ? Below is my yaml file:</p>

<pre><code>steps:
- name: 'node:10.10.0'
  id: installing_npm
  args: ['npm', 'install']
  dir: 'API/system_performance'
- name: 'node:10.10.0'
  #entrypoint: bash
  args: ['bash', 'set GOOGLE_APPLICATION_CREDENTIALS=test/emc-ema-cp-d-267406-a2af305d16e2.json']
  id: run_test_coverage
  args: ['npm', 'run', 'coverage']
  dir: 'API/system_performance'
</code></pre>

<p>Please help me solve this.</p>","<p>You can use the <a href=""https://cloud.google.com/cloud-build/docs/build-config#env"" rel=""nofollow noreferrer""><code>env</code> step parameter</a></p>

<p>However, when you execute Cloud Build, <a href=""https://cloud.google.com/cloud-build/docs/securing-builds/set-service-account-permissions#what_is_the_service_account"" rel=""nofollow noreferrer"">the platform uses its own service account</a> (in the future, it will be possible to specify the service account that you want to use)</p>

<p>Thus, if you grant the Cloud Build service account with the correct role, you don't need to use a key file (which is committed in your Git repository, not a really good practice!)</p>"
"Google Cloud Build - source-context.json SHA mismatch<p>I have a Python 3 project which I am hosting on Google AppEngine Standard. Until a couple of days ago I was able to deploy normally (right since I did the initial setup in July 2019) until a couple of days ago. Now I get the following response:</p>

<pre><code>starting build ""abc""

FETCHSOURCE
BUILD
Starting Step #0 - ""fetcher""
Step #0 - ""fetcher"": Already have image (with digest): gcr.io/cloud-builders/gcs-fetcher
Step #0 - ""fetcher"": Fetching manifest gs://staging.my-project.appspot.com/ae/xxx/manifest.json.
Step #0 - ""fetcher"": Processing 312 files.
Step #0 - ""fetcher"": Failed to fetch gs://staging.my-project.appspot.com/xxx, will no longer retry: fetching ""gs://staging.my-project.appspot.com/xxx"" with timeout 1h0m0s to temp file ""/workspace/.download/staging.my-project.appspot.com-xxx"": source-context.json SHA mismatch, got ""xxx"", want ""yyy""
Step #0 - ""fetcher"": Failed to download at least one file. Cannot continue.
Finished Step #0 - ""fetcher""
ERROR
ERROR: build step 0 ""gcr.io/cloud-builders/gcs-fetcher"" failed: step exited with non-zero status: 1
</code></pre>

<p>Any idea why this would be happening and how to fix it?</p>

<p>P.S. I use the following command for deployment:</p>

<pre><code>gcloud --project my-project app deploy app.yaml
</code></pre>","<p>After conversation with Google engineers (<a href=""https://issuetracker.google.com/issues/154588981?pli=1"" rel=""noreferrer"">https://issuetracker.google.com/issues/154588981?pli=1</a>) the following worked:</p>

<ol>
<li>Remove the <code>source-context.json</code> file</li>
<li>Delete the bucket where the deployment files are, e.g. gs://staging.my-project.appspot.com</li>
<li>Deploy again</li>
</ol>

<p>If you need the <code>source-context.json</code> file, you can follow these steps: <a href=""https://www.google.com/url?q=https://cloud.google.com/debugger/docs/source-context&amp;sa=D&amp;usg=AFQjCNHMB7Dm_jISwG2AnpokQ7XN5GmLAw"" rel=""noreferrer"">https://www.google.com/url?q=https://cloud.google.com/debugger/docs/source-context&amp;sa=D&amp;usg=AFQjCNHMB7Dm_jISwG2AnpokQ7XN5GmLAw</a> </p>"
"What are the applicable 'cloudbuild.yaml' steps to deploy a Java App to Google App Engine Standard edition?<p>What are the applicable 'cloudbuild.yaml' steps to deploy a Java App to Google App Engine Standard edition?</p>

<p>I cannot find anything specific to this in the documentation.</p>

<p>I am trying:</p>

<pre><code>steps:
  - name: 'gcr.io/cloud-builders/mvn'
    args: [ 'install', '--settings', 'settings.xml' ]
  - name: 'gcr.io/cloud-builders/gcloud'
    args: [ 'app', 'deploy' ]
    timeout: '6m0s'
</code></pre>

<p>The first step succeeds but the second step fails with this less-than-helpful message:</p>

<pre><code>Already have image (with digest): gcr.io/cloud-builders/gcloud
ERROR: gcloud crashed (AttributeError): 'NoneType' object has no attribute 'endswith'
</code></pre>

<p>Should I use the maven deploy command 'mvn appengine:deploy' instead?</p>

<p>Thank you in advance.</p>

<hr>

<p>A second attempt:</p>

<pre><code>steps:
  - name: 'gcr.io/cloud-builders/mvn'
    args: [ '--define', 'skipTests', '--settings', 'settings.xml', 
'clean', 'package', 'appengine:deploy' ]
    timeout: '6m0s'
</code></pre>

<p>With this result:</p>

<pre><code>Execution default-cli of goal 
com.google.cloud.tools:appengine-maven-plugin:1.3.1:deploy 
failed: 
The Google Cloud SDK could not be found 
in the customary locations and no path was provided.
</code></pre>","<p>I found the answer at:</p>

<p><a href=""https://medium.com/@Leejjon_net/use-cloud-build-to-do-continuous-delivery-for-your-java-project-on-app-engine-3c59072547ca"" rel=""nofollow noreferrer"">https://medium.com/@Leejjon_net/use-cloud-build-to-do-continuous-delivery-for-your-java-project-on-app-engine-3c59072547ca</a></p>

<pre><code>steps:
  - id: 'Stage app using mvn appengine plugin'
    name: 'gcr.io/cloud-builders/mvn'
    args: [ '--define', 'skipTests', '--settings', 'settings.xml', 'package', 'appengine:stage' ]
  - id: 'Deploy to app engine'
    name: 'gcr.io/cloud-builders/gcloud'
    args: [ 'app', 'deploy', 'target/appengine-staging/app.yaml' ]

</code></pre>

<p>Also, make sure version of plug in is:</p>

<pre><code>   &lt;plugin&gt;
       &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt;
       &lt;artifactId&gt;appengine-maven-plugin&lt;/artifactId&gt;
       &lt;version&gt;2.2.0&lt;/version&gt;
   &lt;/plugin&gt;
</code></pre>"
"Connecting to a private subnet from a GCP service<p>I am trying to reach a server in a private subnet that only has an internal IP from GCP cloud build. Is there something I need to do in order to allow traffic to be routed from google services to my private subnets or see my private DNS zones? Thanks in advance for the help!</p>

<p><strong>Additional Details</strong>
I am trying to hit it via HTTP through an internal load balancer: Step #1 - ""Auth to Vault"": Error authenticating: Put <a href=""http://10.16.6.33:8200/v1/auth/gcp/login"" rel=""nofollow noreferrer"">http://10.16.6.33:8200/v1/auth/gcp/login</a>: dial tcp 10.16.6.33:8200: i/o timeout</p>","<p>Google Cloud Build is a Google service located in Google controlled VPCs. Your internal load balancer is located in your VPC which Google Cloud Build cannot access.</p>

<p>At this time, Google Cloud does not offer a ""VPC Connector"" supporting Cloud Build accessing resources in your VPC using <a href=""https://en.wikipedia.org/wiki/Private_network#Private_IPv4_addresses"" rel=""nofollow noreferrer"">RFC 1918</a> private addresses.</p>"
"Storing Artifacts From a Failed Build<p>I am running some screen diffing tests in one of my Cloud Build steps. The tests produce png files that I would like to view after the build, but it appears to upload artifacts on successful builds.</p>

<p>If my test fail, the process exits with a non-zero code, which results in this error:</p>

<pre><code>ERROR: build step 0 ""gcr.io/k8s-skaffold/skaffold"" failed: step exited with non-zero status: 1
</code></pre>

<p>Which further results in another error</p>

<pre><code>
ERROR: (gcloud.builds.submit) build a22d1ab5-c996-49fe-a782-a74481ad5c2a completed with status ""FAILURE""
</code></pre>

<p>And no artifacts get uploaded.</p>

<p>I added <code>|| true</code> after my tests, so it exits successfully, and the artifacts get uploaded.</p>

<p>I want to:</p>

<ul>
<li>A) Confirm that this behavior is expected</li>
<li>B) Know if there is a way to upload artifacts even if a step fails</li>
</ul>

<p>Edit:</p>

<p>Here is my <code>cloudbuild.yaml</code></p>

<pre><code>
options:
 machineType: 'N1_HIGHCPU_32'

timeout: 3000s

steps:

- name: 'gcr.io/k8s-skaffold/skaffold'
  env:
  - 'CLOUD_BUILD=1'
  entrypoint: bash
  args:
  - -x  # print commands as they are being executed
  - -c  # run the following command...
  - build/test/smoke/smoke-test.sh


artifacts:
  objects:
    location: 'gs://cloudbuild-artifacts/$BUILD_ID'
    paths: [
      '/workspace/build/test/cypress/screenshots/*.png'
    ]
</code></pre>","<p>Google Cloud Build doesn't allow us to upload artifacts (or run some steps ) if a build step fails. This is the expected behavior. </p>

<p>There is an already feature request created in <a href=""https://issuetracker.google.com/issues/128353446"" rel=""noreferrer"">Public Issue Tracker</a> to allow us to run some steps even though the build has finished or failed. Please feel free to star it to get all the related updates on this issue.</p>

<p>A workaround per now is as you mentioned using || true after the tests or use || exit 0 as mentioned in this <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/issues/253#issuecomment-381600420"" rel=""noreferrer"">Github issue</a>.</p>"
"Filter for google cloud build with a given commit_sha<p>I am trying to list Google Cloud builds and filter by <code>source.repo.commit_sha</code> as specified in the <a href=""https://cloud.google.com/cloud-build/docs/view-build-results#filtering_build_results_using_queries"" rel=""nofollow noreferrer"">Viewing build results</a> documentation, but my list is coming back with no items. I am using the following command:</p>

<pre><code>gcloud builds list --filter ""source.repo.commit_sha='${LONG_COMMIT_SHA}'""
</code></pre>

<p>I have tried using the short commit and the long commit SHA-1, but I am not getting any results. The SHA-1 value is the value from the commit that was pushed to github. I am using a trigger to initiate the build, the trigger works correctly.</p>

<p>I have search the internet for information about filtering with a given commit SHA-1, but I have been unable to find any useful information.</p>

<p>Can someone please help with a command to filter with a given commit SHA?</p>","<p>this looks like an Gcloud SDK issue I found a error report on this <a href=""https://issuetracker.google.com/155681468"" rel=""nofollow noreferrer"">Public issue tracker</a> about similar behavior with the filters.</p>

<p>I think that is better continue in the public issue tracker. </p>"
"ERROR: (gcloud.compute.instance-groups.managed.rolling-action.replace) Could not fetch resource<p>I am trying to setup healthchecks for vms in instances groups on google cloud, but whenever I try to build the project I got this error.</p>

<pre><code>Already have image (with digest): gcr.io/cloud-builders/gcloud
ERROR: (gcloud.compute.instance-groups.managed.rolling-action.replace) Could not fetch resource:
 - Required 'compute.healthChecks.use' permission for 'projects/MY-PROJECT-ID/global/healthChecks/hc'

</code></pre>

<p>Build complete successfully when I remove healthcheck.</p>

<p>Which permissions should I enable ? </p>","<p>Accordingly with <a href=""https://cloud.google.com/iam/docs/understanding-roles#compute-engine-roles"" rel=""nofollow noreferrer"">Compute Engine roles</a> you need either: </p>

<ul>
<li><code>roles/compute.loadBalancerAdmin</code> or </li>
<li><code>roles/compute.networkAdmin</code></li>
</ul>"
"How to check if the latest Cloud Run revision is ready to serve<p>I've been using Cloud Run for a while and the entire user experience is simply amazing!</p>

<p>Currently I'm using Cloud Build to deploy the container image, push the image to GCR, then create a new Cloud Run revision.
Now I want to call a script to purge caches from CDN after the latest revision is successfully deployed to Cloud Run, however <code>$ gcloud run deploy</code> command can't tell you if the traffic is started to pointing to the latest revision.</p>

<p>Is there any command or the event that I can subscribe to to make sure no traffic is pointing to the old revision, so that I can safely purge all caches?</p>","<p>@Dustin’s answer is correct, however ""status"" messages are an indirect result of <code>Route</code> configuration, as those things are updated separately (and you might see a few seconds of delay between them). The status message will still be able to tell you the Revision has been taken out of rotation if you don't mind this.</p>

<p>To answer this specific question (emphasis mine) using API objects directly:</p>

<blockquote>
  <p>Is there any command or the event that I can subscribe to to <strong>make sure no traffic is pointing to the old revision</strong>?</p>
</blockquote>

<p>You need to look at <code>Route</code> objects on the API. This is a Knative API (it's available on Cloud Run) but it doesn't have a <code>gcloud</code> command: <a href=""https://cloud.google.com/run/docs/reference/rest/v1/namespaces.routes"" rel=""nofollow noreferrer"">https://cloud.google.com/run/docs/reference/rest/v1/namespaces.routes</a></p>

<p>For example, assume you did 50%-50% traffic split on your Cloud Run service. When you do this, you’ll find your <code>Service</code> object (which you can see on Cloud Console → Cloud Run → YAML tab) has the following <code>spec.traffic</code> field:</p>

<pre><code>spec:
  traffic:
  - revisionName: hello-00002-mob
    percent: 50
  - revisionName: hello-00001-vat
    percent: 50
</code></pre>

<p>This is ""desired configuration"" but it actually might not reflect the status definitively. Changing this field will go and update <code>Route</code> object –which decides how the traffic is splitted.</p>

<p>To see the <code>Route</code> object under the covers (sadly I'll have to use curl here because no <code>gcloud</code> command for this:)</p>

<pre><code>TOKEN=""$(gcloud auth print-access-token)""

curl -vH ""Authorization: Bearer $TOKEN"" \
    https://us-central1-run.googleapis.com/apis/serving.knative.dev/v1/namespaces/GCP_PROJECT/routes/SERVICE_NAME
</code></pre>

<p>This command will show you the output:</p>

<pre><code>  ""spec"": {
    ""traffic"": [
      {
        ""revisionName"": ""hello-00002-mob"",
        ""percent"": 50
      },
      {
        ""revisionName"": ""hello-00001-vat"",
        ""percent"": 50
      }
    ]
  },
</code></pre>

<p>(which you might notice is identical with Service’s <code>spec.traffic</code> –because it's copied from there) that can tell you definitively which revisions are currently serving traffic for that particular Service.</p>"
"How to trigger a Google Cloud Build job periodically from Google cloud scheduler<p>I configured a cloud schedular job following <a href=""https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.triggers/run"" rel=""nofollow noreferrer"">enter link description here</a></p>

<p>But find error <code>status: ""INVALID_ARGUMENT""</code>. did not understand from where it came from. any suggestion to fix this, would be grateful. </p>","<p>You need to follow the bellow steps to trigger:</p>

<p>1.- Create a new <a href=""https://cloud.google.com/iam/docs/creating-managing-service-accounts"" rel=""nofollow noreferrer"">Service Account</a> and add the ""Cloud Build Service Account"" and ""Cloud Scheduler Service Agent"" roles to it.</p>

<p>2.- The HTTP method should be ""post"".</p>

<p>3.- You must specify in the body field the ""repoName"" and the ""branchName"". Use the below as example.</p>

<pre><code>{
  ""repoName"": ""MyRepo"",
  ""branchName"": ""MyBranch""
}
</code></pre>

<p>4.- Select ""Add OAuth token"" as Auth header.</p>

<p>5.-Assign the created SA to your Cloud Scheduler Job that want to use to trigger your cloud Build job. </p>

<p>6.-Use this value ""<a href=""https://www.googleapis.com/auth/cloud-platform"" rel=""nofollow noreferrer"">https://www.googleapis.com/auth/cloud-platform</a>"" as Scope</p>

<p>Once you have these changes, you will be able to execute the trigger.</p>"
"Secrets in a Django app on Google AppEngine (GAE)<p>I'm trying to develop a Django app on GAE, and using CloudBuild for CI/CD. I'm wondering what's the best way to pass secrets to my app (DB credentials, etc).</p>

<p>I was able to follow instructions at <a href=""https://cloud.google.com/cloud-build/docs/securing-builds/use-encrypted-secrets-credentials"" rel=""nofollow noreferrer"">https://cloud.google.com/cloud-build/docs/securing-builds/use-encrypted-secrets-credentials</a> to read the secret from in my build step, and pass it in to my app as an environment variable. It's a bit hacky, but it works:</p>

<pre><code>  - name: gcr.io/cloud-builders/gcloud
    entrypoint: 'bash'
    args:
    - '-c'
    - |
      TEST_PW=$(gcloud secrets versions access latest --secret=test-key)
      echo ""TEST_PASSWORD=$${TEST_PW}"" &gt;&gt; env_vars
      unset TEST_PW
</code></pre>

<p>However, I'm not sure if this practice is safe. I dumped the env variables in running in my app (using <code>print(dict(os.environ))</code> and the only sensitive values there are the secrets I passed in (all other GAE app related values are non-sensitive data).</p>

<p>So questions:</p>

<p>1) Is storing secrets in env variables safe in an app in AppEngine, i.e. can they be stolen by ""somehow"" dumping them through <code>print(dict(os.environ))</code>?</p>

<p>2) Or is the better option to fetch them from Secret Manager in Django (for e.g. in <code>settings.py</code>)? (I'm worried about restarts or version switches here, and if they'll affect this option)</p>

<p>3) Or is there an even better option?</p>

<p>Thanks.</p>","<p>The security issue with what you are doing is not the environment variable itself, but the fact that you are storing the secret's plain decrypted value in it, making it accessible by the <code>os.environ</code> command while your instance is running.</p>

<p>A simpler solution would be to dump that sensitive information to a file and store it on a Cloud Storage Bucket only your app engine's service account has access to, like this:</p>

<pre><code>TEST_PW=$(gcloud secrets versions access latest --secret=test-key)
echo ""TEST_PASSWORD=$${TEST_PW}"" &gt;&gt; [YOUR_FILE_URL]
unset TEST_PW
</code></pre>

<p>If you want to keep using environment variables, you can do it by using Cloud KMS to keep data encrypted, you can find a how to <a href=""https://cloud.google.com/cloud-build/docs/securing-builds/use-encrypted-secrets-credentials#encrypt_credentials"" rel=""nofollow noreferrer"">here</a>, which is a different section of the same documentation you shared on your question.</p>"
"Gcloud comportament differ from shell to cloudbuild.yaml<p>I have been trying to list all the API gateways config on gcloud, and something wrong is happening.</p>
<p>When I run the following command on the terminal with my user logged in, it works like a charm.</p>
<pre><code>gcloud api-gateway api-configs list --api=$API --project=$PROJECT_ID --format=&quot;table(name)&quot;
</code></pre>
<p>But when I run the same command from inside this cloudbuild.yaml</p>
<pre><code>steps:
  - name: &quot;gcr.io/google.com/cloudsdktool/cloud-sdk&quot;
    entrypoint: &quot;bash&quot;
    args:
      - &quot;-c&quot;
      - |
        gcloud api-gateway api-configs list --api=logistics-homolog --project=$PROJECT_ID \
        --filter=serviceConfigId:logistics-mobile-places-* --format=&quot;table(name)&quot;
</code></pre>
<p>It gives me the following error:</p>
<pre><code>ERROR: (gcloud.api-gateway.api-configs.list) PERMISSION_DENIED: Permission 'apigateway.apiconfigs.list' denied on 'projects/$PROJECT_ID/locations/global/apis/logistics-homolog/configs'
</code></pre>
<p>What's wrong with it?!</p>","<p>You need to grant the Cloud Build default service account (pattern: @cloudbuild.gserviceaccount.com) the required permissions for your command.</p>
<p>Have a look on your IAM page to update that.</p>"
"Error with Spring Boot app in Google Cloud Build - Creates working version but reports failed build<p>I have a Java Spring Boot app that was previously building well, and we are now having issues.</p>
<p>We are using GCP, and the cloud build feature to trigger builds automatically when we push to certain branches in GCP. The goal is for the app to build itself, then deploy to app engine. In various iterations before much trial and error we were doing this successfully.</p>
<p>The app builds and deploys successfully. Meaning if I push code, it builds and works. But the cloud build tool keeps reporting that the build failed.</p>
<p>Our cloudbuild.yaml</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- id: 'Stage app using mvn appengine plugin on mvn cloud build image'   
  name: 'gcr.io/cloud-builders/mvn'
  args: ['package', 'appengine:stage', '-Dapp.stage.appEngineDirectory=src/main/appengine/$_GAE_YAML', '-P cloud-gcp']
  timeout: 1600s
- id: &quot;Deploy to app engine using gcloud image&quot;
  name: 'gcr.io/cloud-builders/gcloud'
  args: ['app', 'deploy', 'target/appengine-staging/app.yaml',
         '-q', '$_GAE_PROMOTE', '-v', '$_GAE_VERSION']
  timeout: 1600s
- id: &quot;Splitting Traffic&quot;
  name: 'gcr.io/cloud-builders/gcloud'
  args: ['app', 'services', 'set-traffic', '--splits', '$_GAE_TRAFFIC']
timeout: 3200s
</code></pre>
<p>For reference here is an app.yaml</p>
<pre class=""lang-yaml prettyprint-override""><code>runtime: java
env: flex
runtime_config:
  jdk: openjdk8
env_variables:
  SPRING_PROFILES_ACTIVE: &quot;dev&quot;
handlers:
  - url: /.*
    script: this field is required, but ignored
    secure: always
manual_scaling:
  instances: 1
resources:
  cpu: 2
  memory_gb: 2
  disk_size_gb: 10
  volumes:
    - name: ramdisk1
      volume_type: tmpfs
      size_gb: 0.5
</code></pre>
<p>The first step completes just fine, or seemingly so.</p>
<p>The app becomes available on that specific version and runs just fine.</p>
<p>Here is the current &quot;failure&quot; we are facing, found in the output of the failed builds in the second step:</p>
<pre><code>--------------------------------------------------------------------------------
Updating service [default] (this may take several minutes)...

ERROR: (gcloud.app.deploy) Error Response: [9] An internal error occurred while processing task /app-engine-flex/flex_await_healthy/flex_await_healthy&gt;2021-11-04T14:55:50.087Z257173.in.0:
There was an error while pulling the application's docker image: the image does
not exist, one of the image layers is missing or the default service account
does not have  permission to pull the image. Please check if the image exists.
Also check if the default service account has the role Storage Object Viewer
(roles/storage.objectViewer) to pull images from Google Container
Registry or Artifact Registry Reader (roles/artifactregistry.reader) to pull
images from Artifact Registry. Refer to https://cloud.google.com/container-registry/docs/access-control
in granting access to pull images from GCR. Refer to https://cloud.google.com/artifact-registry/docs/access-control#roles
in granting access to pull images from Artifact Registry.
</code></pre>
<p>We have been having pretty consistent issues with the caching of builds, to the point where in the past we push new code and it launches old versions of the code. I think it may all be related.</p>
<p>We have tried clearing the entire container registry cache for the specific version of the app, and that is when this specific issue started occuring. I have a feeling it is just building and launching one version of the app, then going back and trying to launch a different version of the app right on top of that. Looking for a way to at least get more verbose logging but this is mostly where I am stuck.</p>
<p>How do I go about adjusting the &quot;name: 'gcr.io/cloud-builders/gcloud'&quot; step to properly indicate that a deployment worked? Is that the right approach?</p>","<p>Answering my own question here.</p>
<p>It turns out that the application was deploying but listening on the wrong port. We just added <code>server.port=8080</code> to the application.properties file and things started working again.</p>
<p>I do believe what Chanseok Oh mentioned in the comment above on my question was also true. Although changing the port seemed to be the one and only thing that solved this.</p>
<p>GCP was trying to do a readiness check, and was getting nothing back. It is unclear why this was related at all to the cache of the artifacts, if at all.</p>"
"How to use custom Cloud Builders with images from Google Artifact Repository<p>How do I use a custom builder image in Cloud Build which is stored in a repository in Artifact Registry (instead of Container Registry?)</p>
<p>I have set up a pipeline in Cloud Build where some python code is executed using official python images. As I want to cache my python dependencies, I wanted to create a custom Cloud Builder as shown in the official documentation <a href=""https://cloud.google.com/build/docs/configuring-builds/use-community-and-custom-builders#creating_a_custom_builder"" rel=""nofollow noreferrer"">here</a>.</p>
<p>GCP clearly indicates to switch to Artifact Registry as Container Registry will be replaced by the former. Consequently, I have pushed my docker image to Artifact Registry. I also gave my Cloud Builder Service Account the reader permissions to Artifact Registry.</p>
<p>Using the image in a Cloud Build step like this</p>
<pre class=""lang-html prettyprint-override""><code>steps:
  - name: 'europe-west3-docker.pkg.dev/xxxx/yyyy:latest'
    id: install_dependencies
    entrypoint: pip
    args: [&quot;install&quot;, &quot;-r&quot;, &quot;requirements.txt&quot;, &quot;--user&quot;]
</code></pre>

<p>throws the following error</p>
<blockquote>
<p>Step #0 - &quot;install_dependencies&quot;: Pulling image: europe-west3-docker.pkg.dev/xxxx/yyyy:latest
Step #0 - &quot;install_dependencies&quot;: Error response from daemon: manifest for europe-west3-docker.pkg.dev/xxxx/yyyy:latest not found: manifest unknown: Requested entity was not found.</p>
</blockquote>
<p>&quot;xxxx&quot; is the repository name and &quot;yyyy&quot; the name of my image. The tag &quot;latest&quot; exists.
I can pull the image locally and access the repository.</p>
<p>I could not find any documentation on how to integrate these images from Artifact Registry. There is only <a href=""https://cloud.google.com/artifact-registry/docs/configure-cloud-build#docker"" rel=""nofollow noreferrer"">this official guide</a>, where the image is built using the Docker image from Container Registry – however this should not be future proof.</p>","<p>It looks like you need to add your Project ID to your image name.
You can use the &quot;$PROJECT_ID&quot; Cloud Build default substitution variable.
So your updated image name would look something like this:</p>
<pre><code>steps:
  - name: 'europe-west3-docker.pkg.dev/$PROJECT_ID/xxxx/yyyy:latest'
</code></pre>
<p>For more details about substituting variable values in Cloud Build see:</p>
<p><a href=""https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values</a></p>"
"Maven upgrade unable to run package appengine:stage<p>I have a Java 8 Spring Boot app that is deployed to Google App Engine and being built via GCP CloudBuild. I am trying to upgrade it from Java 8 to Java 11.</p>
<p>In the <code>cloudbuild.yaml</code> file, I changed:</p>
<pre><code>- id: 'Build and Test'
  name: 'gcr.io/cloud-builders/mvn:3.5.0-jdk-8'
  args: ['package', 'appengine:stage']
</code></pre>
<p>to:</p>
<pre><code>- id: 'Build and Test'
  name: 'maven:3.8.3-jdk-11'
  args: ['package', 'appengine:stage']
</code></pre>
<p>When I run the CloudBuild, this step now suddenly fails with the following error:</p>
<pre><code>docker.io/library/maven:3.8.3-jdk-11
/usr/local/bin/mvn-entrypoint.sh: 50: exec: package: not found
</code></pre>
<p>In its previous configuration, it was running just fine. The entire <code>cloudbuild.yaml</code> file is:</p>
<pre><code>steps:
  - id: 'copy file'
    name: 'ubuntu'
    args: ['cp', 'src/main/appengine/app.yaml', src/main/appengine/app.yaml]

  - id: 'Build and Test'
    name: 'maven:3.8.3-jdk-11'
    args: ['package', 'appengine:stage']
</code></pre>
<p><strong>What is going on here?</strong> Does the <code>gcr.io/cloud-builders/mvn:3.5.0-jdk-8</code> image somehow understand <code>mvn package appengine:stage</code>, whereas the <code>maven:3.8.3-jdk-11</code> image doesn't? Mainly I just need someone to help me understand why I'm getting the error. If anyone could also lend some suggestions for how to fix or circumvent it, that'd be greatly appreciated as well. Thanks in advance!</p>","<p>It was so obvious in hindsight. I needed to specify <code>mvn</code> as the entrypoint command in the step like so:</p>
<pre><code>- id: 'Build and Test'
  entrypoint: mvn
  name: 'maven:3.8.3.0-jdk-11'
  args: ['package','appengine:stage']
</code></pre>"
"Is there a way to deploy Cloud Functions directly as zip artifacts to Google Cloud Platform? and not rely on the default Cloud Build?<p>The default setup for firebase functions is to run <code>firebase deploy</code>, which will:</p>
<ul>
<li>Upload the whole project to Cloud Build</li>
<li>Cloud Build will extract the functions</li>
<li>It will run <code>npm install.</code></li>
<li>Create the ZIP artefacts</li>
<li>Upload the ZIP artefacts to the cloud</li>
</ul>
<p>The question is if you know of a way to make these ZIP artefacts on our side and upload them directly?</p>
<p><a href=""https://i.stack.imgur.com/olkHc.png"" rel=""nofollow noreferrer"">Default Cloud Build steps</a>
<a href=""https://i.stack.imgur.com/EAobW.png"" rel=""nofollow noreferrer"">List of the Cloud Build deployments</a></p>","<p>From my point of view - there are plenty of options how to deploy one or more cloud functions.</p>
<p>The <a href=""https://cloud.google.com/functions/docs/deploying/"" rel=""nofollow noreferrer"">Deploying Cloud Functions</a> documentation provides some initial context.</p>
<p>The easiest way, from my point of view, to use <code>gcloud functions deploy</code> command - see <a href=""https://cloud.google.com/sdk/gcloud/reference/functions/deploy"" rel=""nofollow noreferrer"">Cloud SDK CLI - gcloud functions deploy</a></p>
<p>As a side note - from my personal point of view - an idea to use Cloud Build - is not bad, and it has many benefits (security, organization of CI/CD, etc.) but it is up to you. Personally I use Cloud Build with Terraform, and configured deployment in a such a way, that only updated Cloud Functions are redeployed.</p>"
"How to solve permissions for push to Google Artifact Registry from Cloud Build using jib-maven-plugin?<p>This problem seems to be all about my permissions in GCP, but attempting to set the right permissions so far has not worked.</p>
<p>I'm using <code>com.google.cloud.tools:jib-maven-plugin</code> to package a Spring Boot project into a container and push it to Google Artifact Registry (GAR). It works just fine when I run it locally, but it fails when I run the maven build with Google Cloud Build. It says it fails because <code>artifactregistry.repositories.downloadArtifacts</code> permission is missing.</p>
<p>But this is one of the permissions enabled by default according to <a href=""https://cloud.google.com/build/docs/cloud-build-service-account"" rel=""nofollow noreferrer"">Google Docs</a>.</p>
<p>My target is a Google Artifacts (docker) Registry. I'm able to change the target to a Google Container Registry (deprecated, so I need to change to GAR) and that works fine under Cloud Build, no permission problems there. The build also downloads jar files from a maven repository stored in a different GAR, though in the same Google project. So clearly it is okay with permissions for that maven GAR.</p>
<p>I verified that running the maven build locally works, including writing to the GAR, which eliminates something going bad in the jib plugin configuration, or in the GAR configuration. This is using my own user credentials.</p>
<p>What have I tried?</p>
<ul>
<li>I added the appropriate roles to the default service account Cloud Build uses (though they are supposed to be there anyway). The <code>downloadArtifacts</code> permission is included in role Artifact Registry Reader, so I added that role as well as Artifact Registry Writer.</li>
<li>I switched to a different Service Account (build service account, let's call it BSA) and, yes, made sure that had the same appropriate roles (see above).</li>
<li>I added the BSA as a principal to the target GAR and gave it the appropriate roles there as well (getting desperate)</li>
<li>My user credentials include Owner role so I added Owner role to the BSA (not something I want to keep)</li>
</ul>
<p>All of these gave me the same permission denied error. Just in case I had really misunderstood something I added a step to my Cloud Build yaml to runs <code>gcloud info</code> and verified that, yes, it is using the BSA I have configured with the roles I need.</p>
<p>Is there something I missed?
Thanks</p>
<p>...Edit
More info. Most of my builds use jib but one uses Spotify to create a local docker image and then uses docker to push to the registry.
And this works! So the problem is specific to jib. Somehow, under cloud build, jib is not seeing the creds, though it does see them locally.</p>
<p>...Edit
The actual error message:</p>
<pre><code>Failed to execute goal com.google.cloud.tools:jib-maven-plugin:1.6.1:build (build-and-push-docker-image) on project knifethrower: Build image failed, perhaps you should make sure you have permissions for australia-southeast1-docker.pkg.dev/redacted/bonanza-platform/knifethrower and set correct credentials. See https://github.com/GoogleContainerTools/jib/blob/master/docs/faq.md#what-should-i-do-when-the-registry-responds-with-forbidden-or-denied for help: Unauthorized for australia-southeast1-docker.pkg.dev/redacted/bonanza-platform/knifethrower: 403 Forbidden
[ERROR] {&quot;errors&quot;:[{&quot;code&quot;:&quot;DENIED&quot;,&quot;message&quot;:&quot;Permission \&quot;artifactregistry.repositories.downloadArtifacts\&quot; denied on resource \&quot;projects/redacted/locations/australia-southeast1/repositories/bonanza-platform\&quot; (or it may not exist)&quot;}]}
[ERROR] -&gt; [Help 1]
</code></pre>
<p>Also note I'm using the latest version of jib: 3.1.4</p>
<p>...Edit</p>
<p>I've tried a couple more things. I added an earlier step, before the maven build, which does a <code>gcloud auth configure-docker --quiet --verbosity=debug australia-southeast1-docker.pkg.dev</code>. That creates a <code>/builder/home/.docker/config.json</code> file. Because there seems to be confusion about where that file should really live I copied it to <code>/root/.docker</code>. But that did not help.</p>
<p>The second thing I tried was using the <code>$DOCKER_CONFIG</code> to point to the <code>/builder/home/.docker</code> directory (<a href=""https://github.com/GoogleContainerTools/jib/blob/master/docs/faq.md#what-should-i-do-when-the-registry-responds-with-unauthorized"" rel=""nofollow noreferrer"">as suggested here</a>) and that did not help either.</p>
<p>Same error in both cases. I do get a message from the <code>gcloud auth configure-docker...</code></p>
<pre><code>WARNING: `docker` not in system PATH.
`docker` and `docker-credential-gcloud` need to be in the same PATH in order to work correctly together.
gcloud's Docker credential helper can be configured but it will not work until this is corrected.
Adding credentials for: australia-southeast1-docker.pkg.dev
Docker configuration file updated.
INFO: Display format: &quot;default&quot;
</code></pre>
<p>I think this is trying to be helpful and tell me it cannot find docker installed (which is true) but it created the creds anyway (also true). And it should not matter because jib doesn't rely on docker itself, just the creds. However it still doesn't work.</p>","<p>Part of the problem (and thanks again to @ChanseokOh for flagging this) is that I was still using jib 1.6.1 when I thought I was using 3.1.4. Changing that plus some other stuff fixed the problem. So here's the full story for the next person who struggles with this:</p>
<p>First, this is what my pom file has:</p>
<pre><code>&lt;plugin&gt;
    &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt;
    &lt;artifactId&gt;jib-maven-plugin&lt;/artifactId&gt;
    &lt;version&gt;3.1.4&lt;/version&gt;
    &lt;configuration&gt;
        &lt;from&gt;
            &lt;image&gt;${base.image}&lt;/image&gt;
        &lt;/from&gt;
        &lt;to&gt;
            &lt;image&gt;${docker.image.repo}/${project.artifactId}:latest&lt;/image&gt;
            &lt;tags&gt;
                &lt;tag&gt;${VERSION_ID}&lt;/tag&gt;
                &lt;tag&gt;latest&lt;/tag&gt;
            &lt;/tags&gt;
        &lt;/to&gt;
        &lt;creationTime&gt;USE_CURRENT_TIMESTAMP&lt;/creationTime&gt;
        &lt;allowInsecureRegistries&gt;true&lt;/allowInsecureRegistries&gt;
        &lt;container&gt;
            &lt;ports&gt;
                &lt;port&gt;8080&lt;/port&gt;
            &lt;/ports&gt;
        &lt;/container&gt;
    &lt;/configuration&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;id&gt;build-and-push-docker-image&lt;/id&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
                &lt;goal&gt;build&lt;/goal&gt;
            &lt;/goals&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
&lt;/plugin&gt;
</code></pre>
<p>The version is important. Although the old version (1.6.1) worked just fine locally it did not work on Cloud Build.</p>
<p>My Cloud Build file looks like this:</p>
<pre><code>...
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - '-c'
      - &gt;
        gcloud auth configure-docker --quiet --verbosity=debug `echo
        ${_CONTAINER_REPO} | cut -d / -f 1` 
        /root 
    id: gcloud auth
    entrypoint: /bin/bash
...
  - name: 'gcr.io/cloud-builders/mvn:3.5.0-jdk-8'
    args:
      - '-Dmaven.test.skip=false'
      - '-Dmaven.repo.local=/workspace/.m2/repository'
      - '--settings'
      - custom-settings.xml
      - clean
      - install
      - '-DskipITs'
      - '-B'
      - '-X'
      - '-DVERSION_ID=$TAG_NAME'
      - '-DBRANCH_ID=master'
      - '-DPROJECT_ID=$PROJECT_ID'
      - '-DCONTAINER_REPO=${_CONTAINER_REPO}'
      - '-DMAVEN_REPO=${_MAVEN_REPO}'
      - '-DDOCKER_CONFIG=/builder/home/.docker'
      - '-P'
      - release
    id: build
</code></pre>
<p>The <code>gcloud auth</code> step gets the docker credentials file created.</p>
<p>The next step is the maven build and for that the trick is to define DOCKER_CONFIG pointing to the correct location of the docker creds file. I believe creating the docker creds file, defining DOCKER_CONFIG and getting the version number right are all required for the solution.</p>
<p>An interesting aside is that <code>gcr.io/cloud-builders/gcloud</code> and <code>gcr.io/cloud-builders/mvn:3.5.0-jdk-8</code> still reference Google Container Repository which is superseded by Artifact Repository, and is the whole reason I got into this but I have not seen any updated reference for these images. <a href=""http://gcr.io/cloud-builders/mvn:3.5.0-jdk-8"" rel=""nofollow noreferrer"">Docs are here</a>.</p>"
"GCP App Engine and Cloud Build: Cannot find module '/workspace/server.js'<p><strong>TLDR:</strong> Sorry for the longest question in history, but I hope this is comprehensive for any users having a similar problem. My app deploys successfully from cloud shell to my domain; however, when I try cloud build, I get
<code>Cannot find module '/workspace/server.js'</code>
The error likely has to due with my build handlers in the app.yaml, or something to do with my cloudbuild.yaml.</p>
<p>Solution: use the right handlers in app.yaml standard and properly set up your cloudbuild.yaml</p>
<p>I am having trouble using App Engine and Cloud Build together. I am using Cloud Build to set up CICD with my Github repo. I think the issue is due to the fact that I have been not been deploying the production build to app engine. I was able to successfully deploy manually (dev version) with:</p>
<p><code>gcloud app deploy</code></p>
<p>Now, I am having trouble with my Cloud Build. In particular, I am trying to run flex environment, but I keep getting &quot;Neither &quot;start&quot; in the &quot;scripts&quot; section of &quot;package.json&quot; nor the &quot;server.js&quot; file were found.&quot; But my package.json has a startup script?</p>
<p>I also tried standard environment instead of flex, but I couldn't get the handlers figured out. I tried dozens of examples. I have included the standard environment app.yaml so you can see it.</p>
<p>Here's my package.json:</p>
<pre><code>{
  &quot;name&quot;: &quot;flier-mapper&quot;,
  &quot;version&quot;: &quot;0.1.0&quot;,
  &quot;private&quot;: true,
  &quot;dependencies&quot;: {
    &quot;@firebase/storage&quot;: &quot;^0.8.4&quot;,
    &quot;@parcel/transformer-sass&quot;: &quot;^2.0.0&quot;,
    &quot;@react-google-maps/api&quot;: &quot;^2.7.0&quot;,
    &quot;@testing-library/jest-dom&quot;: &quot;^5.11.4&quot;,
    &quot;@testing-library/react&quot;: &quot;^11.1.0&quot;,
    &quot;@testing-library/user-event&quot;: &quot;^12.1.10&quot;,
    &quot;bootstrap&quot;: &quot;^5.1.3&quot;,
    &quot;cors&quot;: &quot;^2.8.5&quot;,
    &quot;emailjs-com&quot;: &quot;^3.2.0&quot;,
    &quot;firebase&quot;: &quot;^9.2.0&quot;,
    &quot;firebase-admin&quot;: &quot;^10.0.0&quot;,
    &quot;firebase-functions&quot;: &quot;^3.16.0&quot;,
    &quot;react&quot;: &quot;^17.0.2&quot;,
    &quot;react-bootstrap&quot;: &quot;^2.0.2&quot;,
    &quot;react-dom&quot;: &quot;^17.0.2&quot;,
    &quot;react-ga&quot;: &quot;^3.3.0&quot;,
    &quot;react-helmet&quot;: &quot;^6.1.0&quot;,
    &quot;react-icons&quot;: &quot;^4.3.1&quot;,
    &quot;react-pricing-table&quot;: &quot;^0.3.0&quot;,
    &quot;react-router-dom&quot;: &quot;^5.3.0&quot;,
    &quot;react-scripts&quot;: &quot;4.0.3&quot;,
    &quot;react-tabs&quot;: &quot;^3.2.3&quot;,
    &quot;stripe&quot;: &quot;^8.188.0&quot;,
    &quot;web-vitals&quot;: &quot;^1.0.1&quot;
  },
  &quot;scripts&quot;: {
    &quot;start&quot;: &quot;react-scripts start&quot;,
    &quot;build&quot;: &quot;react-scripts build&quot;,
    &quot;test&quot;: &quot;react-scripts test&quot;,
    &quot;eject&quot;: &quot;react-scripts eject&quot;
  },
  &quot;eslintConfig&quot;: {
    &quot;extends&quot;: [
      &quot;react-app&quot;,
      &quot;react-app/jest&quot;
    ]
  },
  &quot;browserslist&quot;: {
    &quot;production&quot;: [
      &quot;&gt;0.2%&quot;,
      &quot;not dead&quot;,
      &quot;not op_mini all&quot;
    ],
    &quot;development&quot;: [
      &quot;last 1 chrome version&quot;,
      &quot;last 1 firefox version&quot;,
      &quot;last 1 safari version&quot;
    ]
  }
}
</code></pre>
<p>Here's my cloudbuild.yaml. I also tried deleting the dir line:</p>
<pre><code>steps:

- name: &quot;gcr.io/cloud-builders/npm&quot;
  dir: 'frontend'
  args: ['install']
  timeout: &quot;5m&quot;

- name: &quot;gcr.io/cloud-builders/npm&quot;
  dir: 'frontend'
  args: [&quot;run&quot;, &quot;build&quot;]
  timeout: &quot;5m&quot;

- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  entrypoint: bash
  args: 
    - &quot;-c&quot;
    - |
        cp app.yaml ./build
        cd build
        gcloud app deploy
  timeout: &quot;1600s&quot;

timeout: 60m
</code></pre>
<p>Here's my flex app.yaml:</p>
<pre><code>runtime: nodejs
env: flex

# This sample incurs costs to run on the App Engine flexible environment.
  # The settings below are to reduce costs during testing and are not appropriate
# for production use. For more information, see:
# https://cloud.google.com/appengine/docs/flexible/nodejs/configuring-your-app-with-app-yaml
manual_scaling:
  instances: 1
resources:
  cpu: 1
  memory_gb: 0.5
  disk_size_gb: 10
</code></pre>
<p>And here's one of the many handlers I tried with a standard environemnt:</p>
<pre><code>runtime: nodejs14

# This sample incurs costs to run on the App Engine flexible environment.
# The settings below are to reduce costs during testing and are not appropriate
# for production use. For more information, see:
# https://cloud.google.com/appengine/docs/flexible/nodejs/configuring-your-app-with-app-yaml
manual_scaling:
  instances: 1

handlers:
  # Serve all static files with url ending with a file extension
  - url: /(.*\..+)$
    static_files: build/\1
    upload: build/(.*\..+)$
  # Catch all handler to index.html
  - url: /.*
    static_files: build/index.html
    upload: build/index.html

  - url: /.*
    secure: always
    redirect_http_response_code: 301
    script: auto
</code></pre>
<p><strong>EDIT</strong></p>
<p>So, I got it to run on standard environment, which is useful for https requests. Here's my app.yaml that I got working:</p>
<pre><code>runtime: nodejs14

# This sample incurs costs to run on the App Engine flexible environment.
# The settings below are to reduce costs during testing and are not appropriate
# for production use. For more information, see:
# https://cloud.google.com/appengine/docs/flexible/nodejs/configuring-your-app-with-app-yaml
manual_scaling:
  instances: 2

handlers:

- url: /.*
  secure: always
  redirect_http_response_code: 301
  script: auto
</code></pre>
<p>However, now my problem is this:</p>
<pre><code>Error: Cannot find module '/workspace/server.js'
    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:889:15)
    at Function.Module._load (internal/modules/cjs/loader.js:745:27)
    at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:76:12)
    at internal/main/run_main_module.js:17:47 {
</code></pre>
<p>Which is essentially the same issue as above.</p>
<p><a href=""https://i.stack.imgur.com/z88dl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z88dl.png"" alt=""500 Server Error on GCP App Engine"" /></a></p>
<p><strong>EDIT 2</strong></p>
<p>I'm wondering if it is because now that I am running a production build I need to add the correct handlers. But as mentioned above, I have tried dozens of combinations of various answers on the internet to no avail.</p>
<p>Here's another standard app.yaml I tried:</p>
<pre><code>runtime: nodejs16

# This sample incurs costs to run on the App Engine flexible environment.
# The settings below are to reduce costs during testing and are not appropriate
# for production use. For more information, see:
# https://cloud.google.com/appengine/docs/flexible/nodejs/configuring-your-app-with-app-yaml
manual_scaling:
  instances: 2

handlers:
  - url: /.*
    secure: always
    redirect_http_response_code: 301
    script: auto

  - url: /
    static_files: build/index.html
    upload: build/index.html

  - url: /
    static_dir: build
</code></pre>
<p>Is it something to do with one of my packages? If so, which one? There aren't any specials ones that I know of...</p>
<p>I also added this to my package.json, but it didn't do anything either:</p>
<pre><code>&quot;start&quot;: &quot;PORT=8080 react-scripts start&quot;,
</code></pre>
<p><strong>EDIT 3</strong>
I have tried these posts: <a href=""https://stackoverflow.com/questions/63608035/error-cannot-find-module-workspace-server-js"">Error: Cannot find module &#39;/workspace/server.js&#39;</a>
<a href=""https://stackoverflow.com/questions/61740611/could-not-find-module-workspace-server-js"">could not find module workspace/server.js</a>
<a href=""https://stackoverflow.com/questions/68783153/error-cannot-find-module-workspace-server-js-upon-node-js-deploy-on-google-a"">Error: Cannot find module &#39;/workspace/server.js&#39; upon node js deploy on google app engine</a>
<a href=""https://medium.com/@calebmackdaven/so-you-want-to-start-using-google-cloud-ce9054e84fa8"" rel=""nofollow noreferrer"">https://medium.com/@calebmackdaven/so-you-want-to-start-using-google-cloud-ce9054e84fa8</a></p>
<p>This app.yaml built on cloud build; however, I now get a new error &quot;The requested URL / was not found on this server.&quot;:</p>
<pre><code>runtime: nodejs16

# This sample incurs costs to run on the App Engine flexible environment.
# The settings below are to reduce costs during testing and are not appropriate
# for production use. For more information, see:
# https://cloud.google.com/appengine/docs/flexible/nodejs/configuring-your-app-with-app-yaml
manual_scaling:
  instances: 2

handlers:
  - url: /static/js/(.*)
    static_files: build/static/js/\1
    upload: build/static/js/(.*)
  - url: /static/css/(.*)
    static_files: build/static/css/\1
    upload: build/static/css/(.*)
  - url: /static/media/(.*)
    static_files: build/static/media/\1
    upload: build/static/media/(.*)
  - url: /(.*\.(json|ico))$
    static_files: build/\1
    upload: build/.*\.(json|ico)$
  - url: /
    static_files: build/index.html
    upload: build/index.html
  - url: /.*
    static_files: build/index.html
    upload: build/index.html
  - url: /.*
    secure: always
    redirect_http_response_code: 301
    script: auto
</code></pre>
<p>My logs say:</p>
<pre><code>Static file referenced by handler not found: build/index.html
</code></pre>
<p><strong>EDIT 4</strong>
I tried a different cloudbuild.yaml:</p>
<pre><code>steps:

- name: node
  entrypoint: npm
  args: ['install']

- name: node
  entrypoint: npm
  args: ['run', 'build']

- name: 'bash'
  args: ['gcloud app deploy']
</code></pre>
<p>However, now the error I get in my npm run build stage is :</p>
<pre><code>Error message &quot;error:0308010C:digital envelope routines::unsupported&quot;
</code></pre>
<p>I checked out <a href=""https://stackoverflow.com/questions/69692842/error-message-error0308010cdigital-envelope-routinesunsupported"">this answer</a>, so I tried a different node version  14 and still got the same issue. I tried the following, but got the same error:</p>
<pre><code>&quot;start&quot;: &quot;react-scripts --openssl-legacy-provider start&quot;
</code></pre>
<p>Here's my error:</p>
<pre><code>Step #1: Error: error:0308010C:digital envelope routines::unsupported
Step #1:     at new Hash (node:internal/crypto/hash:67:19)
Step #1:     at Object.createHash (node:crypto:130:10)
Step #1:     at module.exports (/workspace/node_modules/webpack/lib/util/createHash.js:135:53)
Step #1:     at NormalModule._initBuildHash (/workspace/node_modules/webpack/lib/NormalModule.js:417:16)
Step #1:     at /workspace/node_modules/webpack/lib/NormalModule.js:452:10
Step #1:     at /workspace/node_modules/webpack/lib/NormalModule.js:323:13
Step #1:     at /workspace/node_modules/loader-runner/lib/LoaderRunner.js:367:11
Step #1:     at /workspace/node_modules/loader-runner/lib/LoaderRunner.js:233:18
Step #1:     at context.callback (/workspace/node_modules/loader-runner/lib/LoaderRunner.js:111:13)
Step #1:     at /workspace/node_modules/babel-loader/lib/index.js:59:103 {
Step #1:   opensslErrorStack: [ 'error:03000086:digital envelope routines::initialization error' ],
Step #1:   library: 'digital envelope routines',
Step #1:   reason: 'unsupported',
Step #1:   code: 'ERR_OSSL_EVP_UNSUPPORTED'
Step #1: }
Step #1: 
Step #1: Node.js v17.1.0
Finished Step #1
ERROR
ERROR: build step 1 &quot;node&quot; failed: step exited with non-zero status: 1
</code></pre>
<p>Why is it attempting to use Node.js v17.1.0 when I specified v16 in the app.yaml?</p>
<p><strong>EDIT 5</strong></p>
<p>I tried specifying Node v16.13.0:</p>
<pre><code>steps:
- name: node: &quot;16.13.0&quot;
  entrypoint: npm
  args: ['install']

- name: node: &quot;16.13.0&quot;
  entrypoint: npm
  args: ['run', 'build']

- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  entrypoint: bash
  args: [gcloud app deploy]

</code></pre>
<p>However, it didn't build at all:</p>
<pre><code>Your build failed to run: failed unmarshalling build config cloudbuild.yaml: yaml: line 2: mapping values are not allowed in this context
</code></pre>
<p>So I tried adding a substitution:</p>
<pre><code>--- 
steps: 
  - 
    args: 
      - install
    entrypoint: npm
    name: &quot;node= $_NODE_VERSION&quot;
  - 
    args: 
      - run
      - build
    entrypoint: npm
    name: &quot;node= $_NODE_VERSION&quot;
  - 
    args: 
      - &quot;gcloud app deploy&quot;
    entrypoint: bash
    name: gcr.io/cloud-builders/gcloud
substitutions: 
  $_NODE_VERSION: v16.13.0

</code></pre>
<p>But I got:</p>
<pre><code>Your build failed to run: generic::invalid_argument: invalid build: invalid build step name &quot;node= &quot;: could not parse reference: node=
</code></pre>
<p>And if I try node: &quot;16.13.0&quot;, the error is similar:</p>
<pre><code>Your build failed to run: failed unmarshalling build config cloudbuild.yaml: yaml: line 2: mapping values are not allowed in this context
</code></pre>
<p>Okay, that error was just due to spacing. I tried updating my cloudbuild.yaml as follows:</p>
<pre><code>steps:
- name: node:16.13.0
  entrypoint: npm
  args: ['install']

- name: node:16.13.0
  entrypoint: npm
  args: ['run', 'build']

- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  entrypoint: bash
  args: [gcloud app deploy]

</code></pre>
<p>Now I am able to build. However, now I get an issue with the gcloud command:</p>
<pre><code>Step #2: bash: gcloud app deploy: No such file or directory
</code></pre>
<p><strong>EDIT 6</strong></p>
<p>I'm going in circles. My new error is the original one:</p>
<pre><code>Error: Not Found
The requested URL / was not found on this server.
</code></pre>
<p>But at least I got the cloud build to build. So it must be my app.yaml or cloudbuild.yaml.</p>
<pre><code>steps:
- name: node:16.13.0
  entrypoint: npm
  args: ['install']

- name: node:16.13.0
  entrypoint: npm
  args: ['run', 'build']

- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  entrypoint: bash
  args: 
    - &quot;-c&quot;
    - |
        cp app.yaml ./build
        cd build
        gcloud app deploy
</code></pre>
<pre><code>Step #2: bash: gcloud app deploy: No such file or directory
</code></pre>
<p><strong>Solution</strong>
I finally got it working! I was changing directory to build, but shouldn't have been. So here are my working cloudbuild.yaml and app.yaml files:</p>
<p>cloudbuild.yaml</p>
<pre><code>steps:
- name: node:16.13.0
  entrypoint: npm
  args: ['install']

- name: node:16.13.0
  entrypoint: npm
  args: ['run', 'build']

- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  entrypoint: bash
  args: 
    - &quot;-c&quot;
    - |
        cp app.yaml ./build
        gcloud app deploy
</code></pre>
<p>app.yaml</p>
<pre><code>--- 
handlers: 
  - 
    secure: always
    static_dir: build/static
    url: /static
  - 
    secure: always
    static_files: build/\1
    upload: build/.*\.(json|ico|js)$
    url: /(.*\.(json|ico|js))$
  - 
    secure: always
    static_files: build/index.html
    upload: build/index.html
    url: .*
manual_scaling: 
  instances: 1
runtime: nodejs16

</code></pre>","<p><strong>Solution</strong>
I finally got it working! I was changing directory to build, but shouldn't have been. So here are my working cloudbuild.yaml and app.yaml files:</p>
<p>cloudbuild.yaml</p>
<pre><code>steps:
- name: node:16.13.0
  entrypoint: npm
  args: ['install']

- name: node:16.13.0
  entrypoint: npm
  args: ['run', 'build']

- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  entrypoint: bash
  args: 
    - &quot;-c&quot;
    - |
        cp app.yaml ./build #not sure if this is necessary
        gcloud app deploy
</code></pre>
<p>app.yaml</p>
<pre><code>--- 
handlers: 
  - 
    secure: always
    static_dir: build/static
    url: /static
  - 
    secure: always
    static_files: build/\1
    upload: build/.*\.(json|ico|js)$
    url: /(.*\.(json|ico|js))$
  - 
    secure: always
    static_files: build/index.html
    upload: build/index.html
    url: .*
manual_scaling: 
  instances: 1
runtime: nodejs16

</code></pre>"
"Cloud Run /Cloud Code deployment error in intellij<p>trying to follow the <a href=""https://cloud.google.com/code/docs/intellij/quickstart-cloud-run"" rel=""nofollow noreferrer"">Getting Started instructions</a> for Deploying a Cloud Run service with Cloud Code  in Intellij (deploying HelloWorld Flask app container with <code>Cloud Run: Deploy</code>) but getting the following error, any idea why this might be happening</p>
<p>it worked initially i.e. deployed the app on Cloud Run service using the same steps, and then started throwing this error after a week or so when trying to redeploy, there was no change in project settings.</p>
<p>intellij and docker versions are the latest.</p>
<p>authenticated to google cloud project with <code>gcloud auth login --update-adc</code></p>
<p>The local run works fine (<code>Cloud Run: Run Locally</code>),</p>
<p>but running the <code>Cloud Run: Deploy</code>  throws this &quot;code 89&quot; error</p>
<pre><code>Preparing Google Cloud SDK (this may take several minutes for first time setup)...

Creating skaffold file: /var/.../skaffold8013155926954225609.tmp

Configuring image push settings in /var/.../skaffold8013155926954225609.tmp


../Library/Application Support/cloud-code/bin/versions/../
  skaffold build --filename /var/.../skaffold8013155926954225609.tmp --tag latest --skip-tests=true

invalid skaffold config: getting minikube env: 
running [/Users/USER/Library/Application Support/google-cloud-tools-java/managed-cloud-sdk/LATEST/google-cloud-sdk/bin/
  minikube docker-env --shell none -p minikube --user=skaffold]

 - stdout: &quot;false exit code 89&quot;
 - stderr: &quot;&quot;
 - cause: exit status 89

 Failed to build and push Cloud Run container image. 
 Please ensure your builder settings are correct, network is available, you are logged in to a valid GCP project, and try again.
</code></pre>
<p>Edit: I see minikube error code 89: <a href=""https://minikube.sigs.k8s.io/docs/contrib/errorcodes/"" rel=""nofollow noreferrer"">ExGuestUnavailable</a> and it's an error code specific to the guest host, still unclear what might be causing this</p>","<p>Looks like an issue with skaffold attempting to communicate with minikube (which could be used for building images as well). Please try cleaning minikube</p>
<pre><code>minikube stop
minikube delete --all --purge
</code></pre>
<p>and try again.</p>"
"GCP Cloud Build tag release<p>I have a GCP cloud build yaml file that triggers on a new Tag in Github.</p>
<p>I have configure the latest tag to diplay on the app engine version but I need to configure the cloudbuild.yml file to replace the full stop on my tag to hyphen otherwise it fails on the deployment phase.</p>
<pre><code>  - id: web:set-env
    name: 'gcr.io/cloud-builders/gcloud'
    env:
      - &quot;VERSION=${TAG_NAME}&quot;
      
  #Deploy to google cloud app engine
  - id: web:deploy
    dir: &quot;.&quot;
    name: &quot;gcr.io/cloud-builders/gcloud&quot;
    waitFor: ['web:build']
    args:
      [
        'app',
        'deploy',
        'app.web.yaml',
        &quot;--version=${TAG_NAME}&quot;,
        --no-promote,
      ]
</code></pre>
<p>Tried using <code>--version=${TAG_NAME//./-}</code>, but getting an error on the deployment phase.</p>","<p>Managed to replace te fullstop with n hyphen by using the below step in the cloudbuild.yml file:</p>
<pre><code>  - id: tag:release
        name: 'gcr.io/cloud-builders/gcloud'
        args:
        - '-c'
        - |
          version=$TAG_NAME
          gcloud app deploy app.web.yaml --version=${version//./-} --no-promote
        entrypoint: bash
</code></pre>"
"Using Google Cloud Storage in The Google Cloud Build<p>According to the <a href=""https://cloud.google.com/build/docs"" rel=""nofollow noreferrer"">documentation</a> google cloud build can build from google cloud storage or a repository. But I'm having a hard time finding documentation on how to use files I upload to google cloud storage in the build steps. My intention is for a website where my jekyll source code is in the repository and my images are in google cloud storage; I'd like to add the images to the jekyll output and then upload to firebase with these <a href=""https://github.com/GoogleCloudPlatform/cloud-build-samples/tree/main/deploy-firebase-example"" rel=""nofollow noreferrer"">steps</a>.</p>
<p>Thanks for any guidance provided!</p>","<p>For a step in Cloud Build to copy files you would <a href=""https://cloud.google.com/build/docs/securing-builds/configure-access-for-cloud-build-service-account"" rel=""nofollow noreferrer"">provide your Cloud Build service account with IAM permission</a> to read the files in the bucket (probably something like: roles/storage.objectViewer).</p>
<p>Once the Service Account used by Cloud Build can read the files you would create a step to pull the files in before your step to deploy to Firebase.</p>
<p>A rough example:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps: 
  - name: 'gcr.io/cloud-builders/gsutil'
    args: ['cp', 'gs://$MY_BUCKET/*.jpg', '.']

  - name: gcr.io/$PROJECT_ID/firebase
    args: ['deploy', '--project=$PROJECT_ID', '--only=hosting']
</code></pre>"
"gcloud builds submit of Django website results in error ""does not have storage.objects.get access""<p>I'm trying to deploy my Django website with Cloud Run, as described in <a href=""https://cloud.google.com/python/django/run#deploy"" rel=""nofollow noreferrer"">Google Cloud Platform's documentation</a>, but I get the error <code>Error 403: 934957811880@cloudbuild.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object., forbidden</code> when running the command <code>gcloud builds submit --config cloudmigrate.yaml --substitutions _INSTANCE_NAME=trouwfeestwebsite-db,_REGION=europe-west6</code>.</p>
<p>The full output of the command is:  (the error is at the bottom)</p>
<pre><code>Creating temporary tarball archive of 119 file(s) totalling 23.2 MiB before compression.
Some files were not included in the source upload.

Check the gcloud log [C:\Users\Sander\AppData\Roaming\gcloud\logs\2021.10.23\20.53.18.638301.log] t
o see which files and the contents of the
default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn
more).

Uploading tarball of [.] to [gs://trouwfeestwebsite_cloudbuild/source/1635015198.74424-eca822c138ec
48878f292b9403f99e83.tgz]
ERROR: (gcloud.builds.submit) INVALID_ARGUMENT: could not resolve source: googleapi: Error 403: 934957811880@cloudbuild.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object., forbidden
</code></pre>
<p>On the level of my storage bucket, I granted 934957811880@cloudbuild.gserviceaccount.com the permission Storage Object Viewer, as I see on <a href=""https://cloud.google.com/storage/docs/access-control/iam-roles"" rel=""nofollow noreferrer"">https://cloud.google.com/storage/docs/access-control/iam-roles</a> that this covers <code>storage.objects.get</code> access.
I also tried by granting Storage Object Admin and Storage Admin.</p>
<p>I also added the &quot;Viewer&quot; role on IAM level (<a href=""https://console.cloud.google.com/iam-admin/iam"" rel=""nofollow noreferrer"">https://console.cloud.google.com/iam-admin/iam</a>) for 934957811880@cloudbuild.gserviceaccount.com, as suggested in <a href=""https://stackoverflow.com/a/68303613/5433896"">https://stackoverflow.com/a/68303613/5433896</a> and <a href=""https://github.com/google-github-actions/setup-gcloud/issues/105"" rel=""nofollow noreferrer"">https://github.com/google-github-actions/setup-gcloud/issues/105</a>, but it seems fishy to me to give the account such a broad role.</p>
<p>I enabled Cloud run in the Cloud Build permissons tab: <a href=""https://console.cloud.google.com/cloud-build/settings/service-account?project=trouwfeestwebsite"" rel=""nofollow noreferrer"">https://console.cloud.google.com/cloud-build/settings/service-account?project=trouwfeestwebsite</a></p>
<p>With these changes, I still get the same error when running the gcloud builds submit command.</p>
<p>I don't understand what I could be doing wrong in terms of credentials/authentication (<a href=""https://stackoverflow.com/a/68293734/5433896"">https://stackoverflow.com/a/68293734/5433896</a>). I didn't change my google account password nor revoked permissions of that account to the Google Cloud SDK since I initialized that SDK.</p>
<p>Do you see what I'm missing?</p>
<p>The content of my <code>cloudmigrate.yaml</code> is:</p>
<pre><code>steps:
  - id: &quot;build image&quot;
    name: &quot;gcr.io/cloud-builders/docker&quot;
    args: [&quot;build&quot;, &quot;-t&quot;, &quot;gcr.io/${PROJECT_ID}/${_SERVICE_NAME}&quot;, &quot;.&quot;]

  - id: &quot;push image&quot;
    name: &quot;gcr.io/cloud-builders/docker&quot;
    args: [&quot;push&quot;, &quot;gcr.io/${PROJECT_ID}/${_SERVICE_NAME}&quot;]

  - id: &quot;apply migrations&quot;
    name: &quot;gcr.io/google-appengine/exec-wrapper&quot;
    args:
      [
        &quot;-i&quot;,
        &quot;gcr.io/$PROJECT_ID/${_SERVICE_NAME}&quot;,
        &quot;-s&quot;,
        &quot;${PROJECT_ID}:${_REGION}:${_INSTANCE_NAME}&quot;,
        &quot;-e&quot;,
        &quot;SETTINGS_NAME=${_SECRET_SETTINGS_NAME}&quot;,
        &quot;--&quot;,
        &quot;python&quot;,
        &quot;manage.py&quot;,
        &quot;migrate&quot;,
      ]

  - id: &quot;collect static&quot;
    name: &quot;gcr.io/google-appengine/exec-wrapper&quot;
    args:
      [
        &quot;-i&quot;,
        &quot;gcr.io/$PROJECT_ID/${_SERVICE_NAME}&quot;,
        &quot;-s&quot;,
        &quot;${PROJECT_ID}:${_REGION}:${_INSTANCE_NAME}&quot;,
        &quot;-e&quot;,
        &quot;SETTINGS_NAME=${_SECRET_SETTINGS_NAME}&quot;,
        &quot;--&quot;,
        &quot;python&quot;,
        &quot;manage.py&quot;,
        &quot;collectstatic&quot;,
        &quot;--verbosity&quot;,
        &quot;2&quot;,
        &quot;--no-input&quot;,
      ]

substitutions:
  _INSTANCE_NAME: trouwfeestwebsite-db
  _REGION: europe-west6
  _SERVICE_NAME: invites-service
  _SECRET_SETTINGS_NAME: django_settings

images:
  - &quot;gcr.io/${PROJECT_ID}/${_SERVICE_NAME}&quot;
</code></pre>
<p>Thank you very much for any help.</p>","<p>The following solved my problem.</p>
<ol>
<li><p>DazWilkin was right in saying:</p>
<blockquote>
<p>it's incorrectly|unable to reference the bucket</p>
</blockquote>
<p>(comment upvote for that, thanks!!). In my secret (configured on Secret Manager; or alternatively you can put this in a .env file at project root folder level and making sure you don't exclude that file for deployment in a .gcloudignore file then), I now
have set:</p>
<p><code>GS_BUCKET_NAME=trouwfeestwebsite_sasa-trouw-bucket</code> (project ID + underscore + storage bucket ID)</p>
<p>instead of
<code>GS_BUCKET_NAME=sasa-trouw-bucket</code></p>
<p>Whereas the tutorial in fact stated I had to set the first, I had set the latter since I found the underscore splitting weird, nowhere in the tutorial had I seen something similar, I thought it was an error in the tutorial.</p>
<p>Adapting the <code>GS_BUCKET_NAME</code> changed the error of gcloud builds submit to:</p>
<pre><code>Creating temporary tarball archive of 412 file(s) totalling 41.6 MiB before compression.
Uploading tarball of [.] to [gs://trouwfeestwebsite_cloudbuild/source/1635063996.982304-d33fef2af77a4744a3bb45f02da8476b.tgz]
ERROR: (gcloud.builds.submit) PERMISSION_DENIED: service account &quot;934957811880@cloudbuild.gserviceaccount.com&quot; has insufficient permission to execute the build on project &quot;trouwfeestwebsite&quot;
</code></pre>
<p>That would mean that least now the bucket is found, only a permission is missing.</p>
<p>Edit (a few hours later): I noticed this <code>GS_BUCKET_NAME=trouwfeestwebsite_sasa-trouw-bucket</code> (project ID + underscore + storage bucket ID) setting then caused trouble in a later stage of the deployment, when deploying the static files (last step of the cloudmigrate.yaml). This seemed to work for both (notice that the project ID is no longer in the GS_BUCKET_NAME, but in its separate environment variable):</p>
<pre><code>DATABASE_URL=postgres://myuser:mypassword@//cloudsql/mywebsite:europe-west6:mywebsite-db/mydb
GS_PROJECT_ID=trouwfeestwebsite
GS_BUCKET_NAME=sasa-trouw-bucket
SECRET_KEY=my123Very456Long789Secret0Key
</code></pre>
</li>
<li><p>Then, it seemed that there also really was a permissions problem:</p>
<ul>
<li><p>for the sake of completeness, afterwards, I tried adding the permissions as stated in <a href=""https://stackoverflow.com/a/55635575/5433896"">https://stackoverflow.com/a/55635575/5433896</a>, but it didn't prevent the error I reported in my question.</p>
</li>
<li><p>This answer however helped me: <a href=""https://stackoverflow.com/a/33923292/5433896"">https://stackoverflow.com/a/33923292/5433896</a>. =&gt;
Setting the <code>Editor</code> role on the cloudbuild service account helped the gcloud builds submit command to continue its process further without throwing the permissions error.</p>
</li>
</ul>
</li>
<li><p>If you have the same problem: I think a few things mentioned in my question can also help you - for example I think doing this may also have been important:</p>
<blockquote>
<p>I enabled Cloud run in the Cloud Build permissons tab:
<a href=""https://console.cloud.google.com/cloud-build/settings/service-account?project=trouwfeestwebsite"" rel=""nofollow noreferrer"">https://console.cloud.google.com/cloud-build/settings/service-account?project=trouwfeestwebsite</a></p>
</blockquote>
</li>
</ol>"
"Specifying Node.js version for Google Cloud App Engine Flexible<p>I'm trying to deploy a GCloud App Engine Flexible service. I have a yaml file, in which it has the Node.js runtime and the <code>env</code> specified.</p>
<pre class=""lang-yaml prettyprint-override""><code>runtime: nodejs
env: flex
</code></pre>
<p>As the documentation says &quot;You can specify a different Node.js version in your application's package.json file by using the engines field.&quot;, I also added the following to <code>package.json</code>:</p>
<pre class=""lang-json prettyprint-override""><code>&quot;name&quot;: &quot;@bindr/dev&quot;,
&quot;version&quot;: &quot;1.0.0&quot;,
&quot;engines&quot;: {
  &quot;node&quot;: &quot;&gt;=14.0.0&quot;
},
</code></pre>
<p>However, when I run <code>gcloud app deploy</code>, I get the following error:</p>
<pre><code>error @bindr/dev@1.0.0: The engine &quot;node&quot; is incompatible with this module. Expected version &quot;&gt;=14.0.0&quot;. Got &quot;12.19.0&quot;
</code></pre>
<p>It seems like the deployment process doesn't take the <code>engines</code> property into account, because even if I specify an invalid version (e.g. <code>&gt;=18.0.0</code>) it still doesn't complain, only the <code>yarn install</code> fails. How can I make the build process use the specified Node version?</p>
<p>I found that I could specify the version of Node in <code>cloudbuild.yaml</code> for the certain steps of the build, like so:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: node:node-14.10.0
    args: ['predeploy.js', 'content-server']
  - name: 'gcr.io/cloud-builders/yarn:node-14.17.1'
    args: ['install']
  - name: 'gcr.io/cloud-builders/gcloud'
    args: ['app', 'deploy']
timeout: '900s'
</code></pre>
<p>In this process, the <code>yarn install</code> step succeeds, but the <code>gcloud app deploy</code> step still fails while trying to install the dependencies (I couldn't find how I could specify the node version to <code>gcr.io/cloud-builders/gcloud</code>, it doesn't seem to be such tag).</p>
<p>I also checked and the same <code>12.19.0</code> version is running on the production instances, so it is not only the build environment that has an older version.</p>
<p>What am I doing wrong?</p>","<p>I encountered the same issue and created an issue for it <a href=""https://issuetracker.google.com/issues/203594728"" rel=""nofollow noreferrer"">here</a>. I suspect it is a bug with Google App Engine and not with your app.</p>
<p>As a workaround, I ended up using a <a href=""https://cloud.google.com/appengine/docs/flexible/custom-runtimes/build"" rel=""nofollow noreferrer"">custom runtime</a> for my app. To do this, in your GAE configuration file you switch from <code>runtime: nodejs</code> to <code>runtime: custom</code> and add a Dockerfile to your project root. There are good docs on writing a dockerfile <a href=""https://docs.docker.com/language/nodejs/build-images/"" rel=""nofollow noreferrer"">here</a> but here is a simple one you can use:</p>
<pre><code># syntax=docker/dockerfile:1

FROM node:14.10.0
ENV NODE_ENV=production

WORKDIR /app

COPY [&quot;package.json&quot;, &quot;package-lock.json*&quot;, &quot;./&quot;]

RUN npm install --production

COPY . .

CMD [ &quot;node&quot;, &quot;server.js&quot; ]
</code></pre>
<p>You'll probably also want a <code>.dockerignore</code> file that at least contains <code>node_modules</code>.</p>
<p>Ultimately I think a GAE fix would be nicer, since it would be simpler to just configure your node version in <code>package.json</code>, like you're supposed to be able to. But this should be enough to get things working and sidestep what appears to be a GAE bug.</p>"
"Have the Google cloud-builder images moved to Artifact Registry?<p>Are there updated locations for <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/gcloud"" rel=""nofollow noreferrer"">Google Cloud Builders</a>? Google is encouraging us to move away from the Google Container Registry to the Artifact Registry but the Cloud Builder images are still addressed by eg <code>gcr.io/cloud-builders/mvn:3.5.0-jdk-8</code> which is a Container Registry address.
I'm in the process of revising all my builds to use Artifact Registry and I would rather not have to make another pas through them to change these <code>gcr.io/cloud-builders...</code> references.</p>
<p>Does anyone know?</p>","<p>Container Registry will still continue to work and will not go away soon. You can still use both services in the same project, that was stated in this <a href=""https://stackoverflow.com/a/65731142/16531380"">post</a> that discusses the differences of Container Registry and Artifact Registry.</p>
<p>There's a guide in <a href=""https://cloud.google.com/artifact-registry/docs/docker/copy-from-gcr"" rel=""nofollow noreferrer"">copying images from Container Registry</a> to Artifact Registry that you can also check in your transition process.</p>
<p>While the official cloud builders aren't hosted yet in Artifact Registry, you have an option to clone the source code and deploy it yourself. Furthermore refer to this <a href=""https://cloud.google.com/artifact-registry/docs/transition/changes-gcp"" rel=""nofollow noreferrer"">link</a> for changes in building and deploying in Google Cloud. There's an available guide in <a href=""https://cloud.google.com/artifact-registry/docs/transition/setup-gcr-repo"" rel=""nofollow noreferrer"">transitioning repositories with gcr.io domain</a>. Just a note, this feature is still on Alpha, and this feature might have limited support. You can further check the previous link for further details in this feature.</p>
<p>Lastly, feel free to submit a <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/issues/new?assignees=&amp;labels=enhancement%2C+help+wanted&amp;template=feature_request.md&amp;title=%5BFR%5D"" rel=""nofollow noreferrer"">feature request</a> on GitHub.</p>"
"App engine is not serving images of public folder in react<p>I hosted a react app in app engine , and i have a lot of pictures in the public folder how are displayed correctly in my local machine , but when i deploy it in standard environnement , all the files are deployed , i checked the sources in url images and he serve them but they seems corrupted (when i download them) maybe i did something wrong in the configuration ? the only thing that i can acess is the favicon . this is what i get in source .
<a href=""https://i.stack.imgur.com/VRtLC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VRtLC.png"" alt=""source image"" /></a></p>
<p>and my app.yaml is :</p>
<pre><code>runtime: nodejs12   
service: default    
instance_class: F4_1G
handlers:
   - url: /static
    static_dir: build/static

   - url: /assets
    static_dir: build/assets

  - url: /(.*\.(json|ico|js))$
    static_files: build/\1
    upload: build/.*\.(json|ico|js)$

  - url: .*
    static_files: build/index.html
    upload: build/index.html
</code></pre>",<p>Your <code>app.yaml</code> does not have a handler for <code>/media</code>. So the GCP returns <code>index.html</code> when accessing <code>viper8.JPG</code>. Add handler for <code>/media</code>.</p>
"How can I create CI/CD pipeline with cloudbuild.yaml in GCP?<p>I am trying to create a simple CI/CD pipeline. After the client makes <code>git push</code>, it will start a trigger with the below <code>cloudbuilder.yaml</code>:</p>
<pre class=""lang-sh prettyprint-override""><code># steps:
# - name: 'docker/compose:1.28.2'
#   args: ['up', '-d']
# - name: &quot;docker/compose:1.28.2&quot;
#   args: [&quot;build&quot;]
# images: ['gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose']
# - name: 'gcr.io/cloud-builders/docker'
#   id: 'backend'
#   args: ['build','-t', 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:latest','.']
# - name: 'gcr.io/cloud-builders/docker'
#   args: ['push', 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:latest']

# steps:
# - name: 'docker/compose:1.28.2'
#   args: ['up', '-d']
# - name: &quot;docker/compose:1.28.2&quot;
#   args: [&quot;build&quot;]
# images:
# - 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose'

# In this directory, run the following command to build this builder.
# $ gcloud builds submit . --config=cloudbuild.yaml
substitutions:
  _DOCKER_COMPOSE_VERSION: 1.28.2
steps:
- name: 'docker/compose:1.28.2'
  args:
  - 'build'
  - '--build-arg'
  - 'DOCKER_COMPOSE_VERSION=${_DOCKER_COMPOSE_VERSION}'
  - '-t'
  - 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:latest'
  - '-t'
  - 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:${_DOCKER_COMPOSE_VERSION}'
  - '.'
- name: 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose'
  args: ['version']

images:
- 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:latest'
- 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:${_DOCKER_COMPOSE_VERSION}'
tags: ['cloud-builders-community']

</code></pre>
<p>It returns the error below: it cannot create images in the repository. How can I solve this issue?</p>
<pre class=""lang-sh prettyprint-override""><code>ERROR: failed to pull because we ran out of retries.
ERROR
ERROR: build step 1 &quot;gcr.io/internal-invoicing-solution/cloudbuild-demo-dockercompose&quot; failed: error pulling build step 1 &quot;gcr.io/internal-invoicing-solution/cloudbuild-demo-dockercompose&quot;: generic::unknown: retry budget exhausted (10 attempts): step exited with non-zero status: 1
```


</code></pre>","<p>Before pulling your image in the 2nd step, you need to push it. When you declare the <code>images</code> at the end of your yaml definition, the image are pushed automatically at the end on the pipeline. Here you need it in the middle.</p>
<hr />
<p><strong>EDIT 1</strong></p>
<p>I just added a docker push step, copy and paste from your comments. Does it work?</p>
<pre><code>substitutions:
  _DOCKER_COMPOSE_VERSION: 1.28.2
steps:
- name: 'docker/compose:1.28.2'
  args:
  - 'build'
  - '--build-arg'
  - 'DOCKER_COMPOSE_VERSION=${_DOCKER_COMPOSE_VERSION}'
  - '-t'
  - 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:latest'
  - '-t'
  - 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:${_DOCKER_COMPOSE_VERSION}'
  - '.'
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:latest']
- name: 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose'
  args: ['version']

images:
- 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:latest'
- 'gcr.io/$PROJECT_ID/cloudbuild-demo-dockercompose:${_DOCKER_COMPOSE_VERSION}'
tags: ['cloud-builders-community']

</code></pre>"
"PEP8 integration with Google Cloud Build<p>Is there any way I can integrate a Code linting build step in Google Cloud Build, specifically Pylint, and any code that gets a score of less than 8 would fail to build?</p>
<p>My CICD setup moves code from Github to Google Cloud Composer (Airflow) GCS Bucket.</p>","<h2>Quick answer</h2>
<p>You could do so by adding a step prior to other build steps just as you would do to run <a href=""https://cloud.google.com/build/docs/building/build-python#:%7E:text=Add-,unit%20tests,-%3A%20If%20you%27ve%20defined"" rel=""nofollow noreferrer"">unit tests</a>, and execute the Pylint command in there with the <a href=""https://pylint.pycqa.org/en/latest/technical_reference/features.html#:%7E:text=register%20additional%20checkers.-,fail-under,-Specify%20a%20score"" rel=""nofollow noreferrer""><code>fail-under</code></a> option as @Pierre.Sassoulas mentioned, so the building process will stop if it fails under a certain score (ie, fail if &lt;8.0).</p>
<p>For instance:</p>
<pre class=""lang-yaml prettyprint-override""><code>  # Build step to run pylint on my prod.py file. 
  # It will stop the process if the score is less than 8.0.
  - name: python
    entrypoint: python
    args: [&quot;-m&quot;, &quot;pylint&quot;, &quot;prod.py&quot;, &quot;--fail-under=8.0&quot;]
</code></pre>
<hr />
<h2>Example</h2>
<p>As a reference, I've made a small running example using FastAPI, inspired by <a href=""https://medium.com/@ryangordon210/build-a-python-ci-cd-system-code-quality-with-pylint-f6dea78461c6"" rel=""nofollow noreferrer"">this article</a>:</p>
<p><strong>cloudbuild.yaml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  # Install dependencies
  - name: python
    entrypoint: pip3
    args: [&quot;install&quot;, &quot;-r&quot;, &quot;./requirements.txt&quot;, &quot;--user&quot;]
  # Build step to run pylint on my prod.py file. 
  # It will stop the process if the score is less than 8.0.
  - name: python
    entrypoint: python
    args: [&quot;-m&quot;, &quot;pylint&quot;, &quot;prod.py&quot;, &quot;--fail-under=8.0&quot;]
  # Yay!
  - name: 'bash'
    args: ['echo', 'Success!']
</code></pre>
<p><strong>FastAPI <a href=""https://fastapi.tiangolo.com/tutorial/first-steps/"" rel=""nofollow noreferrer"">sample code</a></strong>:</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
async def root():
    return {&quot;message&quot;: &quot;Hello World&quot;}
</code></pre>
<p><strong>Result (Fail):</strong></p>
<pre class=""lang-sh prettyprint-override""><code>Starting Step #1
Step #1: Already have image (with digest): python
Step #1: ************* Module prod
Step #1: prod.py:1:0: C0114: Missing module docstring (missing-module-docstring)
Step #1: prod.py:4:0: C0116: Missing function or method docstring (missing-function-docstring)
Step #1:
Step #1: -----------------------------------
Step #1: Your code has been rated at 5.00/10
Step #1:
Finished Step #1
2021/11/11 12:29:23 Step Step #1 finished
2021/11/11 12:29:23 status changed to &quot;ERROR&quot;
ERROR
ERROR: build step 1 &quot;python&quot; failed: exit status 16
2021/11/11 12:29:23 Build finished with ERROR status
</code></pre>
<p>Now, modifying sample code:</p>
<pre class=""lang-py prettyprint-override""><code>&quot;&quot;&quot;
Basic API taken from https://fastapi.tiangolo.com/tutorial/first-steps/
&quot;&quot;&quot;
from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
async def root():
    &quot;&quot;&quot;
    Top level endpoint to test server
    &quot;&quot;&quot;
    return {&quot;message&quot;: &quot;Hello World&quot;}
</code></pre>
<p><strong>Result (Success):</strong></p>
<pre class=""lang-sh prettyprint-override""><code>Starting Step #1
Step #1: Already have image (with digest): python
Step #1:
Step #1: ------------------------------------
Step #1: Your code has been rated at 10.00/10
Step #1:
Finished Step #1
2021/11/11 12:43:13 Step Step #1 finished
Starting Step #2
Step #2: Pulling image: bash
Step #2: Using default tag: latest
.
.
.
Step #2: Success!
Finished Step #2
2021/11/11 12:43:15 Step Step #2 finished
2021/11/11 12:43:15 status changed to &quot;DONE&quot;
DONE
</code></pre>"
"How to dynamically pass service account to Cloud Build config?<p>I was trying to parametrize service account in Cloud Build configuration:</p>
<pre><code>steps:
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    args: [ 'gcloud', '--help']
substitutions:
  _SERVICE_ACCOUNT: my-service-account@developer.gserviceaccount.com
serviceAccount: 'projects/$PROJECT_ID/serviceAccounts/${_SERVICE_ACCOUNT}'
options:
  logging: CLOUD_LOGGING_ONLY
</code></pre>
<p>but this returns error:</p>
<pre><code>ERROR: (gcloud.builds.submit) INVALID_ARGUMENT: could not parse service account URL
</code></pre>
<p>So, I guess, custom substitutions are not supported in the service account field.
If this is the case, what is a recommended way to reuse cloud build configuration with different service accounts?</p>","<p>From my deep understanding of Cloud build, it can't work.</p>
<p>Think about the architecture. When you start your Cloud Build instance, you have to load all the Google Cloud standard context, such as the metadata server. the metadata server serves the credentials and it must be known when the instance starts.</p>
<p>The substitution variable are interpreted at runtime, so after the instance start.</p>
<hr />
<p>Therefore, you can't interpret the service account email with a substitution variable before the instance start, but only after. But after, it's too late to load the correct service account in the metadata server.</p>"
"Cloudbuild to trigger Github merge to master branch<p>I have a Google Cloudbuild pipeline, that tests the code committed to the dev branch of my Github repo, and deploys the code to the dev environment. I want to add another step to the pipeline, which would merge my dev branch with the prod branch, upon successful deployment to the dev environment. Is there any way to implement this?</p>","<p>You probably can do that in your 'cloudbuild.yaml' (or another name) file which is 'executed' by the Cloud Build trigger.</p>
<p>In my experience you will need a pair of keys - the public key should be in GitHub, the private key value - stored as a secret in the Secret Manager.</p>
<p>Thus, in the yaml file you might need to use (change according to your case, do not use like this):</p>
<pre><code>  volumes:
  - name: 'ssh'
    path: /root/.ssh
</code></pre>
<p>So far:</p>
<ol>
<li>get the private key (from the secret) and save it as a a file <code>/root/.ssh/github</code> by using something like <code>gcloud secrets versions access latest ...</code></li>
<li>change permissions <code>chmod 600 /root/.ssh/github</code></li>
<li>create a config file:</li>
</ol>
<pre><code>cat &lt;&lt;EOF &gt;/root/.ssh/config
Hostname github.com
IdentityFile /root/.ssh/github
EOF
</code></pre>
<ol start=""4"">
<li>save GitHub public key to a known_hosts file <code>ssh-keyscan -t rsa github.com &gt; /root/.ssh/known_hosts</code></li>
</ol>
<p>After that, you (actually your Cloud Build service account using provided credentials) can do with GitHub repository whatever is required (subject to permissions granted in GitHub) using ordinary <code>git</code> commands.</p>"
"Application Default Credentials in Google Cloud Build<p>Within my code, I am attempting to gather the Application Default Credentials from the associated service account in Cloud Build:</p>
<pre class=""lang-py prettyprint-override""><code>from google.auth import default

credentials, project_id = default()
</code></pre>
<p>This works fine in my local space because I have set the environment variable <code>GOOGLE_APPLICATION_CREDENTIALS</code> appropriately. However, when this line is executed (via a test step in my build configuration) within Cloud Build, the following error is raised:</p>
<pre><code>google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. 
Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. 
For more information, please see https://cloud.google.com/docs/authentication/getting-started
</code></pre>
<p>This is confusing me because, according to the docs:</p>
<blockquote>
<p>By default, Cloud Build uses a special service account to execute builds on your behalf. This service account is called the Cloud Build service account and it is created automatically when you enable the Cloud Build API in a Google Cloud project.
<a href=""https://cloud.google.com/build/docs/securing-builds/configure-user-specified-service-accounts"" rel=""nofollow noreferrer"">Read Here</a></p>
</blockquote>
<blockquote>
<p>If the environment variable GOOGLE_APPLICATION_CREDENTIALS isn't set, ADC uses the service account that is attached to the resource that is running your code.
<a href=""https://cloud.google.com/docs/authentication/production"" rel=""nofollow noreferrer"">Read Here</a></p>
</blockquote>
<p>So why is the default call not able to access the Cloud Build service account credentials?</p>","<p>There is a trick: you have to define the network to use in your Docker build. Use the parameter <code>--network=cloudbuild</code>, like that</p>
<pre><code>steps:
  - name: gcr.io/cloud-builders/docker
    entrypoint: 'docker'
    args: 
      - build
      - '--no-cache'
      - '--network=cloudbuild'
      - '-t'
      - '$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:$COMMIT_SHA'
      - .
      - '-f' 
      - 'Dockerfile'
...
</code></pre>
<p>You can find the documentation <a href=""https://cloud.google.com/build/docs/build-config-file-schema#network"" rel=""noreferrer"">here</a></p>"
"Module not found in Nest JS after deploying to App engine<p>I tried to deploy a Nest JS app to App Engine through Google Build Cloud &amp; Manually
Both tries gives same error Modules not found</p>
<p><a href=""https://i.stack.imgur.com/i2hec.png"" rel=""nofollow noreferrer"">Error when deploying manually through Shell</a></p>
<p><a href=""https://i.stack.imgur.com/hw8ae.png"" rel=""nofollow noreferrer"">Error when dploying using clouf build</a></p>
<p>My cloudbuild.yaml</p>
<pre><code>  - name: node:14.15.1
    entrypoint: npm
    args: [&quot;install&quot;]
  - name: node:14.15.1
    entrypoint: npm
    args: [&quot;run&quot;, &quot;build&quot;]
  - name: node:14.15.1
    entrypoint: npm
    args: [&quot;run&quot;, &quot;create-env&quot;]
    env:
      - &quot;_APP_LUNE_DEV_TOKEN=${_APP_LUNE_DEV_TOKEN}&quot;
      - &quot;_BUCKET_NAME=${_BUCKET_NAME}&quot;
      - &quot;_FIREBASE_API_KEY=${_FIREBASE_API_KEY}&quot;
      - &quot;_FIREBASE_APP_ID=${_FIREBASE_APP_ID}&quot;
      - &quot;_FIREBASE_AUTH_DOMAIN=${_FIREBASE_AUTH_DOMAIN}&quot;
      - &quot;_FIREBASE_MESSAGING_SENDER_ID=${_FIREBASE_MESSAGING_SENDER_ID}&quot;
      - &quot;_FIREBASE_PROJECT_ID=${_FIREBASE_PROJECT_ID}&quot;
      - &quot;_FIREBASE_STORAGE_BUCKET=${_FIREBASE_STORAGE_BUCKET}&quot;
      - &quot;_GOOGLE_APPLICATION_CREDENTIALS=${_GOOGLE_APPLICATION_CREDENTIALS}&quot;
      - &quot;_LUNE_API_BASE_URL=${_LUNE_API_BASE_URL}&quot;
      - &quot;_STRIPE_SECRET_KEY=${_STRIPE_SECRET_KEY}&quot;
  - name: &quot;gcr.io/cloud-builders/gcloud&quot;
    args: [&quot;app&quot;, &quot;deploy&quot;]
timeout: &quot;1600s&quot;
options:
  logging: CLOUD_LOGGING_ONLY
</code></pre>
<p>My app.yaml</p>
<pre><code>runtime: nodejs14

service: backend
</code></pre>
<p>What I'm doing wrong ?</p>","<p>The problem was caused by Nest Js CLI Aliases, When I changed src/example/service to ../example/service the deployment worked
It's recommended to use typescript aliases in tsconfig file instead of CLI</p>"
"GCP project owner can't list builds or triggers<p>I have the <code>owner</code> role on a GCP project. I created 2 build triggers, which are visible to me in the console.</p>
<p><a href=""https://i.stack.imgur.com/bQWSZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bQWSZ.png"" alt=""GCP console showing 2 build triggers"" /></a></p>
<p>I want to list these triggers using a service account and the <a href=""https://github.com/googleapis/nodejs-cloudbuild"" rel=""nofollow noreferrer"">node.js client</a>.</p>
<p>I made a custom role and assigned <code>cloudbuild.builds.viewer</code> and I think the following should work, but it does not:</p>
<pre><code>$ gcloud projects get-iam-policy myprojectid --flatten=&quot;bindings[].members&quot; --format='table(bindings.role)' --filter=&quot;bindings.members:build-trigger-viewer@myprojectid.iam.gserviceaccount.com&quot;
ROLE
roles/cloudbuild.builds.viewer
$ gcloud iam service-accounts keys create trigger-viewer-credentials.json --iam-account=build-trigger-viewer@myprojectid.iam.gserviceaccount.com
$ GOOGLE_APPLICATION_CREDENTIALS=trigger-viewer-credentials.json node
&gt; var {CloudBuildClient} = require(&quot;@google-cloud/cloudbuild&quot;)
&gt; var cb = new CloudBuildClient()
&gt; await cb.listBuildTriggers({projectId: &quot;myprojectid&quot;})
[ [], null, null ]
</code></pre>
<p>Backing up from that, I've realised that for some reason listing these triggers or even builds from <code>gcloud</code> also fails.</p>
<pre class=""lang-bash prettyprint-override""><code>$ gcloud beta builds triggers list
Listed 0 items.
$ gcloud builds list
Listed 0 items.
</code></pre>
<p>Just to verify I'm not insane:</p>
<pre class=""lang-bash prettyprint-override""><code>$ gcloud projects get-iam-policy myprojectid | rg -A 1 user:myuser@mydomain.com
  - user:myuser@mydomain.com
  role: roles/container.admin
--
  - user:myuser@mydomain.com
  role: roles/gkehub.admin
--
  - user:myuser@mydomain.com
  role: roles/owner

</code></pre>
<p>Does anyone have any thoughts on what might be happening?</p>","<p>As mentioned by the documentation, the command <a href=""https://cloud.google.com/sdk/gcloud/reference/beta/builds/triggers/list"" rel=""nofollow noreferrer""><code>gcloud beta builds triggers list</code></a> will list all the triggers in the global scope.</p>
<p>In your image you show that the triggers are in <code>northamerica-northeast1</code> so you may need to use the <code>—-region</code> flag.</p>
<p>In the case of the code you may need to set the <code>parent</code> as especified in this <a href=""https://cloud.google.com/nodejs/docs/reference/cloudbuild/latest/cloudbuild/protos.google.devtools.cloudbuild.v1.listbuildtriggersrequest"" rel=""nofollow noreferrer"">doc</a> which is the region where the trigger lives (<a href=""https://cloud.google.com/build/docs/api/reference/rest/v1/projects.triggers/list#query-parameters"" rel=""nofollow noreferrer"">API reference</a>), otherwise you're getting global triggers.</p>
<pre><code>await cb.listBuildTriggers({
projectId: &quot;myprojectid&quot;, 
parent: &quot;projects/myprojectid/locations/northamerica-northeast1&quot;
})
</code></pre>
<p>If the account didn't have permissions, you would get a 403 error and not empty results.</p>
<p>This also applies for <code>gcloud builds list</code>.</p>"
"Docker Compose network_mode - adding argument causes local testing to fail<p>I'm trying to build an application that is able to use local integration testing via Docker Compose with Google Cloud emulator containers, while <strong>also</strong> being able to run that same Docker Compose configuration on a Docker-based CI/CD tool (Google Cloud Build).</p>
<p>The kind of <code>docker-compose.yml</code> configuration I'm using is:</p>
<pre class=""lang-yaml prettyprint-override""><code>version: '3.7'
services:
  
  main-application:
    build:
      context: .
      target: dev
    image: main-app-dev
    container_name: main-app-dev
    network_mode: $DOCKER_NETWORK
    environment:
      - MY_ENV=my_env
    command: [&quot;sh&quot;, &quot;-c&quot;, &quot;PYTHONPATH=./ python app/main.py&quot;]
    volumes:
      - ~/.config:/home/appuser/.config
      - ./app:/home/appuser/app
      - ./tests:/home/appuser/tests
    depends_on:
      - firestore

  firestore:
    image: google/cloud-sdk
    container_name: firestore
    network_mode: $DOCKER_NETWORK
    environment:
      - GCP_PROJECT_ID=dummy-project
    command: [&quot;sh&quot;, &quot;-c&quot;, &quot;gcloud beta emulators firestore start --project=$$GCP_PROJECT_ID --host-port=0.0.0.0:9000&quot;]
    ports:
      - &quot;9000:9000&quot;
</code></pre>
<p>I added the <code>network_mode</code> arguments to enable the configuration to use the &quot;<em>cloudbuild</em>&quot; network type available on the CI/CD pipeline, which is currently working perfectly. However this network configuration is not available to local Docker, hence why I've tried to use environment variables to enable the switch depending on local vs Cloud Build CI/CD environment.</p>
<p>Before I added these <code>network_mode</code> params/args for the CI/CD, the local testing was working just fine. However since I added them, my application either can't run, or can't connect to its accompanying services, like the firestore one specified in the YAML above.
I have tried the following valid Docker network modes with no success:</p>
<ul>
<li><strong>&quot;bridge&quot;</strong> - <em>runs the service, but doesn't allow connection between containers</em></li>
<li><strong>&quot;host&quot;</strong> - <em>doesn't allow the service to run because of not being compatible with assigning ports</em></li>
<li><strong>&quot;none&quot;</strong> - <em>doesn't allow the service to connect externally</em></li>
<li><strong>&quot;service&quot;</strong> - <em>doesn't allow the service to run due to invalid mode/service</em></li>
</ul>
<p>Anyone able to provide advice on what I'm missing here?
I would assume one of these network modes would be what Docker Compose would be using if the <code>network_mode</code> is not assigned, so I'm not sure why all of them are failing.</p>
<p>I want to avoid having a separate cloud build file for the remote and local configurations, and would also like to avoid the hassle of setting up my own docker network locally. Ideally if there were some way of only applying <code>network_mode</code> only remotely, that would work best in my case I think.</p>
<p>TL;DR:
Specifying <code>network_mode</code> does not give me the same result as not specifying it when running <code>docker-compose up</code> locally.
Due to running the configuration in the cloud I can't avoid specifying it.</p>","<p>Found a solution thanks to <a href=""https://stackoverflow.com/questions/43754095/how-to-join-the-default-bridge-network-with-docker-compose-v2"">this thread</a> and the comment by David Maze.</p>
<p>As far as I understand it, Docker Compose when not provided a specific <code>network_mode</code> for all the containers, creates its own private default network, named after the folder in which the <code>docker-compose.yml</code> file exists (as default).</p>
<p>Specifying a network mode like the default &quot;bridge&quot; network, without using this custom network created by docker compose means container discovery between services isn't possible, as in <code>main-application</code> couldn't find the <code>firestore:9000</code> container.</p>
<hr />
<p>Basically all I had to do was set the <code>network_mode</code> variable to <code>myapplication_default</code>, if the folder where docker-compose.yml sat in was called &quot;MyApplication&quot;, to force app apps to use the same custom network set up in <code>docker-compose up</code></p>"
"gcloud scp in cloud build fails due to known hosts problem<p>One step in my cloud build is to copy files from a VM in another project. After a series of problems, I've set up the service account access, and can successfully do this scp from my own workstation. However, in cloud build itself, I get this error on this step:</p>
<p><code>2022-08-03 22:21:32.170 EDTStep #4 - &quot;Copy in static images&quot;: Failed to add the host to the list of known hosts (/builder/home/.ssh/google_compute_known_hosts).</code></p>
<p>The step runs a shell script. The pertinent part does this:</p>
<pre><code>    args:
      - '-c'
      - ./auto-image-xfer.sh
    id: Copy in static images 
    entrypoint: bash
</code></pre>
<p>The shell script does this:
<code>gcloud compute scp --recurse user@vmname:/path/to/images ./destination --zone us-central1-a --ssh-key-file=./google_compute_engine --project &quot;projectname&quot;</code></p>
<p>Again, I hasten to add that I worked out a series of service account issues that originally prevented my ssh key from working prior to this, so I think it's just down to not being able to write the known_hosts file.</p>
<p>I looked into the -o ssh options to specify an alternative known hosts file, but these aren't valid for the gcloud compute scp command, and can't seem to be passed through with the scp-flags option.</p>
<p>I'm wondering if I need a custom builder for this, or is there an easier solution I'm overlooking?</p>","<p>This stack overflow post was very informative:
<a href=""https://stackoverflow.com/questions/42963774/using-ssh-keys-with-google-container-builder"">Using SSH keys with Google Container Builder</a></p>
<p>As was this documentation item about using ssh to access github from within a build:
<a href=""https://cloud.google.com/build/docs/access-github-from-build"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/access-github-from-build</a></p>
<p>It turned out it was necessary to get the known hosts file into the build.</p>
<p>My solution was cribbed from one of the stack overflow comments. I added this step:</p>
<pre><code>  - name: gcr.io/google.com/cloudsdktool/cloud-sdk
    args:
      - '-c'
      - ./copy-known-hosts.sh
    id: Copy in known hosts
    entrypoint: sh
</code></pre>
<p>The shell script does this:</p>
<pre><code>
exitfn () {
  trap SIGINT
  rm ./google_compute_*
}

trap &quot;exitfn&quot; INT

gcloud secrets versions access 1 --secret=known-hosts &gt; google_compute_known_hosts
mkdir -p /builder/home/.ssh
cp ./google_compute_known_hosts /builder/home/.ssh/google_compute_known_hosts
chmod 400 /builder/home/.ssh

exitfn
</code></pre>"
"How to restrict cloud build permission to all other buckets<p>I created a service account for devs, so they can use the &quot;gcloud run deploy&quot; command. I've manage to find the correct roles: Artifact Registry Administrator and Cloud Run Admin; but there is a third permission about the storage part which I can't find the right for, &quot;Cloud Build Service Account&quot; or &quot;Storage Admin&quot; cause both give full permissions to all buckets and objects.</p>","<p>You can restrict your service account to only the default Cloud Build bucket (<code>&lt;PROJECT_ID&gt;_cloudbuild</code>) by going to the <a href=""https://stackoverflow.com/questions/65529727/"">bucket permissions tab and adding</a> a new principal (your service account) with the <strong>Storage Admin</strong> role.</p>
<p><strong>However,</strong> another thing you must do is to give the service account the <code>storage.buckets.list</code> permission for the whole project, which is why you keep seeing the error.</p>
<p>You can verify this by running <code>gcloud run deploy</code> with <code>--verbosity=debug</code>, to see the internal error message:</p>
<pre class=""lang-bash prettyprint-override""><code>&quot;error&quot;: {
    &quot;code&quot;: 403,
    &quot;message&quot;: &quot;&lt;SERVICE_ACCOUNT&gt; does not have storage.buckets.list access to the Google Cloud project.&quot;,
    ...
</code></pre>
<p>To give minimal permissions, you can <a href=""https://cloud.google.com/iam/docs/creating-custom-roles#creating_a_custom_role"" rel=""nofollow noreferrer"">create a custom role</a> with only the <code>storage.buckets.list</code> permission and assign it to your service account, which is enough to solve the problem as I verified.</p>
<p>Here are the permissions I ended up using for the service account (relevant <a href=""https://cloud.google.com/run/docs/deploying-source-code#permissions_required_to_deploy"" rel=""nofollow noreferrer"">doc</a>):</p>
<ul>
<li>Cloud Build Editor role (project-wide)</li>
<li>Artifact Registry Admin role (project-wide)</li>
<li>Storage Admin role (only for cloud build bucket)</li>
<li>Cloud Run Admin role (project-wide)</li>
<li>Service Account User role (project-wide)</li>
<li>Custom role (project wide, only contains storage.buckets.list)</li>
</ul>
<p>Let me know if this was useful.</p>"
"How to SSH/SCP from Cloud Build thru IAP Tunnel?<p>I need to execute commands on my Compute Engine VM. We need an initial setup for the SQL and the plan is to use cloud build (will only be triggered once) for this; IAP is implemented and Firewall rule is already in place. (Allow TCP 22 from 35.235.240.0/20)</p>
<p>This is my build step:</p>
<pre><code># Setup Cloud SQL
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Setup Cloud SQL Tables'
    entrypoint: 'bash'
    args: 
      - -c
      - |
        echo &quot;Upload File to $_SQL_JUMP_BOX_NAME&quot; &amp;&amp;
        gcloud compute scp --recurse cloud-sql/setup-sql.sh --tunnel-through-iap --zone $_ZONE &quot;$_SQL_JUMP_BOX_NAME:~&quot; &amp;&amp;
        echo &quot;SSH to $_SQL_JUMP_BOX_NAME&quot; &amp;&amp;
        gcloud compute ssh --tunnel-through-iap --zone $_ZONE &quot;$_SQL_JUMP_BOX_NAME&quot; --project &quot;$_TARGET_PROJECT_ID&quot; --command=&quot;chmod +x setup-sql.sh &amp;&amp; ./setup-sql.sh&quot;
</code></pre>
<p>I am receiving this error:</p>
<pre><code>root@compute.3726515935009049919: Permission denied (publickey).
WARNING: 

To increase the performance of the tunnel, consider installing NumPy. For instructions,
please see https://cloud.google.com/iap/docs/using-tcp-forwarding#increasing_the_tcp_upload_bandwidth

root@compute.3726515935009049919: Permission denied (publickey).
ERROR: (gcloud.compute.scp) Could not SSH into the instance.  It is possible that your SSH key has not propagated to the instance yet. Try running this command again.  If you still cannot connect, verify that the firewall and instance are set to accept ssh traffic.
</code></pre>
<p><a href=""https://i.stack.imgur.com/8nAX4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8nAX4.png"" alt=""Error"" /></a></p>
<p><em>This will also be triggered/executed to multiple environments, hence we use cloud build for reusability.</em></p>","<p>Already working!
I stumbled upon this blog -- <a href=""https://hodo.dev/posts/post-14-cloud-build-iap/"" rel=""nofollow noreferrer"">https://hodo.dev/posts/post-14-cloud-build-iap/</a></p>
<p>Made changes on my script, <strong>need to specify user on SCP/SSH command</strong>:</p>
<h2><strong>Working Script/Step:</strong></h2>
<pre><code># Setup Cloud SQL
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Setup Cloud SQL Tables'
    entrypoint: 'bash'
    args: 
      - -c
      - |
        echo &quot;Upload File to $_SQL_JUMP_BOX_NAME&quot; &amp;&amp;
        gcloud compute scp --recurse cloud-sql/setup-sql.sh --tunnel-through-iap --zone $_ZONE cloudbuild@$_SQL_JUMP_BOX_NAME:~ &amp;&amp;
        echo &quot;SSH to $_SQL_JUMP_BOX_NAME&quot; &amp;&amp;
        gcloud compute ssh --tunnel-through-iap --zone $_ZONE cloudbuild@$_SQL_JUMP_BOX_NAME --project &quot;$_TARGET_PROJECT_ID&quot; --command=&quot;chmod +x setup-sql.sh &amp;&amp; ./setup-sql.sh&quot;
</code></pre>
<p><em>Need changes related to the destination VM</em></p>
<p>Before:
gcloud compute ssh --tunnel-through-iap --zone $_ZONE &quot;$_SQL_JUMP_BOX_NAME&quot;</p>
<p>After:
gcloud compute ssh --tunnel-through-iap --zone $_ZONE <strong>cloudbuild@$_SQL_JUMP_BOX_NAME</strong></p>"
"Cloud build defines more than 100 secret values<p>I include secret during Cloud build time but it's choking with error, I believe there Is some hard limit of 100 variables on GCP Secret manager.</p>
<pre><code>ERROR: (gcloud.builds.submit) INVALID_ARGUMENT: invalid build: invalid secrets: build defines more than 100 secret values
</code></pre>
<p><a href=""https://stackoverflow.com/a/60007364"">Code used to submit pipeline from here</a></p>","<p>That is correct.</p>
<p>The limit is 100 args.
<a href=""https://cloud.google.com/build/docs/build-config-file-schema"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/build-config-file-schema</a></p>
<p>Basically each arg counts as step, same with secret values.</p>
<p><a href=""https://i.stack.imgur.com/MBG75.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MBG75.png"" alt=""enter image description here"" /></a></p>"
"CloudBuild with GITLAB at module level<p>I was working on <code>GITHUB</code> and <code>GCP(Cloud Build for deployments)</code> and working good. Below are the steps:</p>
<ul>
<li>Created multiple Cloud Functions and used same GIT HUB repository.</li>
<li>Created separate Cloud Build Trigger for each Cloud Function where separate <code>cloudbuild.yml</code> in each Cloud Function folder in repository.</li>
<li>Trigger gets run when there are changes in respective cloud function scripts.</li>
</ul>
<p>Now i need to integrate <code>Cloud Build</code> with <code>GITLAB</code>.
I have gone through the documentation but found that only <code>webhook</code> is the option and the trigger will be based on whole repository changes. It will require separate repository for each cloud function or Cloud Run. There is no option to select the repository itself.</p>
<p>Can experts guide me on this how I can do this integration because, we are planning to have one repo and multiple service/applications stored in that repository. And we want CI to run on GCP environment itself.</p>","<p>Personally I found GitLab being the worst in comparison to GitHub and BitBucket in terms of integration with the GCP Cloud Build (to run the deployment within GCP).</p>
<p>I don't know ideal solutions, but I probably have 2 ideas. None of them is good from my point of view.</p>
<p>1/ Mirror GitLab repository into GCP repository as described here - <a href=""https://cloud.google.com/architecture/mirroring-gitlab-repositories-to-cloud-source-repositories"" rel=""nofollow noreferrer"">Mirroring GitLab repositories to Cloud Source Repositories</a> One of the biggest drawbacks from my point of view - the integration solution is based on a personal credentials, and there should be a person to make it working -</p>
<blockquote>
<p>Mirroring stops working if the Google Account is closed or loses access rights to the Git repository in Cloud Source Repositories</p>
</blockquote>
<p>When mirroring is done - you probably can work with the GCP based repository in an ordinary way and trigger cloud build jobs as usual. A separate question - how to provide deployment logs to those who initiated the deployment...</p>
<p>2/ Use webhooks. That does not depend on any personal accounts, but not very granular - as you mentioned push on the whole repository level. To overcome that limitation, there might be a very tricky (inline) yaml file - executed by a cloud build trigger. In that yaml file, not only we should fetch the code, but also parse all changes (all commits) in that push to find out which subdirectories (thus separate components - cloud functions) are potentially modified. Then, for each affected (modified) subdirectory we can trigger (asynchronously) some other cloud build job (with a yaml file for it located inside that subdirectory).</p>
<p>An obvious drawback - not clear who and how should get the logs from all those deployments, especially if something went wrong, and the development (and management) of such deployment process might be time/effort consuming and not easy.</p>"
"Use variable to change machine type in Cloud Build<p>In cloud build trigger I created a <code>variable: key _MACHINE_TYPE value: E2_HIGHCPU_8</code></p>
<p>I set this variable in my <code>cloudbuild.yaml</code>:</p>
<pre><code> steps:
  # Build Backend image and push with kaniko
  - name: 'gcr.io/kaniko-project/executor:latest'
    id: '[Backend] Build &amp; Push image'
    args:
      - --dockerfile=backend/Dockerfile
      - --destination=$_BACKEND_IMAGE_REPO:$_APP_VERSION
      - --destination=$_BACKEND_IMAGE_REPO:$_PROJECT
      - --cache=true
      - --build-arg=user=user
      - --build-arg=uid=1000
      - --context=/workspace/backend
      - --target=production

  # Build Frontend image and push with kaniko
  - name: 'gcr.io/kaniko-project/executor:latest'
    id: '[Frontend] Build &amp; Push image'
    args:
      - --dockerfile=frontend/Dockerfile
      - --destination=$_FRONTEND_IMAGE_REPO:$_APP_VERSION
      - --destination=$_FRONTEND_IMAGE_REPO:$_PROJECT
      - --cache=true
      - --build-arg=REACT_APP_VERSION=$_APP_VERSION
      - --build-arg=REACT_APP_WEBSITE_NAME=$_WEBSITE_NAME
      - --build-arg=REACT_APP_BACKEND_BASE_URL=$_BACKEND_URL
      - --build-arg=REACT_APP_GOOGLE_MAP_KEY=$_GOOGLE_MAP_KEY
      - --context=/workspace/frontend
      - --target=production

  # Laravel Migration
  - name: 'gcr.io/google-appengine/exec-wrapper'
    id: 'Laravel Migration'
    args: [
      '-i', $_BACKEND_IMAGE_REPO:$_PROJECT,
      '-e', 'DB_CONNECTION=$_DB_CONNECTION',
      '-e', 'DB_SOCKET=$_DB_SOCKET',
      '-e', 'DB_PORT=$_DB_PORT',
      '-e', 'DB_DATABASE=$_DB_DATABASE',
      '-e', 'DB_USERNAME=$_DB_USERNAME',
      '-e', 'DB_PASSWORD=$_DB_PASSWORD',
      '-s', $_DB_INSTANCE_NAME,
      '--', 'scripts/migration.sh', '--config=$_MIGRATION_CONFIG'
    ]
    waitFor:
      - '[Backend] Build &amp; Push image'
      - '[Frontend] Build &amp; Push image'

 # Deploy Backend image to Cloud Run
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: '[Backend] Cloud Run Deployment'
    entrypoint: gcloud
    args: [
      'run', 'deploy', '$_BACKEND_SERVICE_NAME',
      '--image', '$_BACKEND_IMAGE_REPO:$_PROJECT', 
      '--region', '$_REGION',
      '--update-env-vars', 'APP_VERSION=$TAG_NAME'
    ]
    waitFor:
      - '[Backend] Build &amp; Push image'
      - '[Frontend] Build &amp; Push image'
      - 'Laravel Migration'

  # Deploy Frontend image to Cloud Run
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: '[Frontend] Cloud Run Deployment'
    entrypoint: gcloud
    args: [
      'run', 'deploy', '$_FRONTEND_SERVICE_NAME',
      '--image', '$_FRONTEND_IMAGE_REPO:$_PROJECT',
      '--region', '$_REGION'
    ]
    waitFor:
      - '[Backend] Build &amp; Push image'
      - '[Frontend] Build &amp; Push image'
      - 'Laravel Migration'
      - '[Backend] Cloud Run Deployment'

  # Deploy Cloud Scheduler Crons
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Deploy Cloud Scheduler Crons'
    env:
      - 'PROJECT_ID=$PROJECT_ID'
      - 'PROJECT=$_PROJECT'
      - 'REGION=$_REGION'
      - 'BACKEND_URL=$_BACKEND_URL'
      - 'TASK_MANAGER_TOKEN=$_TASK_MANAGER_TOKEN'
      - 'TASK_PAUSE=$_TASK_PAUSE'
    entrypoint: 'bash'
    args: ['./create-crons.sh']

 options:
  dynamic_substitutions: true
  substitution_option: 'ALLOW_LOOSE'
  machineType: $_MACHINE_TYPE
 timeout: 3600s
</code></pre>
<p>But I have this error when I start my pipeline:</p>
<pre><code>failed unmarshalling build config cloudbuild.yaml: unknown value &quot;\&quot;$_MACHINE_TYPE\&quot;&quot; for enum google.devtools.cloudbuild.v1.BuildOptions.MachineType`
</code></pre>
<p>How can I use a variable to change the machine type with a substitution ?</p>","<p>The <a href=""https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values"" rel=""nofollow noreferrer"">Substituting variable values</a> documentation states that, Use substitutions in your build's steps and images to resolve their values at build time. So, variable substitutions are only available for use within these two steps.  The <a href=""https://cloud.google.com/build/docs/api/reference/rest/v1/projects.builds#Build.BuildOptions"" rel=""nofollow noreferrer"">BuildOptions</a> object would be a different object and therefore it is not qualified for this type of variable substitution.</p>
<p>If you really need to use variable substitution, consider using the <a href=""https://cloud.google.com/sdk/gcloud/reference/builds/submit#--machine-type"" rel=""nofollow noreferrer""><code>gcloud builds submit</code></a> command line instead, then call it with the <code>--machine-type</code> flag.</p>"
"Run next cloud build trigger after two other finished successfully<p>Let's say that currently my CI/CD pipeline runs these two triggers after merging. (<code>0</code> and <code>1</code>)</p>
<p><a href=""https://i.stack.imgur.com/zv09n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zv09n.png"" alt=""enter image description here"" /></a></p>
<p>And in order to run <code>Trigger production deployment</code> I need both to finish successfully. Is there a way to run them (<code>Cypress E2E</code> and <code>Python E2E</code>) separately, and wait for each to &quot;enable&quot; the &quot;1 of 2&quot; and &quot;2 of 2&quot; required conditions to then execute the deployment?</p>","<p>Synchronizing Cloud Builds to trigger a 3rd one can be done but it's complex.</p>
<p>However, it exists an option on <a href=""https://cloud.google.com/build/docs/configuring-builds/configure-build-step-order#examples"" rel=""nofollow noreferrer"">Cloud Build to run steps in parallel</a>. According to the documentation, you can do that</p>
<pre><code>steps:
  - name: ...
    id: cypress
    args: ...
  - name: ...
    id: python
    args: ...
    waitFor: ['-']

  - name: ...
    id: deploy
    args: ...
    waitFor: ['cypress','python']
</code></pre>
<p>If you need more CPU to run all the steps in parallel, you can <a href=""https://cloud.google.com/build/docs/optimize-builds/increase-vcpu-for-builds#increase_vcpu_for_default_pools"" rel=""nofollow noreferrer"">configure the machine type</a></p>"
"Google Cloud App Engine - NestJS App deploys fine and runs instance, but can't access it<p>I am trying to host a NestJS application on Google App Engine. I'm using Cloud Build to deploy the app into App Engine after certain action on my GitHub repo.</p>
<p>Everything goes fine; the app builds and is deployed to App Engine. Even an instance is created for said deployment. However I cannot seem to access the host that's assigned to the service where I'm hosting the application on. I don't understand why as the test NodeJS project for App Engine ran just fine. <a href=""https://i.stack.imgur.com/Q0q6G.png"" rel=""nofollow noreferrer"">This is what I get when trying to visit the endpoint.</a></p>
<p><strong>app.yaml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>runtime: nodejs16
instance_class: F4
service: rest
</code></pre>
<p><strong>cloudbuild.yaml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: node:16.0.0
  entrypoint: npm
  args: [&quot;install&quot;]
- name: node:16.0.0
  entrypoint: npm
  args: [&quot;run&quot;, &quot;build&quot;]
- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  args: [&quot;app&quot;, &quot;deploy&quot;, &quot;-v&quot;, &quot;20220628t175507&quot;]
timeout: &quot;1600s&quot;
</code></pre>
<p>I don't understand what I'm doing wrong. Just for reference the application is simply a REST API app, only utilized to make requests to.</p>","<h1>Solved</h1>
<p>GAE docs do not tell you this (they slightly hint it when talking about <code>app.yaml</code>'s <code>environment_variables</code>, but not explicitly).</p>
<p>Make sure the port your Nest application is pointing to is <code>process.env.PORT</code> as GAE sets the best port available for your application automatically. Since I had a static value assigned to the port it wouldn't run.</p>
<p>Basically changed:</p>
<pre class=""lang-js prettyprint-override""><code>app.run(3000);
</code></pre>
<p>To:</p>
<pre class=""lang-js prettyprint-override""><code>app.run(process.env.PORT);
</code></pre>
<p>Ran latest build and everything works perfectly fine.</p>"
"Google Cloud Build Time Out after 10 mins<p><a href=""https://i.stack.imgur.com/2jAiW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2jAiW.png"" alt="""" /></a></p>
<p><a href=""https://i.stack.imgur.com/IXRKv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IXRKv.png"" alt="""" /></a></p>
<p>I have a Google Cloud Build build that times out after 10 min. The build status is set to &quot;Build failed (timeout)&quot; and I'm okay with it taking longer than 10 minutes.</p>","<p>We can give time out manually as below using YAML :</p>
<p>In cloudbuild.yaml you have to add something like <a href=""https://cloud.google.com/build/docs/build-config-file-schema#timeout_2"" rel=""nofollow noreferrer"">timeout</a>: 1200s</p>
<p>E.g.</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  args: [ 'build', '-t', 'gcr.io/[PRODUCT_ID]/[CONTAINER_IMAGE]', '.' ]
images: 
- 'gcr.io/[PRODUCT_ID]/[CONTAINER_IMAGE]'
timeout: 1200s
</code></pre>
<p><strong>Note</strong> : The maximum value that can be applied to timeout is 24 hours. The timeout must be specified in seconds with up to nine fractional digits, terminated by 's'. Example: &quot;3.5s&quot; .
If timeout is not set, a default timeout of 10 minutes will apply to the build.</p>
<p>See the full definition of a <a href=""https://cloud.google.com/container-builder/docs/api/reference/rest/v1/projects.builds#resource-build"" rel=""nofollow noreferrer"">Build Resource</a>  and <a href=""https://cloud.google.com/build/docs/deploying-builds/deploy-appengine#configuring_the_deployment"" rel=""nofollow noreferrer"">configuring the deployment</a> in the documentation.</p>"
"Google Cloud Platform container list tags permission denied<p>When executing the command:</p>
<pre><code>gcloud container images list-tags gcr.io/x/x
</code></pre>
<p>In the terminal, I get the following error:</p>
<blockquote>
<p>(gcloud.container.images.list-tags) Access denied: gcr.io/x/x</p>
</blockquote>
<p>Which role do I need to give the user so that they are able to execute that command? Thank you.</p>","<p>For read-only purposes (e.g. <code>list-tags</code>), <code>roles/storage.objectViewer</code> should (!?) be sufficient.</p>
<p>Google Container Registry (GCR) is slightly confusing because:</p>
<ol>
<li>the API is <a href=""https://docs.docker.com/registry/spec/api/"" rel=""nofollow noreferrer"">Docker Registry API</a></li>
<li>the backing storage is Google Cloud Storage (GCS).</li>
</ol>
<p>As a result of:</p>
<ol>
<li>there's no Google IAM roles specifically for GCR because there's no Google-specific API</li>
<li>the IAM role that's used is applicable to GCS.</li>
</ol>"
"How to build/tag an existing Docker image in Cloud Builder<p>I currently use Cloud Build to build my app as a Docker image based on a Dockerfile. All this works fine.</p>
<p>However, I would like to build/push an existing image (without a Dockerfile): quay.io/soketi/soketi:1.0-16-debian</p>
<p>How can I instruct Cloud Build to tag/push the above Docker image hosted on quay.io?</p>
<p>Once I push the image, I will create a Cloud Run based on the socket image.</p>
<p>Thank you</p>","<p>You can do it directly by pulling the image, add the new tag and push it to the new registry in one step:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  entrypoint: 'bash'
  args:
  - '-eEuo'
  - 'pipefail'
  - '-c'
  - |-
    docker pull quay.io/soketi/soketi:1.0-16-debian &amp;&amp; \
    docker tag quay.io/soketi/soketi:1.0-16-debian gcr.io/PROJECT_ID/soketi:1.0-16-debian &amp;&amp; \
    docker push gcr.io/PROJECT_ID/soketi:1.0-16-debian
</code></pre>
<p>Of course you can change the registry and only if you want to re-tag the image</p>"
"Google Kubernetes Engine + Github Actions with google build workflow stuck at build step<p>I've been trying to run Google Kubernetes Engine deploy action for my github repo.</p>
<p>I have made a github workflow job run and it is stuck at the build step, not updating. No matter how long I wait it is not changing its current state:</p>
<p><a href=""https://i.stack.imgur.com/PpKaZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PpKaZ.png"" alt=""enter image description here"" /></a></p>
<p>My dockerfile (I got it from the internet):</p>
<pre><code>FROM ubuntu:20.04

# Install.
RUN \
  sed -i 's/# \(.*multiverse$\)/\1/g' /etc/apt/sources.list &amp;&amp; \
  apt-get update &amp;&amp; \
  apt-get -y upgrade &amp;&amp; \
  apt-get install -y build-essential &amp;&amp; \
  apt-get install -y software-properties-common &amp;&amp; \
  apt-get install -y byobu curl git htop man unzip vim wget &amp;&amp; \
  rm -rf /var/lib/apt/lists/*

# Add files.
ADD root/.bashrc /root/.bashrc
ADD root/.gitconfig /root/.gitconfig
ADD root/.scripts /root/.scripts

# Set environment variables.
ENV HOME /root

# Define working directory.
WORKDIR /root

# Define default command.
CMD [&quot;bash&quot;]
</code></pre>
<p>google.yml (The tamplate that github gave me):</p>
<pre class=""lang-yaml prettyprint-override""><code>
name: Build and Deploy to GKE

on:
  push:
    branches:
      - &quot;main&quot;

env:
  PROJECT_ID: ${{ secrets.GKE_PROJECT }}
  GAR_LOCATION: europe-west1 # TODO: update region of the Artifact Registry
  GKE_CLUSTER: cluster-1    # TODO: update to cluster name
  GKE_ZONE: europe-west1   # TODO: update to cluster zone
  DEPLOYMENT_NAME: gke-deployment # TODO: update to deployment name
  REPOSITORY: samples # TODO: update to Artifact Registry docker repository
  IMAGE: static-site

jobs:
  setup-build-publish-deploy:
    name: Setup, Build, Publish, and Deploy
    runs-on: ubuntu-latest
    environment: production

    permissions:
      contents: 'read'
      id-token: 'write'

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    # Configure Workload Identity Federation and generate an access token.
    - id: 'auth'
      name: 'Authenticate to Google Cloud'
      uses: 'google-github-actions/auth@v0'
      with:
        token_format: 'access_token'
        workload_identity_provider: '---'
        service_account: '---'

    - name: Docker configuration
      run: |-
        echo ${{steps.auth.outputs.access_token}} | docker login -u oauth2accesstoken --password-stdin https://$GAR_LOCATION-docker.pkg.dev
    # Get the GKE credentials so we can deploy to the cluster
    - name: Set up GKE credentials
      uses: google-github-actions/get-gke-credentials@v0
      with:
        cluster_name: ${{ env.GKE_CLUSTER }}
        location: ${{ env.GKE_ZONE }}

    # Build the Docker image
    - name: Build
      run: |-
        docker build \
          --tag &quot;$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$GITHUB_SHA&quot; \
          --build-arg GITHUB_SHA=&quot;$GITHUB_SHA&quot; \
          --build-arg GITHUB_REF=&quot;$GITHUB_REF&quot; \
          .
    # Push the Docker image to Google Artifact Registry
    - name: Publish
      run: |-
        docker push &quot;$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$GITHUB_SHA&quot;
    # Set up kustomize
    - name: Set up Kustomize
      run: |-
        curl -sfLo kustomize https://github.com/kubernetes-sigs/kustomize/releases/download/v3.1.0/kustomize_3.1.0_linux_amd64
        chmod u+x ./kustomize
    # Deploy the Docker image to the GKE cluster
    - name: Deploy
      run: |-
        # replacing the image name in the k8s template
        ./kustomize edit set image LOCATION-docker.pkg.dev/PROJECT_ID/REPOSITORY/IMAGE:TAG=$GAR_LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE:$GITHUB_SHA
        ./kustomize build . | kubectl apply -f -
        kubectl rollout status deployment/$DEPLOYMENT_NAME
        kubectl get services -o wide
</code></pre>
<p>I'm trying to deploy a C++ code.</p>","<p>For anyone having the same issue:</p>
<p>The problem is that when you say <code>FROM ubuntu:20.04</code> in your docker file it tries to use a full version of ubuntu and when it's done, just like you are installing ubuntu in your computer, it asks for the basic setup questions like &quot;what is your timezone&quot; or &quot;create a user&quot;. You must either get a different version of ubuntu that is more fit for an image or simply add this line after your &quot;FROM ubuntu:20.04&quot; line:</p>
<pre><code>RUN apt-get update &amp;&amp;  DEBIAN_FRONTEND=noninteractive apt-get install -y tzdata
</code></pre>"
"Make unittest failures cause build failure in Google Cloud build<p>I am setting up CI/CD on a Python project running on Google Cloud Platform. The code deployment works fine, but I also created some unit tests for the separate functions. These unit tests are in a few different files, so I created a new file that takes all these unit tests and add them to a test suite. For example, I have a file called <code>testFunction1.py</code> as follows:</p>
<pre><code>from main import Function1

class TransformTests(unittest.TestCase):

    def test_function(self):
        # Test goes here


if __name__ == '__main__':
    logging.getLogger().setLevel(logging.ERROR)
    unittest.main()

</code></pre>
<p>Then I have a similar file called <code>testFunction2.py</code> for a different function.
Finally I have a file called <code>executeTests.py</code> as follows:</p>
<pre><code>import unittest

testmodules = [
    'testFunction1',
    'testFunction2'
]

suite = unittest.TestSuite()
loader = unittest.TestLoader()

for t in testmodules:
    mod = __import__(t, globals(), locals(), ['suite'])
    suite.addTests(loader.loadTestsFromModule(mod))

unittest.TextTestRunner().run(suite)
</code></pre>
<p>Now I am running this file to execute all the tests at once and this also works great. My problem is to run this file in Google Cloud Build. My cloudbuild.yaml file is structured as follows:</p>
<pre><code>steps:
  - name: &quot;docker.io/library/python:3.7&quot;
    args: [&quot;pip3&quot;, &quot;install&quot;, &quot;-t&quot;, &quot;/workspace/lib&quot;, &quot;-r&quot;, &quot;requirements.txt&quot;]
  - name: &quot;docker.io/library/python:3.7&quot;
    args: [&quot;python3&quot;, &quot;executeTests.py&quot;]
    env: [&quot;PYTHONPATH=/workspace/lib&quot;]
</code></pre>
<p>This works fine, but if a test fails, the cloud build still passes which is of course not what I want.</p>
<p>Does anyone know how I can change this setup so that Cloud build will fail when a test fails? Also this is a simple example, in the real repo I have about 50 test files, so I don't want to add each of them to the cloudbuild.yaml file.</p>","<p>Thanks to <a href=""https://stackoverflow.com/a/65180511/13734234"">guillaume blaquire's answer</a>, I fixed the issue by replacing the last line of code in the <code>executeTests.py</code> file by this:</p>
<pre><code>ret = not unittest.TextTestRunner().run(suite).wasSuccessful()
sys.exit(ret)
</code></pre>"
"Do I need to add substituions values that I'll provide via CLI to the cloudbuild.yaml file?<p>In my <code>cloudbuild.yaml</code> file, I'm relying on a value that will be passed through the <code>builds submit --substitutions=_SERVER_ENV=TEST</code> command.</p>
<p>This is how I'm referencing it:</p>
<p><strong>cloudbuild.yaml</strong></p>
<pre><code>steps:
  - name: &quot;gcr.io/cloud-builders/docker&quot;
    args: [
      &quot;build&quot;,
      &quot;--build-arg SERVER_ENV=$_SERVER_ENV&quot;,       // &lt;----- IT WILL BE PASSED TO DOCKER --build-arg
      &quot;.&quot;
    ]
</code></pre>
<p>My question is: do I need to add it to my <code>cloudbuild.yaml</code> file in the <code>substitutions</code> section?</p>
<p>Example:</p>
<p><strong>cloudbuild.yaml</strong></p>
<pre><code>substitutions:               // DO I NEED TO ADD THIS SECTION?
  - &quot;_SERVER_ENV=TEST&quot;       // GIVEN THE FACT THAT I'LL BE PROVIDING IT THROUGH THE CLI
</code></pre>
<p>Or can I leave it out and be sure that the builder will look for it the CLI command like:</p>
<p><code>builds submit --substitutions=_SERVER_ENV=TEST</code></p>
<hr />
<p><strong>REFERENCES</strong></p>
<p><a href=""https://cloud.google.com/sdk/gcloud/reference/builds/submit"" rel=""nofollow noreferrer"">DOC: gcloud builds submit</a></p>
<p><a href=""https://cloud.google.com/cloud-build/docs/build-config#substitutions"" rel=""nofollow noreferrer"">DOC: build config substitutions</a></p>
<p><a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values"" rel=""nofollow noreferrer"">DOC: build config - Substituting variable values</a></p>","<p>You don't need the substitutions part in the <code>cloudbuild.yaml</code> part. However, the value will be empty and can break your Build, or worse passes with an unstable status</p>
<p>As you prefer</p>
<ul>
<li>Either a build that passes with a default not critical value</li>
<li>Or a build that passes with an empty value with unknow/untested behavior.</li>
</ul>"
"Google Cloud Build gke-deploy could not parse reference ""eu.gcr.io/personal-134286/portfolio:6ccad35""<p>I have setup a cloud build pipeline <a href=""https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-gke#building_and_deploying_a_new_container_image"" rel=""nofollow noreferrer"">using this guide</a> but on the last step where it should apply the k8s configuration it gives a non descriptive error:</p>
<pre><code>Already have image (with digest): gcr.io/cloud-builders/gke-deploy
Error: could not parse reference: &quot;eu.gcr.io/personal-134286/portfolio:6ccad35&quot;
</code></pre>
<p>last step in <code>cloudbuild.yaml</code></p>
<pre class=""lang-yaml prettyprint-override""><code>- name: &quot;gcr.io/cloud-builders/gke-deploy&quot;
  args:
  - run
  - --filename=config/deployment.yaml
  - --image=&quot;eu.gcr.io/${_CLOUDSDK_CONTAINER_PROJECT_ID}/${_IMAGE_NAME}:$SHORT_SHA&quot;
  - --location=${_CLOUDSDK_COMPUTE_ZONE}
  - --cluster=${_CLOUDSDK_CONTAINER_CLUSTER}
</code></pre>
<p>spec of <code>config/deployment.yaml</code></p>
<pre class=""lang-yaml prettyprint-override""><code>spec:
  containers:
  - name: portfolio
    image: &quot;eu.gcr.io/personal-134286/portfolio&quot;
    resources:
      limits:
        memory: &quot;52Mi&quot;
        cpu: &quot;10m&quot;
    ports:
    - containerPort: 80
</code></pre>
<p>I verified that the image exists and if I manually place the image into deployment.yaml and run <code>kubectl apply</code> on it it works without a hitch.</p>","<p>The issue lies in the host of the container registry. After changing from <code>eu.gcr.io</code> to <code>gcr.io</code> the builds pass.</p>
<p>I opened an <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/issues/739"" rel=""nofollow noreferrer"">issue in the cloud build repository</a></p>"
"How to deploy a function on GCP with Cloud Build?<p>I have created a Cloud Function called <code>pupetter-e2e</code> with a trigger on changes to a bucket storage, called homepage. I want to deploy updates to the function with the following <code>cloudbuild.yaml</code>:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions
  - deploy
  - pupetter-e2e
  - --source=.
  - --trigger-bucket homepage
</code></pre>
<p>(trigger described: <a href=""https://cloud.google.com/functions/docs/deploying/filesystem"" rel=""nofollow noreferrer"">https://cloud.google.com/functions/docs/deploying/filesystem</a>)</p>
<p>or alternatively:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions
  - deploy
  - pupetter-e2e
  - --source=.
  - --trigger-resource hjemmeside  
  - --trigger-event google.storage.object.finalize
</code></pre>
<p>(as described as <a href=""https://cloud.google.com/functions/docs/calling/storage"" rel=""nofollow noreferrer"">https://cloud.google.com/functions/docs/calling/storage</a>)
sadly, I get</p>
<blockquote>
<p>ERROR: (gcloud.functions.deploy) unrecognized arguments: --trigger-bucket hjemmeside (did you mean '--trigger-bucket'?) or
--trigger-resource hjemmeside (did you mean --trigger-resource?)</p>
</blockquote>
<p>I have attempted to use <code>--trigger-bucket</code>, but can not get it to work properly. Could someone please help me by correcting the mistake in my <code>cloudbuild.yaml</code>?</p>","<p>You have 2 solution to solve this (and even more, but 2 is already good). Firstly, Space aren't accepted in the args list, so:</p>
<ul>
<li>Replace the space by an equal.</li>
</ul>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions
  - deploy
  - pupetter-e2e
  - --source=.
  - --trigger-bucket=homepage
</code></pre>
<ul>
<li>Put the param value in a new ARGS value (new line)</li>
</ul>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - functions
  - deploy
  - pupetter-e2e
  - --source=.
  - --trigger-bucket 
  - homepage
</code></pre>"
"Not able to set environment variable in cloudbuild.yaml file<p>I am trying to set env variable in the cloudbuild.yaml file but it's not getting set. Am I missing something ? Below is the yaml file:</p>
<p>cloudbuild.yaml</p>
<pre><code>steps:
  # Install npm
  - name: &quot;node:10.16.3&quot;
    id: installing_npm
    args: [&quot;npm&quot;, &quot;install&quot;]
    dir: &quot;/workspace/API/ground_truth_trigger&quot;
  # Test Cloud Function
  - name: 'node:10.16.3'
    id: run_test_coverage
    dir: '/workspace/API/ground_truth_trigger'
    entrypoint: bash
    env: ['BUCKET_NAME = dummycblbucket', 'AUDIT_BUCKET_NAME = dummyAuditbucket']
    args:
    - '-c'
    - |
      if [[ $BRANCH_NAME =~ ^ground_truth_trigger-[0-9]+-api$ ]]
      then
      npm run test
      fi
    # env:
    # - 'BUCKET_NAME = dummycblbucket'
    # - 'AUDIT_BUCKET_NAME = dummyAuditbucket'
</code></pre>
<p>Below are the logs:</p>
<pre><code>Step #1 - &quot;run_test_coverage&quot;: Already have image: node:10.16.3
Step #1 - &quot;run_test_coverage&quot;: 
Step #1 - &quot;run_test_coverage&quot;: &gt; ground_truth_trigger@1.0.0 test /workspace/API/ground_truth_trigger
Step #1 - &quot;run_test_coverage&quot;: &gt; nyc --reporter=lcov --reporter=text mocha test/unit/*
Step #1 - &quot;run_test_coverage&quot;: 
Step #1 - &quot;run_test_coverage&quot;: envs  { npm_config_cache_lock_stale: '60000',
Step #1 - &quot;run_test_coverage&quot;:   npm_config_ham_it_up: '',
Step #1 - &quot;run_test_coverage&quot;:   npm_config_legacy_bundling: '',
Step #1 - &quot;run_test_coverage&quot;:   npm_config_sign_git_tag: '',
Step #1 - &quot;run_test_coverage&quot;:   npm_config_user_agent: 'npm/6.9.0 node/v10.16.3 linux x64',
Step #1 - &quot;run_test_coverage&quot;:    '{&quot;_&quot;:[&quot;mocha&quot;],&quot;reporter&quot;:[&quot;lcov&quot;,&quot;text&quot;],&quot;r&quot;:[&quot;lcov&quot;,&quot;text&quot;],&quot;cwd&quot;:&quot;/workspace/API/ground_truth_trigger&quot;,&quot;temp-dir&quot;:&quot;./.nyc_output&quot;,&quot;t&quot;:&quot;./.nyc_output&quot;,&quot;tempDir&quot;:&quot;./.nyc_output&quot;,&quot;exclude&quot;:[&quot;coverage/**&quot;,&quot;packages/*/test{,s}/**&quot;,&quot;**/*.d.ts&quot;,&quot;test{,s}/**&quot;,&quot;test{,-*}.{js,cjs,mjs,ts}&quot;,&quot;**/*{.,-}test.{js,cjs,mjs,ts}&quot;,&quot;**/__tests__/**&quot;,&quot;**/{ava,nyc}.config.{js,cjs,mjs}&quot;,&quot;**/jest.config.{js,cjs,mjs,ts}&quot;,&quot;**/{karma,rollup,webpack}.config.js&quot;,&quot;**/{babel.config,.eslintrc,.mocharc}.{js,cjs}&quot;],&quot;x&quot;:[&quot;coverage/**&quot;,&quot;packages/*/test{,s}/**&quot;,&quot;**/*.d.ts&quot;,&quot;test{,s}/**&quot;,&quot;test{,-*}.{js,cjs,mjs,ts}&quot;,&quot;**/*{.,-}test.{js,cjs,mjs,ts}&quot;,&quot;**/__tests__/**&quot;,&quot;**/{ava,nyc}.config.{js,cjs,mjs}&quot;,&quot;**/jest.config.{js,cjs,mjs,ts}&quot;,&quot;**/{karma,rollup,webpack}.config.js&quot;,&quot;**/{babel.config,.eslintrc,.mocharc}.{js,cjs}&quot;],&quot;exclude-node-modules&quot;:true,&quot;excludeNodeModules&quot;:true,&quot;include&quot;:[],&quot;n&quot;:[],&quot;extension&quot;:[&quot;.js&quot;,&quot;.cjs&quot;,&quot;.mjs&quot;,&quot;.ts&quot;,&quot;.tsx&quot;,&quot;.jsx&quot;],&quot;e&quot;:[&quot;.js&quot;,&quot;.cjs&quot;,&quot;.mjs&quot;,&quot;.ts&quot;,&quot;.tsx&quot;,&quot;.jsx&quot;],&quot;ignore-class-methods&quot;:[],&quot;ignoreClassMethods&quot;:[],&quot;auto-wrap&quot;:true,&quot;autoWrap&quot;:true,&quot;es-modules&quot;:true,&quot;esModules&quot;:true,&quot;parser-plugins&quot;:[&quot;asyncGenerators&quot;,&quot;bigInt&quot;,&quot;classProperties&quot;,&quot;classPrivateProperties&quot;,&quot;dynamicImport&quot;,&quot;importMeta&quot;,&quot;objectRestSpread&quot;,&quot;optionalCatchBinding&quot;],&quot;parserPlugins&quot;:[&quot;asyncGenerators&quot;,&quot;bigInt&quot;,&quot;classProperties&quot;,&quot;classPrivateProperties&quot;,&quot;dynamicImport&quot;,&quot;importMeta&quot;,&quot;objectRestSpread&quot;,&quot;optionalCatchBinding&quot;],&quot;compact&quot;:true,&quot;preserve-comments&quot;:true,&quot;preserveComments&quot;:true,&quot;produce-source-map&quot;:true,&quot;produceSourceMap&quot;:true,&quot;source-map&quot;:true,&quot;sourceMap&quot;:true,&quot;require&quot;:[],&quot;i&quot;:[],&quot;instrument&quot;:true,&quot;exclude-after-remap&quot;:true,&quot;excludeAfterRemap&quot;:true,&quot;branches&quot;:0,&quot;functions&quot;:0,&quot;lines&quot;:90,&quot;statements&quot;:0,&quot;per-file&quot;:false,&quot;perFile&quot;:false,&quot;check-coverage&quot;:false,&quot;checkCoverage&quot;:false,&quot;report-dir&quot;:&quot;coverage&quot;,&quot;reportDir&quot;:&quot;coverage&quot;,&quot;show-process-tree&quot;:false,&quot;showProcessTree&quot;:false,&quot;skip-empty&quot;:false,&quot;skipEmpty&quot;:false,&quot;skip-full&quot;:false,&quot;skipFull&quot;:false,&quot;silent&quot;:false,&quot;s&quot;:false,&quot;all&quot;:false,&quot;a&quot;:false,&quot;eager&quot;:false,&quot;cache&quot;:true,&quot;c&quot;:true,&quot;babel-cache&quot;:false,&quot;babelCache&quot;:false,&quot;use-spawn-wrap&quot;:false,&quot;useSpawnWrap&quot;:false,&quot;hook-require&quot;:true,&quot;hookRequire&quot;:true,&quot;hook-run-in-context&quot;:false,&quot;hookRunInContext&quot;:false,&quot;hook-run-in-this-context&quot;:false,&quot;hookRunInThisContext&quot;:false,&quot;clean&quot;:true,&quot;in-place&quot;:false,&quot;inPlace&quot;:false,&quot;exit-on-error&quot;:false,&quot;exitOnError&quot;:false,&quot;delete&quot;:false,&quot;complete-copy&quot;:false,&quot;completeCopy&quot;:false,&quot;$0&quot;:&quot;node_modules/.bin/nyc&quot;,&quot;instrumenter&quot;:&quot;./lib/instrumenters/istanbul&quot;}',
Step #1 - &quot;run_test_coverage&quot;:   NYC_CWD: '/workspace/API/ground_truth_trigger',
Step #1 - &quot;run_test_coverage&quot;:   NODE_OPTIONS:
Step #1 - &quot;run_test_coverage&quot;:    ' --require /workspace/API/ground_truth_trigger/node_modules/node-preload/preload-path/node-preload.js',
Step #1 - &quot;run_test_coverage&quot;:   NODE_PRELOAD_904597faf3dd793b123e0cc47c7e6f55e1b18fb4:
Step #1 - &quot;run_test_coverage&quot;:    '/workspace/API/ground_truth_trigger/node_modules/nyc/lib/register-env.js:/workspace/API/ground_truth_trigger/node_modules/nyc/lib/wrap.js',
Step #1 - &quot;run_test_coverage&quot;:   NYC_PROCESS_ID: '2403b1ad-d5b2-4715-b9de-abbb54f424cf' }
Step #1 - &quot;run_test_coverage&quot;: 
Step #1 - &quot;run_test_coverage&quot;: Error: A bucket name is needed to use Cloud Storage.
Step #1 - &quot;run_test_coverage&quot;:     at Storage.bucket (/workspace/API/ground_truth_trigger/node_modules/@google-cloud/storage/build/src/storage.js:151:19)
Step #1 - &quot;run_test_coverage&quot;:     at /workspace/API/ground_truth_trigger/src/index.js:4:48
</code></pre>
<p>Could you please help!</p>","<p>You need to remove the space before and after the =</p>
<pre><code>env: ['BUCKET_NAME=dummycblbucket', 'AUDIT_BUCKET_NAME=dummyAuditbucket']
</code></pre>
<p>You can check the value in Cloud Build by performing</p>
<ul>
<li>An echo of the env var <code>echo $$BUCKET_NAME</code>. The double $ is required to indicate to Cloud Build to not replace with substituons variables.</li>
<li>Use the <code>printenv</code> command.</li>
</ul>"
"How can I update an Image in Google Artifact Registry?<p>When viewing Images in Google Cloud Platform's Artifact Registry, there is an &quot;Updated&quot; time column.  However, whenever I build the same image and push it again, it creates a new image.<br />
<a href=""https://i.stack.imgur.com/KzNn7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KzNn7.png"" alt=""enter image description here"" /></a></p>
<p>As part of a Cloud Build process, I am pulling this Ruby-based image, updating gems, then pushing it back to the Artifact Registry for use in later build steps (DB migration, unit tests).   My hope is that upon updating the Ruby gems, nothing would happen in most cases, resulting in an identical Docker Image.  In such a case, I'd expect no new layers to be pushed.  However, every time I build, there is always a new layer pushed, and therefore a new Artifact.</p>
<p>Thus, the problem may be with how Cloud Build's <code>gcr.io/cloud-builders/gsutil</code> works rather than Artifact Registry itself.  Here're my relevant build steps in case it matters:</p>
<pre><code>  - id: update_gems
    name: 'gcr.io/cloud-builders/docker'
    args: [ 'build', '-t', 'us-central1-docker.pkg.dev/$PROJECT_ID/{my repo}/{my image}:deploy',
            '-f', 'docker/bundled.Dockerfile', '.' ]
  - id: update_image
    name: 'gcr.io/cloud-builders/docker'
    args: [ 'push',  'us-central1-docker.pkg.dev/$PROJECT_ID/{my repo}/{my image}:deploy' ]
</code></pre>
<p>The first step refers to &quot;bundled.Dockerfile&quot; which has these contents:</p>
<pre><code>FROM us-central1-docker.pkg.dev/{same project as above}/{my repo}/{my image}:deploy
WORKDIR /workspace
RUN bundle update
RUN bundle install
</code></pre>
<p>Is there a way to accomplish what I'm currently doing (ie update a Deploy-time container used to run <code>rspec</code> tests and run <code>rake db:migrate</code> without making new images every time we build?  I assume those images are taking up space and I'm <a href=""https://cloud.google.com/artifact-registry/pricing"" rel=""nofollow noreferrer"">getting billed for it</a>.  I assume there's a way to &quot;Update&quot; an existing Image in the Artifact Registry since there is an &quot;Updated&quot; column.</p>","<p>You are not looking at container &quot;images&quot;. You are looking at &quot;layers&quot; of an image. The combination of layers results in a container image. These can also be artifacts for Cloud Build, etc.</p>
<p>You cannot directly modify a layer in Artifact Registry. Any changes you make to the image creation will result in one or more layers changing while results in one or more new layers being created. Creating an image does usually does not result in all layers changing. Your new image is probably the result of old and new layers. Layers are cached in Artifact Registry for future images/builds.</p>
<p>More than one container image can use the same layers. If Google allowed you to modify individual layers, you would break/corrupt the resulting containers.</p>"
"Why aren't precompiled assets served on Google App Engine Standard?<p>I have a Ruby on Rails GAE app running in standard environment.  When I deploy using a Cloudbuild which watches my github repo, I can see it precompiling assets in the build logs and storing them in <code>/workspace/public/assets</code> just as expected.  This only happens if I don't have precompiled assets in my repo too, otherwise the build logs say:</p>
<pre><code>Step #3 - &quot;detector&quot;: ======== Output: google.ruby.rails@0.9.0 ========
Step #3 - &quot;detector&quot;: Rails assets do not need precompilation.
Step #3 - &quot;detector&quot;: ======== Results ========
</code></pre>
<p>My cloudbuild step for deployment is</p>
<pre><code>  - id: deploy_to_GAE
    name: gcr.io/cloud-builders/gcloud
    args: ['app', 'deploy', '--project', '${PROJECT_ID}', '--version', '${BRANCH_NAME}', '${_APP_CONFIG}']
</code></pre>
<p>I also have this handler in my app.yaml</p>
<pre><code>handlers:
  - url: /(.*\.(gif|png|jpg|ico|html|txt|webmanifest))$
    static_files: public/\1
</code></pre>
<p>However, after deployment, none of the static assets can be loaded - they all get a 404 response and and the server logs say something like:
<code>Static file referenced by handler not found: public/assets/star_custom3-bf067f238ca10c6a873fd0bdd42e55a22f65a9842f010d10e55e2e4acb12ae5b.png</code></p>
<p>However,  if I precompile locally, and push the precompiled static files to my git repo, then Cloud Build will still deploy successfully (and not precompile assets again per above), and all the assets load properly in my browser.</p>
<p>In both cases, if I &quot;Debug&quot; the version deployed in GAE's web console, I can see what files were actually deployed and see them all right where they belong within /public/assets.  Yet only in the latter case can a web browser access them.</p>
<p>I've also tried adding build steps in my cloudbuild.yaml to precompile assets using a custom Cloud Builder container.  It creates assets in /workspace as expected, but for some reason the <code>depoly_to_GAE</code> step above re-precompiles them.</p>
<p>Lastly, if I delete all my precompiled assets locally, then manually deploy to GAE with
<code> gcloud app deploy app-ticket.yaml --project={my project}</code>, the Cloud Build logs show it precompiling, and just like above, the assets are not reachable by the web browser (404 errors)</p>
<p>When the <code>deploy_to_GAE</code> build step precompiles assets, it also sets <code>RAILS_ENV=production</code>, however, the actual environment for the rest of the deployment and the env var set in the app.yaml file is <code>RAILS_ENV=staging</code>.   I don't think this should matter though.</p>
<p>Why does the built-in asset precompiler for GAE result in usable static assets?</p>
<p><strong>UPDATE:</strong>
Even when deploying locally with <code>gcloud app deploy</code> and no <code>/public/assets</code> folder, I can see that cloudbuild will compiled assets, and <em>appear</em> to put them properly in <code>/public/assets</code>:</p>
<p>However, when using <a href=""https://console.cloud.google.com/debug"" rel=""nofollow noreferrer"">the GCP Debugger</a>, I notice that the path uses a literal &quot;/&quot; in the folder name instead of making a subfolder.  This seems to be a flaw in the GAE builder:
<a href=""https://i.stack.imgur.com/Whltg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Whltg.png"" alt=""enter image description here"" /></a></p>
<p>I would expect &quot;assets&quot; folder to be properly nested under &quot;public&quot; rather than a mutant folder named &quot;public/assets&quot;.</p>","<p>It turns out that GAE does some undocumented magic with precompiled assets by not only auto-precompiling them at build-time, but also by creating routes to those assets without needing to add handlers in app.yaml.  Once I removed the handler show in the main question (and copied below), then everything worked as expected:  If I had no <code>/public/assets</code> folder, then <code>gcloud app deploy</code> would precompile assets automatically, and so long as I didn't try to make a handler, then it would serve those assets properly.</p>"
"Error on cloudbuild.yaml : (gcloud.builds.submit) interpreting cloudbuild.yaml as build config: 'list' object has no attribute 'items'<p>This is my <code>cloudbuild.yaml</code> file:</p>
<pre><code>steps:
  # BUILD IMAGE
  - name: &quot;gcr.io/cloud-builders/docker&quot;
    args:
      - &quot;build&quot;
      - &quot;--build-arg&quot;
      - &quot;PROJECT_ID=$PROJECT_ID&quot;
      - &quot;--build-arg&quot; 
      - &quot;SERVER_ENV=$_SERVER_ENV&quot;
      - &quot;--tag&quot;
      - &quot;gcr.io/$PROJECT_ID/my-image:$TAG_NAME&quot;
      - &quot;.&quot;
    env:
      - &quot;PROJECT_ID=$PROJECT_ID&quot;
    timeout: 180s
  # PUSH IMAGE TO REGISTRY
  - name: &quot;gcr.io/cloud-builders/docker&quot;
    args:
      - &quot;push&quot;
      - &quot;gcr.io/$PROJECT_ID/my-image:$TAG_NAME&quot;
    timeout: 180s
  # DEPLOY CONTAINER WITH GCLOUD
  - name: &quot;gcr.io/google.com/cloudsdktool/cloud-sdk&quot;
    entrypoint: gcloud
    args:
      - &quot;run&quot;
      - &quot;deploy&quot;
      - &quot;my-service&quot;
      - &quot;--image=gcr.io/$PROJECT_ID/my-image:$TAG_NAME&quot;
      - &quot;--platform=managed&quot;
      - &quot;--region=us-central1&quot;
      - &quot;--min-instances=1&quot;
      - &quot;--max-instances=3&quot;
      - &quot;--port=8080&quot;
    timeout: 180s

images: 
  - &quot;gcr.io/$PROJECT_ID/my-image:$TAG_NAME&quot;
substitutions: 
  - &quot;_SERVER_ENV=TEST&quot;
</code></pre>
<p>Is there anything wrong with this file?</p>
<p>Here is the error I'm getting when I'm running the following command:</p>
<pre><code>gcloud builds submit ./cloudRun                   \
--config=./cloudRun/cloudbuild.yaml               \
--substitutions=_SERVER_ENV=TEST,TAG_NAME=MY_TAG  \
--project=MY_PROJECT_ID
</code></pre>
<blockquote>
<p>ERROR: (gcloud.builds.submit) interpreting ./cloudRun/cloudbuild.yaml as <strong>build config: 'list' object has no attribute 'items'</strong></p>
</blockquote>","<p>Just found out what was wrong:</p>
<p><code>substitutions</code> is not an <strong>ARRAY</strong>, but an <strong>OBJECT</strong>:</p>
<p><strong>So this is NOT correct:</strong></p>
<pre><code>substitutions: 
  - &quot;_SERVER_ENV=TEST&quot;
</code></pre>
<p>But this is correct:</p>
<pre><code>substitutions: 
 _SERVER_ENV: &quot;TEST&quot;
</code></pre>"
"gcloud beta command in build step in cloudbuild.yaml. Should I use entrypoint or args?<p>I'm trying to build and deploy a Docker image to Cloud Run. And I'd like to set <code>min-instances=1</code> so I can avoid cold starts.</p>
<p>I'm building and deploying it using Cloud Build through the <code>gcloud</code> CLI.</p>
<p>So this is was my 1st attempt from the <code>gcloud</code> CLI:</p>
<pre><code>gcloud builds submit . --config=./cloudbuild.yaml
</code></pre>
<p>And here are the build steps that are described in my <code>cloudbuild.yaml</code>:</p>
<pre><code>steps:
  # STEP_1: DOCKER BUILDS IMAGE
  # STEP_2: DOCKER PUSHES IMAGE TO CLOUD REGISTRY

  # STEP_3: GCLOUD SHOULD DEPLOY TO CLOUD RUN (DESCRIBED BELOW)

  - name: &quot;gcr.io/google.com/cloudsdktool/cloud-sdk&quot;
    entrypoint: gcloud
    args:
      - &quot;run&quot;
      - &quot;deploy&quot;
      - &quot;my-service&quot;
      - &quot;--image=gcr.io/$PROJECT_ID/my-image&quot;
      - &quot;--platform=managed&quot;
      - &quot;--region=us-central1&quot;
      - &quot;--min-instances=1&quot;
</code></pre>
<p>You see that the build <code>STEP_3</code> runs: <code>gcloud run deploy my-service ... min-instances=1</code></p>
<p>And I'm getting the following error:</p>
<pre><code>The `--min-instances` flag is not supported in the GA release track on the 
fully managed version of Cloud Run. Use `gcloud beta` to set `--min-instances` on Cloud Run (fully managed).
</code></pre>
<p>So I guess I'll have to use <code>gcloud beta</code> commands. But I have some questions in that case:</p>
<p>Do I also need to add the <code>beta</code> command to my <code>gcloud builds submit .</code> command?</p>
<p>And how should I set it in <code>cloudbuilt.yaml</code>? Do I add it to the <code>entrypoint</code> or as an argument in <code>args</code>?</p>
<p><strong>OPTION #1</strong></p>
<pre><code>  - name: &quot;gcr.io/google.com/cloudsdktool/cloud-sdk&quot;
    entrypoint: &quot;gcloud beta&quot;
    args:
      - &quot;run&quot;
   // ETC
</code></pre>
<p><strong>OPTION #2</strong></p>
<pre><code>  - name: &quot;gcr.io/google.com/cloudsdktool/cloud-sdk&quot;
    entrypoint: gcloud
    args:
      - &quot;beta&quot;
      - &quot;run&quot;
   // ETC
</code></pre>","<p>There is nothing like a hidden reason for either.</p>
<p>Use under args. All the elements are concatenated into a string.</p>"
"How to submit a GCP AI Platform training job frominside a GCP Cloud Build pipeline?<p>I have a pretty standard <code>CI</code> pipeline using <code>Cloud Build</code> for my Machine Learning training model based on container:</p>
<ul>
<li>check python error use flake8</li>
<li>check syntax and style issue using pylint, pydocstyle ...</li>
<li>build a base container (CPU/GPU)</li>
<li>build a specialized ML container for my model</li>
<li>check the vulnerability of the packages installed</li>
<li>run tests units</li>
</ul>
<p>Now in Machine Learning it is impossible to validate a model without testing it with real data. Normally we add 2 extra checks:</p>
<ul>
<li>Fix all random seed and run on a test data to see if we find the exact same results</li>
<li>Train the model on a batch and see if we can over fit and have the loss going to zero</li>
</ul>
<p>This allow to catch issues inside the code of model. In my setup, I have my <code>Cloud Build</code> in a build <code>GCP</code> project and the data in another <code>GCP</code> project.</p>
<p>Q1: did somebody managed to use <code>AI Platform training</code> service in <code>Cloud Build</code> to train on data sitting in another <code>GCP</code> project ?</p>
<p>Q2: how to tell Cloud Build to wait until the <code>AI Platform training</code> job finished and check what is the status (successful/failed) ? It seems that the only option when looking at the documentation <a href=""https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training"" rel=""nofollow noreferrer"">link</a> it to use <code>--stream-logs</code>but it seems non optimal (using such option, I saw some huge delay)</p>","<p>When you submit an AI platform training job, you can specify a <a href=""https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training#--service-account"" rel=""nofollow noreferrer"">service account email to use</a>.</p>
<p>Be sure that the service account has enough authorization in the other project to use data from there.</p>
<p>For you second question, you have 2 solutions</p>
<ul>
<li>Use <code>--stream-logs</code> as you mentioned. If you don't want the logs in your Cloud Build, you can redirect the stdout and/or the stderr to <code>/dev/null</code></li>
</ul>
<pre><code>- name: name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args:
    - -c
    - |
         gcloud ai-platform jobs submit training &lt;your params&gt; --stream-logs &gt;/dev/null 2&gt;/dev/null

</code></pre>
<p>Or you can create an infinite loop that check the status</p>
<pre><code>- name: name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args:
    - -c
    - |
        JOB_NAME=&lt;UNIQUE Job NAME&gt;
        gcloud ai-platform jobs submit training $${JOB_NAME} &lt;your params&gt; 
        # test the job status every 60 seconds
        while [ -z &quot;$$(gcloud ai-platform jobs describe $${JOB_NAME} | grep SUCCEEDED)&quot; ]; do sleep 60; done
</code></pre>
<p><em>Here my test is simple, but you can customize the status tests as you want to match your requirement</em></p>
<p>Don't forget to set the timeout as expected.</p>"
"Google Cloud Build loses ability to authenticate with my private GitHub repository<p>I'm having problems running my cloud build triggers reliably because of what appears to be a credentials timeout.</p>
<p>I have set up my Google Cloud devops project within Google Cloud Build and authenticated the triggers with my private GitHub repository. My private GitHub repository has the Cloud Build GitHub App integration.</p>
<p>My Cloud Build triggers work for a while, running fine whether run automatically or manually.</p>
<p>Then after a few hours I started to get errors in my trigger execution:</p>
<pre><code>...
FETCHSOURCE
Initialized empty Git repository in /workspace/.git/
remote: Invalid username or password.
fatal: Authentication failed for 'https://github.com...
</code></pre>
<p>To try to resolve this I have used the Cloud Build triggers page to disconnect and then re-connect and re-authenticate with my private GitHub repository. The repository management page shows a successful re-authentication, but I still get a &quot;Invalid username or password&quot; error when I run my triggers.</p>
<p>Is there something I'm missing about the way private repository authentication works? Is there another setting I'm missing that auto-refreshes Cloud Build authentication?</p>","<p>This issue is the same to this public tracker issue:</p>
<p><a href=""https://issuetracker.google.com/172325510"" rel=""nofollow noreferrer"">Unable to fetch repository from Github</a></p>
<p>Google Cloud Engineering Team is working on solving the issue. You can star the public issue tracker feature requests and add comment in the thread that you are also affected to ensure that you will receive the updates about it.</p>"
"npm run build is not cached when running docker build with kaniko cache<p>I'm trying to speed up my Google Cloud Build for a React application (<a href=""https://github.com/thdk/team-timesheets"" rel=""nofollow noreferrer"">github repo</a>). Therefor I started <a href=""https://cloud.google.com/cloud-build/docs/kaniko-cache"" rel=""nofollow noreferrer"">using Kaniko Cache</a> as suggested in the official Cloud Build docs.</p>
<p>It seems the <code>npm install</code> part of my build process is now indeed cached. However, I would have expected that <code>npm run build</code> would also be cached when source files haven't changed.</p>
<p>My Dockerfile:</p>
<pre><code># Base image has ubuntu, curl, git, openjdk, node &amp; firebase-tools installed
FROM gcr.io/team-timesheets/builder as BUILDER   

## Install dependencies for functions first
WORKDIR /functions
COPY functions/package*.json ./

RUN npm ci

## Install app dependencies next
WORKDIR /
COPY package*.json ./

RUN npm ci

# Copy all app source files
COPY . .

# THIS SEEMS TO BE NEVER CACHED, EVEN WHEN SOURCE FILES HAVENT CHANGED
RUN npm run build:refs \
    &amp;&amp; npm run build:production

ARG VCS_COMMIT_ID
ARG VCS_BRANCH_NAME
ARG VCS_PULL_REQUEST
ARG CI_BUILD_ID
ARG CODECOV_TOKEN

ENV VCS_COMMIT_ID=$VCS_COMMIT_ID
ENV VCS_BRANCH_NAME=$VCS_BRANCH_NAME
ENV VCS_PULL_REQUEST=$VCS_PULL_REQUEST
ENV CI_BUILD_ID=$CI_BUILD_ID
ENV CODECOV_TOKEN=$CODECOV_TOKEN

RUN npm run test:cloudbuild \
    &amp;&amp; if [ &quot;$CODECOV_TOKEN&quot; != &quot;&quot; ]; \
        then curl -s https://codecov.io/bash | bash -s - -X gcov -X coveragepy -X fix -s coverage; \
    fi

WORKDIR /functions

RUN npm run build

WORKDIR /

ARG FIREBASE_PROJECT_ID
ARG FIREBASE_TOKEN

RUN if [ &quot;$FIREBASE_TOKEN&quot; != &quot;&quot; ]; \
       then firebase deploy --project $FIREBASE_PROJECT_ID --token $FIREBASE_TOKEN; \
    fi
</code></pre>
<p>Build output:</p>
<pre><code>BUILD
Pulling image: gcr.io/kaniko-project/executor:latest
latest: Pulling from kaniko-project/executor
Digest: sha256:b9eec410fa32cd77cdb7685c70f86a96debb8b087e77e63d7fe37eaadb178709
Status: Downloaded newer image for gcr.io/kaniko-project/executor:latest
gcr.io/kaniko-project/executor:latest
INFO[0000] Resolved base name gcr.io/team-timesheets/builder to builder 
INFO[0000] Using dockerignore file: /workspace/.dockerignore 
INFO[0000] Retrieving image manifest gcr.io/team-timesheets/builder 
INFO[0000] Retrieving image gcr.io/team-timesheets/builder 
INFO[0000] Retrieving image manifest gcr.io/team-timesheets/builder 
INFO[0000] Retrieving image gcr.io/team-timesheets/builder 
INFO[0000] Built cross stage deps: map[]                
INFO[0000] Retrieving image manifest gcr.io/team-timesheets/builder 
INFO[0000] Retrieving image gcr.io/team-timesheets/builder 
INFO[0000] Retrieving image manifest gcr.io/team-timesheets/builder 
INFO[0000] Retrieving image gcr.io/team-timesheets/builder 
INFO[0001] Executing 0 build triggers                   
INFO[0001] Resolving srcs [functions/package*.json]...  
INFO[0001] Checking for cached layer gcr.io/team-timesheets/app/cache:9307850446a7754b17d62c95be0c1580672377c1231ae34b1e16fc284d43833a... 
INFO[0001] Using caching version of cmd: RUN npm ci     
INFO[0001] Resolving srcs [package*.json]...            
INFO[0001] Checking for cached layer gcr.io/team-timesheets/app/cache:7ca523b620323d7fb89afdd0784f1169c915edb933e1d6df493f446547c30e74... 
INFO[0001] Using caching version of cmd: RUN npm ci     
INFO[0001] Checking for cached layer gcr.io/team-timesheets/app/cache:1fd7153f10fb5ed1de3032f00b9fb904195d4de9dec77b5bae1a3cb0409e4530... 
INFO[0001] No cached layer found for cmd RUN npm run build:refs     &amp;&amp; npm run build:production 
INFO[0001] Unpacking rootfs as cmd COPY functions/package*.json ./ requires it. 
INFO[0026] WORKDIR /functions                           
INFO[0026] cmd: workdir                                 
INFO[0026] Changed working directory to /functions      
INFO[0026] Creating directory /functions                
INFO[0026] Taking snapshot of files...                  
INFO[0026] Resolving srcs [functions/package*.json]...  
INFO[0026] COPY functions/package*.json ./              
INFO[0026] Resolving srcs [functions/package*.json]...  
INFO[0026] Taking snapshot of files...                  
INFO[0026] RUN npm ci                                   
INFO[0026] Found cached layer, extracting to filesystem 
INFO[0029] WORKDIR /                                    
INFO[0029] cmd: workdir                                 
INFO[0029] Changed working directory to /               
INFO[0029] No files changed in this command, skipping snapshotting. 
INFO[0029] Resolving srcs [package*.json]...            
INFO[0029] COPY package*.json ./                        
INFO[0029] Resolving srcs [package*.json]...            
INFO[0029] Taking snapshot of files...                  
INFO[0029] RUN npm ci                                   
INFO[0029] Found cached layer, extracting to filesystem 
INFO[0042] COPY . .                                     
INFO[0043] Taking snapshot of files...                  
INFO[0043] RUN npm run build:refs     &amp;&amp; npm run build:production 
INFO[0043] Taking snapshot of full filesystem...        
INFO[0061] cmd: /bin/sh                                 
INFO[0061] args: [-c npm run build:refs     &amp;&amp; npm run build:production] 
INFO[0061] Running: [/bin/sh -c npm run build:refs     &amp;&amp; npm run build:production] 

&gt; thdk-timesheets-app@1.2.16 build:refs /
&gt; tsc -p common


&gt; thdk-timesheets-app@1.2.16 build:production /
&gt; webpack --env=prod

Hash: e33e0aec56687788a186
Version: webpack 4.43.0
Time: 81408ms
Built at: 12/04/2020 6:57:57 AM
....
</code></pre>
<p>Now, with the overhead of the cache system, there doesn't even seem to be a speed benefit.</p>
<p>I'm relatively new to Dockerfiles, so hopefully I'm just missing a simple line here.</p>","<p>Short answer: Cache invalidation is hard.</p>
<p>In a <code>RUN</code> section of a Dockerfile, <em>any</em> command can be run. In general, docker (when using local caching) or Kaniko now have do decide, if this step can be cached or not. This is usually determined by checking, if the output is deterministic - in other words: if the same command is run again, does it produce the same file changes (relative to the last image) than before?</p>
<p>Now, this simplistic view is not enough to determine a cacheable command, because any command can have side-effects that do not affect the local filesystem - for example, network traffic. If you run a <code>curl -XPOST https://notify.example.com/build/XYZ</code> to post a successful or failed build to some notification API, this should <em>not</em> be cached. Maybe your command is generating a random password for an admin-user and saves that to an external database - this step also should never be cached.</p>
<p>On the other hand, a completely reproducible <code>npm run build</code> could still result in two different bundled packages due to the way, that minifiers and bundlers work - e.g. where minified and uglified builds have different short variable names. Although the resulting builds are <em>semantically</em> the same, they are not on a byte-level - so although this step <em>could</em> be cached, docker or kaniko have no way of identifying that.</p>
<p>Distinguishing between cacheable and non-cacheable behavior is basically impossible and therefore you'll encounter problematic behavior in form of false-positives or false-negatives in caching again and again.</p>
<p>When I consult clients in building pipelines, I usually split Dockerfiles up into stages or put the cache-miss-or-hit-logic into a script, if docker decides wrong for a certain step.</p>
<p>When you split Dockerfiles, you have a base-image (which contains all dependencies and other preparation steps) and split off the custom-cacheable part into its own Dockerfile - the latter then references the former base-image. This usually means, that you have to have some form of templating in place (e.g. by having a <code>FROM ${BASE_IMAGE}</code> at the start, which then is rendered via <code>envsubst</code> or a more complex system like helm).</p>
<p>If that is not suitable for your usecase, you can choose to implement the logic yourself in a script. To find out, which files change, you can use <code>git diff --name-only HEAD HEAD~1</code>. By combining this with some more logic, you can customize your script behavior to only perform some logic if a certain set of files changed:</p>
<pre><code>#!/usr/bin/env bash
# only rebuild, if something changed in 'app/'
if [[ ! -z &quot;$(git diff --name-only HEAD HEAD~1 | grep -e '^(app/|package.*)')&quot; ]]; then
  npm run build:ref
  curl -XPOST https://notify.api/deploy/$(git rev-parse --short HEAD)
  // ... further steps ...
fi
</code></pre>
<p>You can easily extend this logic to your exact needs and take full control over the caching logic yourself - but you should only do this for steps involving false-positives or false-negatives by docker or kaniko, since all following steps will not be cached due to the undeterministic behavior.</p>"
"Cloud Build - Credentials error using exec wrapper to connect to Cloud SQL Proxy + other GCP resources<p>I'm attempting to use the <code>exec-wrapper</code> in a Cloud Build step to run the Cloud SQL Proxy and run a Node script to do a custom database migration. Here is what my cloud build config looks like:</p>
<pre><code>steps:
- name: gcr.io/cloud-builders/docker
  args: ['build', '-t', 'gcr.io/$PROJECT_ID/api-stg', '.']
- name: gcr.io/cloud-builders/docker
  args: ['push', 'gcr.io/$PROJECT_ID/api-stg']
- name: gcr.io/cloud-builders/gcloud
  args: ['app', 'deploy', 'app-stg.yaml', '--image-url=gcr.io/$PROJECT_ID/api-stg']
- name: &quot;gcr.io/google-appengine/exec-wrapper&quot;
  args: [&quot;-i&quot;, &quot;gcr.io/$PROJECT_ID/api-stg&quot;,
         &quot;-s&quot;, &quot;$PROJECT_ID:us-central1:&lt;Cloud SQL Instance Name&gt;&quot;,
         &quot;--&quot;, &quot;scripts/management/custom_migration&quot;]

images: ['gcr.io/$PROJECT_ID/api-stg']
timeout: 1200s # 20 minutes
</code></pre>
<p>And in my <code>custom_migration.js</code> file I have things like:</p>
<pre><code>const {Storage} = require('@google-cloud/storage');

const storage = new Storage();
const bucket = storage.bucket(BUCKET_NAME);
const file = await bucket.file(key);
const result = await new Promise(resolve =&gt; file.download((err, data) =&gt; {...}));
...
</code></pre>
<p>This causes the following error from the <code>google-auth-library</code>:</p>
<pre><code>Error: The file at /root/.google/credentials does not exist, or it is not a file. 
Error: ENOENT: no such file or directory, lstat '/root/.google'
</code></pre>
<p>My App Engine Flexible environment is able to run this code when deployed in a new version, but the same code in a Cloud Build step isn't credentialed correctly. How can I allow the exec-wrapper to use the default credentials of my App Engine Flexible environment?</p>","<p>This step works for me</p>
<pre><code>steps:
  - name: 'node:14-alpine'
    entrypoint: 'sh'
    args:
      - -c
      - |
        wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy
        chmod +x cloud_sql_proxy
        ./cloud_sql_proxy -instances=my-project-id:us-central1:vertx=tcp:5432 &amp;
        npm install @google-cloud/storage pg
        node index-test.js
</code></pre>
<p>Because I don't know the content of your custom container, you can try to adapt something like this</p>
<pre><code>steps:
  - name: 'gcr.io/$PROJECT_ID/api-stg'
    entrypoint: 'sh'
    args:
      - -c
      - |
        wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy
        chmod +x cloud_sql_proxy
        ./cloud_sql_proxy -instances=my-project-id:us-central1:vertx=tcp:5432 &amp;
        npm install @google-cloud/storage pg
        node scripts/management/custom_migration
</code></pre>
<hr />
<p>About the issue with the standard library, my supposition is a conflict between the AppEngine environment and the Cloud Build environment.</p>
<p>If you have a look to the Google Auth library, you can see a case for App Engine credentials, and another one for Compute credentials. The compute is used in standard on any Google Cloud services (Cloud Run, Cloud Functions, Compute Engine, Cloud Build,...) but App Engine has its specificities.</p>"
"ANGULAR build --prod error deploying in Google App Engine using CloudBuild<p>Helle there,</p>
<p>To contextualize, I am working on an web application using Google App Engine (GAE). In fact, I use three different GAE services. For the backend, analysis and the frontend with Angular.</p>
<p>In GIT my architecture is the following:</p>
<p><a href=""https://i.stack.imgur.com/Z1GAU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z1GAU.png"" alt=""Screen capture showing the GIT folder architecture"" /></a></p>
<p>Indeed, I wrote a global <code>cloudbuild.yaml</code> file on root as the following to call all of others specific <code>cloudbuild.yaml</code> files in each services directories (&quot;/api&quot;, &quot;/analyse&quot;, &quot;/frontend&quot;):</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    for d in */; do
      config=&quot;${d}cloudbuild.yaml&quot;
      if [[ ! -f &quot;${config}&quot; ]]; then
        continue
      fi

      echo &quot;Building $d ... &quot;
      (
        gcloud builds submit $d --config=${config}
      ) &amp;
    done
    wait
</code></pre>
<p>During the deployment, an error occurs in the &quot;frontend&quot; service steps let see his <code>cloudbuild.yaml</code> file:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:

  # Install Angular
  - name: 'gcr.io/cloud-builders/npm'
    args: ['install','-g','@angular/cli' ]

  # Install node packages
  - name: 'gcr.io/cloud-builders/npm'
    args: [ 'install' ]

  # Build productive files
  - name: 'node:12'
    entrypoint: npm
    args: [ 'run', 'build', '--prod', '--aot' ]

  # Deploy to google cloud app egnine
  - name: 'gcr.io/cloud-builders/gcloud'
    args: ['app', 'deploy', 'client.yaml']
</code></pre>
<p>The error occurs for the third step (in log step 2 because of starting at 0):</p>
<p><a href=""https://i.stack.imgur.com/1rqyc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1rqyc.png"" alt=""Screen capture showing error logs"" /></a></p>
<p>I am really confused with this error because I do not understand why the build method not working well. In fact, when I am running <code>npm run build --prod</code> locally there is no problem.</p>
<p>I hope you could help me to solve this problem.</p>
<p>Thanks to spend time on my issue. I love you community ❤️</p>
<p><strong>[EDIT]</strong></p>
<p>A little update to add the <code>package.json</code> dependencies.</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;name&quot;: &quot;frontend&quot;,
  &quot;version&quot;: &quot;0.0.0&quot;,
  &quot;description&quot;: &quot;The frontend part for the 4th year project&quot;,
  &quot;scripts&quot;: {
    &quot;ng&quot;: &quot;ng&quot;,
    &quot;start&quot;: &quot;ng serve&quot;,
    &quot;build&quot;: &quot;ng build&quot;,
    &quot;test&quot;: &quot;ng test&quot;,
    &quot;lint&quot;: &quot;ng lint&quot;,
    &quot;e2e&quot;: &quot;ng e2e&quot;
  },
  &quot;private&quot;: true,
  &quot;dependencies&quot;: {
    &quot;@angular/animations&quot;: &quot;~11.0.0&quot;,
    &quot;@angular/common&quot;: &quot;~11.0.0&quot;,
    &quot;@angular/compiler&quot;: &quot;~11.0.0&quot;,
    &quot;@angular/core&quot;: &quot;~11.0.0&quot;,
    &quot;@angular/forms&quot;: &quot;~11.0.0&quot;,
    &quot;@angular/platform-browser&quot;: &quot;~11.0.0&quot;,
    &quot;@angular/platform-browser-dynamic&quot;: &quot;~11.0.0&quot;,
    &quot;@angular/router&quot;: &quot;~11.0.0&quot;,
    &quot;rxjs&quot;: &quot;~6.6.0&quot;,
    &quot;tslib&quot;: &quot;^2.0.0&quot;,
    &quot;zone.js&quot;: &quot;~0.10.2&quot;
  },
  &quot;devDependencies&quot;: {
    &quot;@angular-devkit/build-angular&quot;: &quot;~0.1100.1&quot;,
    &quot;@angular/cli&quot;: &quot;~11.0.1&quot;,
    &quot;@angular/compiler-cli&quot;: &quot;~11.0.0&quot;,
    &quot;@types/jasmine&quot;: &quot;~3.6.0&quot;,
    &quot;@types/node&quot;: &quot;^12.11.1&quot;,
    &quot;codelyzer&quot;: &quot;^6.0.0&quot;,
    &quot;jasmine-core&quot;: &quot;~3.6.0&quot;,
    &quot;jasmine-spec-reporter&quot;: &quot;~5.0.0&quot;,
    &quot;karma&quot;: &quot;~5.1.0&quot;,
    &quot;karma-chrome-launcher&quot;: &quot;~3.1.0&quot;,
    &quot;karma-coverage&quot;: &quot;~2.0.3&quot;,
    &quot;karma-jasmine&quot;: &quot;~4.0.0&quot;,
    &quot;karma-jasmine-html-reporter&quot;: &quot;^1.5.0&quot;,
    &quot;protractor&quot;: &quot;~7.0.0&quot;,
    &quot;ts-node&quot;: &quot;~8.3.0&quot;,
    &quot;tslint&quot;: &quot;~6.1.0&quot;,
    &quot;typescript&quot;: &quot;~4.0.2&quot;
  }
}
</code></pre>
<p><strong>I suspect that the problem comes from the <code>&lt;base href=&quot;&quot;&gt;</code> or from paths in the <code>client.yaml</code> inside the frontend folder</strong></p>
<p>The <code>client.yaml</code> file:</p>
<pre class=""lang-yaml prettyprint-override""><code>service: default
runtime: python27
api_version: 1
threadsafe: true

handlers:
- url: /
  static_files: dist/index.html
  upload: frontend/index.html
- url: /
  static_dir: dist
</code></pre>
<p>And the <code>.gcloudignore</code></p>
<pre><code>e2e/
node_modules/
src/
coverage
^(.*/)?\..*$
^(.*/)?.*\.json$
^(.*/)?.*\.md$
^(.*/)?.*\.yaml$
^LICENSE
</code></pre>","<p>I found a solution.</p>
<p>In fact, this error occurs because the &quot;./src&quot; folder does not exist after deploying. My mistake, in my <code>.cloudignore</code> file I include the <code>src</code> folder so it is impossible for my application to build the <code>src</code> folder because it never imported.</p>
<p>The error sended was not really clear but I can go ahead now.</p>
<p>Thanks @DanielOcando for your time.</p>"
"Google Cloud Container Registry - Check image size<p>I sent a docker file for Google Cloud Build. The build was created successfully.</p>
<p>The artifact URL is:</p>
<blockquote>
<p>gcr.io/XXX/api/v1:abcdef017e651ee2b713828662801b36fc2c1</p>
</blockquote>
<p>How I can check the image size? (MB\GB)</p>","<p>There isn't API for this. But I have a workaround, this Linux command line</p>
<pre><code>curl -H &quot;Authorization: Bearer $(gcloud auth print-access-token)&quot; \
  https://gcr.io/v2/XXX/api/v1/manifests/abcdef017e651ee2b713828662801b36fc2c1 2&gt;/dev/null | \
  jq &quot;.layers[].size&quot; | \
  awk '{s+=$1} END {print s}'
</code></pre>
<p>Detail line by line</p>
<ol>
<li>Create a curl with a secure token from the GCLOUD CLI</li>
<li>Get the image manifest, which describe all the layers and their size</li>
<li>Get only the layers' sizes</li>
<li>Sum the sizes</li>
</ol>
<p>-&gt; Result is in Byte.</p>"
"How to set different ENV variable when building and deploying Docker Image to Cloud Run?<p>I have a backend service that I'll need to deploy to <code>Google Cloud Run</code>.</p>
<p>From <a href=""https://cloud.google.com/run/docs/quickstarts/build-and-deploy#node.js"" rel=""nofollow noreferrer"">Google's tutorial on Cloud Run</a>, we get that:</p>
<p>First you need to build your image and send it to <strong>Cloud Build</strong>.</p>
<pre><code>gcloud builds submit --tag gcr.io/PROJECT-ID/helloworld
</code></pre>
<p>Only then you deploy it to Cloud Run:</p>
<pre><code>gcloud run deploy --image gcr.io/PROJECT-ID/helloworld --platform managed
</code></pre>
<p>I get the sequence above. But I'll be deploying this service to 2 different environments: <code>TEST</code> and <code>PROD</code>.</p>
<p>So I need an <code>SERVER_ENV</code> variable, that should be <code>&quot;PROD&quot;</code> on my production environment, and of course it should be <code>&quot;TEST&quot;</code> on my test environment. This is so my server (express server that will be run from the container) knows which database to connect to.</p>
<p>But the problem is that I only have a single <code>Dockerfile</code>:</p>
<pre><code>FROM node:12-slim

ENV SERVER_ENV=PROD

WORKDIR /

COPY ./package.json ./package.json
COPY ./distApp ./distApp
COPY ./distService ./distService
COPY ./public ./public

RUN npm install

ENTRYPOINT npm start
</code></pre>
<p>So how can I set different <code>ENV</code> variables while following the build &amp; deploy sequence above? Is there an option in the <code>gcloud builds submit</code> comment that I can maybe override something? Or use a different <code>Dockerfile</code>? Anybody got other ideas?</p>
<p><strong>AN IDEA:</strong></p>
<p>Maybe use the <a href=""https://cloud.google.com/cloud-build/docs/build-config"" rel=""nofollow noreferrer"">Cloud Build configuration file</a>?</p>
<p><code>cloudbuild.yaml</code></p>","<p>You can't achieve this without a <code>cloudbuild.yaml</code> file. The command <code>gcloud builds submit --tag ...</code> doesn't accept extra docker parameter.</p>
<p>Here an example of configuration</p>
<pre><code>FROM node:12-slim

ARG SERVER_CONF=PROD
ENV SERVER_ENV=$SERVER_CONF

WORKDIR /

COPY ./package.json ./package.json
COPY ./distApp ./distApp
COPY ./distService ./distService
COPY ./public ./public

RUN npm install

ENTRYPOINT npm start
</code></pre>
<p><em>I created a build argument <code>SERVER_CONF</code>. Your <code>ENV</code> will take this value at build time. The default value is <code>PROD</code></em></p>
<p>Now your <code>cloudbuild.yaml</code> file</p>
<pre><code>step:
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '--tag=gcr.io/PROJECT-ID/helloworld', '--build-arg=&quot;SERVER_CONF=$_SERVER_CONF&quot;', '.']
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/PROJECT-ID/helloworld']
substitutions:
  _SERVER_CONFPROD: PROD
</code></pre>
<p><em>Use substitution variables to change the environment. Not that here you can also set a default value, that override your Dockerfile value. Take care of this!</em></p>
<p><em>You can also set the tag as substitution variable if you want</em></p>
<p>Eventually, how to call your Cloud Build</p>
<pre><code># With default server conf (no substitution variables, the the file default)
gcloud builds submit

# With defined server conf
gcloud builds submit --substitutions=_SERVER_CONF=TEST
</code></pre>"
"In what region does cloud build execute its builds in?<p>My organisation is based in the EU, and as such we have created all associated gcp-resources within the eu-region.</p>
<p>To speed up our builds, we maintain a dependency cache zip in a storage bucket, which we read from at the start of a build, and write to at the end of a build, depending on whether the dependencies have changed or not.</p>
<p>Now I wonder if we can expect builds to go faster, if we use a storage bucket located within a us-region, which brings me to my question:</p>
<p>In what region are cloud build builds executed in?</p>","<p>To check this, you can run this step:</p>
<pre class=""lang-yaml prettyprint-override""><code>  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - -c
      - |
        curl -H &quot;Metadata-Flavor: Google&quot; http://metadata.google.internal/computeMetadata/v1/instance/zone
</code></pre>
<p>For my project it's <code>projects/751286965207/zones/us-central1-f</code></p>
<p>Not that is the same thing for all projects. In any case, a new feature is coming: <a href=""https://cloud.google.com/sdk/gcloud/reference/beta/builds/worker-pools/create"" rel=""nofollow noreferrer"">worker-pool</a></p>
<p>The API is public but the project is in private preview. The principle is to have a pool of VM on which you can trigger Cloud Build jobs. Thanks to that, you will be able to define the region, but also, the connectivity to a private network (useful to deploy to a private GKE cluster for example) and to size your VM as you need. The counterpart, it should be more expensive.</p>"
"What represents a STEP in Cloud Build<p>Actually I am working on a pipeline, all good until there because it has worked. But when I want to explain it is not clear to me what each step represents physically, for example a step &quot;could&quot; be a node within a cluster.Please, if someone has a clear explanation of it, explain it to us.</p>
<ul>
<li>Example 1 of a step</li>
</ul>
<p>File config cloud build:</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
 - name: &quot;gcr.io/google.com/cloudsdktool/cloud-sdk&quot;
  args: [&quot;bin/deploy-dags-composer.sh&quot;]
  env:
    - 'COMPOSER_BUCKET=${_COMPOSER_BUCKET}'
    - 'ARTIFACTS_BUCKET=${_ARTIFACTS_BUCKET}'
  id: 'DEPLOY-DAGS-PLUGINS-DEPENDS-STEP'
</code></pre>
<p>Bash File</p>
<pre class=""lang-sh prettyprint-override""><code>#! /bin/bash
gsutil cp bin/plugins/* gs://$COMPOSER_BUCKET/plugins/
gsutil cp bin/dependencies/* gs://$ARTIFACTS_BUCKET/dags/dependencies/
gsutil cp bin/dags/* gs://$COMPOSER_BUCKET/dags/
gsutil cp bin/sql-scripts/* gs://$ARTIFACTS_BUCKET/path/bin/sql-scripts/composer/
</code></pre>
<ul>
<li>Example 2 several steps</li>
</ul>
<p>File config cloud build</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: gcr.io/cloud-builders/gsutil
  args: ['cp', '*', 'gs://${_COMPOSER_BUCKET}/plugins/']
  dir: 'bin/plugins/'
  id: 'deploy-plugins-step'
- name: gcr.io/cloud-builders/gsutil
  args: ['cp', '*', 'gs://${_ARTIFACTS_BUCKET}/dags/dependencies/']
  dir: 'bin/dependencies/'
  id: 'deploy-dependencies-step'
- name: gcr.io/cloud-builders/gsutil
  args: ['cp', '*', 'gs://${_COMPOSER_BUCKET}/dags/']
  dir: 'bin/dags/'
  id: 'deploy-dags-step'
- name: gcr.io/cloud-builders/gsutil
  args: ['cp', '*', 'gs://${_ARTIFACTS_BUCKET}/projects/path/bin/sql-scripts/composer/']
  dir: 'bin/sql-scripts/'
  id: 'deploy-scripts-step'
</code></pre>","<p>In Cloud Build, a step is a stage of the processing. This stage is described by a container to load, containing the required binaries for the processing to perform in the stage.</p>
<p>To this container, you can define an entrypoint, the binary to run in side the container, and args to pass to it.</p>
<p>You have also several option that you can see <a href=""https://cloud.google.com/cloud-build/docs/build-config"" rel=""nofollow noreferrer"">here</a>.</p>
<p>An important concept to understand is that ONLY the <code>/workspace</code> directory is kept from one step to another one. At the end of each step, the container is offloaded and you lost all the data in memory (like environment variable) and the data stored outside of the <code>/workspace</code> directory (such as system dependency installation). Keep this in mind, many of issues come from there.</p>
<hr />
<p>EDIT 1:</p>
<p>In a step, you can, out of the box, run 1 command on one container. gsutil, gcloud, mvn, node,.... All depends on your container and your entrypoint.</p>
<p>But there is a useful advance capacity, when you need to run many commands on the same container. It can occur for many reason. You can do such like this</p>
<pre><code>  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - -c
      - |
        MY_ENV_VAR=$(curl -H &quot;Metadata-Flavor: Google&quot; http://metadata.google.internal/computeMetadata/v1/instance)
        echo $${MY_ENV_VAR} &gt; env-var-save.file
        # Continue the commands that you need/want ....
</code></pre>"
"Does not getting build failure status even the build not successful run(cloud-build remote builder)<p>Cloud-build is not showing build failure status</p>

<p>I created my own remote-builder which <strong>scp</strong> all files from <strong>/workspace</strong> to my <strong>Instance</strong> and running build on using <strong>gcloud compute ssh -- COMMAND</strong>
<a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/remote-builder"" rel=""nofollow noreferrer"">remote-builder</a></p>

<pre><code>#!/bin/bash 
USERNAME=${USERNAME:-admin}
REMOTE_WORKSPACE=${REMOTE_WORKSPACE:-/home/${USERNAME}/workspace/}
GCLOUD=${GCLOUD:-gcloud}


KEYNAME=builder-key
ssh-keygen -t rsa -N """" -f ${KEYNAME} -C ${USERNAME} || true
chmod 400 ${KEYNAME}*

cat &gt; ssh-keys &lt;&lt;EOF
${USERNAME}:$(cat ${KEYNAME}.pub)
EOF

${GCLOUD} compute scp --compress --recurse \
       $(pwd)/ ${USERNAME}@${INSTANCE_NAME}:${REMOTE_WORKSPACE} \
       --ssh-key-file=${KEYNAME}

${GCLOUD} compute ssh --ssh-key-file=${KEYNAME} \
       ${USERNAME}@${INSTANCE_NAME} -- ${COMMAND}

</code></pre>

<p>below is the example of the code to run build(<em>cloudbuild.yaml</em>)</p>

<pre><code>steps:
- name: gcr.io/$PROJECT_ID/remote-builder
  env:
    - COMMAND=""docker build -t [image_name]:[tagname] -f Dockerfile .""
</code></pre>

<p>During docker build inside Dockerfile it got failure and show errors in log but status showing <strong>SUCCESS</strong></p>

<p>can any help me how to resolve it.</p>

<p>Thanks in advance.</p>","<p>I wrote all docker commands on bash script and add below error handling code to it.</p>

<pre><code>handle_error() {
    echo ""FAILED: line $1, exit code $2""
    exit 1
}
trap 'handle_error $LINENO $?' ERR
</code></pre>

<p>It works!</p>"
"Stenciljs e2e tests on Google Cloud Build<p>TL;DR: does anyone know how to make a valid stencil.js docker image to run stencil build &amp; test?</p>

<p>Long form:</p>

<p>To run stencil.js e2e tests on Google Cloud Build you need a <a href=""https://cloud.google.com/cloud-build/docs/create-custom-build-steps"" rel=""nofollow noreferrer"">custom build step</a> as a docker image.<br>
Here's a sample Dockerfile:</p>

<pre><code># THESE STEPS GET STENCIL BUILD WORKING &amp; SHOULD HAVE GOT TESTING WORKING
FROM node:10-jessie-slim
WORKDIR /
RUN npm init stencil app stencil
WORKDIR /stencil
COPY package*.json ./
RUN npm install
WORKDIR /stencil/node_modules/puppeteer
RUN npm install
WORKDIR /stencil

# STEPS ADDED BASED ON https://github.com/puppeteer/puppeteer/blob/master/docs/troubleshooting.md#running-puppeteer-in-docker
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends ca-certificates curl wget &amp;&amp; rm -rf /var/lib/apt/lists/*
RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \
    &amp;&amp; sh -c 'echo ""deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main"" &gt;&gt; /etc/apt/sources.list.d/google.list' \
    &amp;&amp; apt-get update \
    &amp;&amp; apt-get install -y google-chrome-unstable fonts-ipafont-gothic fonts-wqy-zenhei fonts-thai-tlwg fonts-kacst fonts-freefont-ttf \
      --no-install-recommends \
    &amp;&amp; rm -rf /var/lib/apt/lists/*
RUN npm i puppeteer \
    # Add user so we don't need --no-sandbox.
    # same layer as npm install to keep re-chowned files from using up several hundred MBs more space
    &amp;&amp; groupadd -r pptruser &amp;&amp; useradd -r -g pptruser -G audio,video pptruser \
    &amp;&amp; mkdir -p /stencil/home/pptruser/Downloads \
    &amp;&amp; chown -R pptruser:pptruser /stencil/home/pptruser \
    &amp;&amp; chown -R pptruser:pptruser /stencil/node_modules

ENTRYPOINT [""npm""]
</code></pre>

<p>Now you insert this into a cloud build.yaml file:</p>

<pre><code>steps:
  #1 Build stencil project
  - name: 'gcr.io/$PROJECT_ID/stencil'
    args: ['run','build']
  #2 Test stencil project
  - name: 'gcr.io/$PROJECT_ID/stencil'
    args: ['test']
</code></pre>

<p>In this build file step #1 works, validating the stencil install.  However, step #2 fails with error message:</p>

<blockquote>
  <p>[ ERROR ]  Chromium revision is not downloaded. Run ""npm install"" or ""yarn
  Step #2:            install"" Error: Chromium revision is not downloaded. Run ""npm
  Step #2:            install"" or ""yarn install"" at Launcher.launch
  Step #2:            (/workspace/node_modules/puppeteer/lib/Launcher.js:120:15)</p>
</blockquote>

<p>The error above is about puppeteer not finding chromium (even though a local version is installed) and I have already ran the npm install on puppeteer and validated that the local chromium is installed.  However <a href=""https://github.com/puppeteer/puppeteer/blob/master/docs/troubleshooting.md#running-puppeteer-in-docker"" rel=""nofollow noreferrer"">Puppeteer documentation</a> does mention running puppeteer in Docker is tricky and gives a solution, but their solution is for a docker container dedicated just to Puppeteer.</p>

<p>Does anyone have an idea how I can create a valid stencil.js docker image with a valid puppeteer setup?</p>","<p>I finally managed to get stencil working in a container suitable for Google Cloud Build.
The main problem was that puppeteer, which is needed for e2e test, does not work as installed because it does not have a chrome install with all necessary dependencies.  </p>

<p>To fix you have to do three things:</p>

<ol>
<li>Install Chrome separately</li>
<li>Point puppeteer to the installed Chrome</li>
<li>Modify stencil config to invoke testing without a sandbox</li>
</ol>

<p>1 &amp; 2 are addressed with the following Dockerfile:</p>

<pre><code># Need jessie to install dependencies
FROM node:10-jessie-slim

# Copy files from stencil project
WORKDIR /
COPY package*.json ./
COPY node_modules/ ./node_modules

# Install wget &amp; dependencies needed to install Chrome (next step)
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends ca-certificates curl wget &amp;&amp; rm -rf /var/lib/apt/lists/*

# Install Chromium dev &amp; dependencies
RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \
    &amp;&amp; sh -c 'echo ""deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main"" &gt;&gt; /etc/apt/sources.list.d/google.list' \
    &amp;&amp; apt-get update \
    &amp;&amp; apt-get install -y google-chrome-unstable fonts-ipafont-gothic fonts-wqy-zenhei fonts-thai-tlwg fonts-kacst fonts-freefont-ttf \
      --no-install-recommends \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

# Point puppeteer to the Chromium just installed
ENV PUPPETEER_EXECUTABLE_PATH '/usr/bin/google-chrome-unstable'

# Set entry point
ENTRYPOINT [""npm""]
</code></pre>

<p>Note that this Dockerfile must be placed in the same directory as your stencil project --i.e. it's in the same location as your stencil.config.ts and package.json.  </p>

<p>Also note that this will only work if you have already run your tests in your local environment.  Doing so ensures that the necessary testing dependencies are installed.</p>

<p>3 is fixed by modifying your stencil.config.ts as shown in <a href=""https://stenciljs.com/docs/end-to-end-testing#caveat-about-e2e-tests-automation-on-cd-ci"" rel=""nofollow noreferrer"">stencil documentation</a> so chrome runs without a sandbox:</p>

<pre><code>export const config: Config = {
  namespace: 'Foo',
  testing: {
    // run chrome with no sandbox to have it work in a container
    browserArgs: ['--no-sandbox', '--disable-setuid-sandbox'],
  },
  outputTargets: [
    { type: 'dist' },
    {
      type: 'www',
    },
  ],
};
</code></pre>"
"Cloudbuild with Python 2.7 and Apache Beam<p>I created a pipeline with Apache Beam on Python 2.7 that runs on Google Dataflow. This pipeline works well when I deploy it locally from my laptop. I now wish to deploy it via CloudBuild. This is my cloudbuild.yaml file:</p>

<pre><code>steps:
  - name: ""docker.io/library/python:2.7""
    args: [""pip"", ""install"", ""-t"", ""/workspace/lib"", ""-r"", ""requirements.txt""]
  - name: ""docker.io/library/python:2.7""
    args: [""python2"", ""tests.py""]
    env: [""PYTHONPATH=/workspace/lib""]
</code></pre>

<p>When the CloudBuild is triggered, it successfully installs all the requirements, but then when it tries to import apache_beam in the tests.py file, I recieve the following error:</p>

<pre><code>   File ""tests.py"", line 3, in &lt;module&gt;
     import apache_beam as beam
   File ""/workspace/lib/apache_beam/__init__.py"", line 97, in &lt;module&gt;
     from apache_beam import coders
   File ""/workspace/lib/apache_beam/coders/__init__.py"", line 19, in &lt;module&gt;
     from apache_beam.coders.coders import *
   File ""/workspace/lib/apache_beam/coders/coders.py"", line 29, in &lt;module&gt;
     import google.protobuf.wrappers_pb2
 ImportError: No module named google.protobuf.wrappers_pb2
</code></pre>

<p>In the requirements.txt file I have <em>inter alia</em> the following:</p>

<pre><code>apache-beam==2.16.0
protobuf==3.11.0
</code></pre>

<p>Note: All the necessary requirements are listed in requirements.txt, since I can deploy the pipeline locally.</p>","<p>I was able to reproduce your issue with only another file named <code>tests.py</code> with one line:</p>

<pre class=""lang-py prettyprint-override""><code>import apache_beam as beam
</code></pre>

<p>It seems that it is a <a href=""https://github.com/tensorflow/tensorflow/issues/6341"" rel=""nofollow noreferrer"">known issue</a> in some communities. </p>

<p>First of all, this error does not happen if instead of using the deprecated Python2 you use instead Python3.7. I suppose though that Python2.7 is required.</p>

<p>I created a Dockerfile such as:</p>

<pre><code>FROM python:2

WORKDIR /usr/src/app

COPY requirements.txt ./

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ ""python"", ""./tests.py"" ]
</code></pre>

<p>And then, changed the <code>cloudbuild.yaml</code>:</p>

<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/quickstart-image', '.' ]
</code></pre>

<p>which builds succesfully:</p>

<pre><code>Step 5/6 : COPY . .
 ---&gt; 082407f9672d
Step 6/6 : CMD [ ""python"", ""./tests.py"" ]
 ---&gt; Running in 7d53e96370f9
Removing intermediate container 7d53e96370f9
 ---&gt; d60a6d473d21
Successfully built d60a6d473d21
</code></pre>"
"Pass Repository Name to Slack bot during Cloud Build<p>I am following <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/slackbot"" rel=""nofollow noreferrer"">the documentation</a> to get a Slack bot to report on Cloud Build progress for a Project in Google Cloud Platform, using the Slack image found in <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community"" rel=""nofollow noreferrer"">Google Cloud Builder Community</a>. I've been able to get the out-of-the-box messages to post successfully.</p>

<p>Since I have multiple repositories that will get built, I want to post the name of the Repository that is being built:</p>

<p>e.g. </p>

<pre><code>Build bot: Repo Fizz-buzz built successfully...
Build bot: Repo Wizz-bang failed to build...
</code></pre>

<p>In the <code>Notify()</code> function, the <code>cloudbuild.Build</code> struct has a few properties that might do what I am wanting. The <code>Source.RepoSource.RepoName</code> looks ideal, but it's nil.</p>

<p>Another option would be to have the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/blob/271173e4539787c5c8a7a0c16a5e767598b932c8/slackbot/slackbot/trigger.go#L18"" rel=""nofollow noreferrer"">trigger()</a> function add the repository name as an argument:</p>

<pre><code>Args: []string{
                    fmt.Sprintf(""--build=%s"", build),
                    fmt.Sprintf(""--webhook=%s"", webhook),
                    fmt.Sprintf(""--repoName=%s"", repoName),
                    ""--mode=monitor"",
                },
</code></pre>

<p>I'd prefer to have the Notify() function handle this, but I haven't found the appropriate struct property.</p>

<p>Is there a property that I can use during the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/blob/07ff8507dd8bfac6ebd7ea291ba3d9f7515123d8/slackbot/slackbot/notify.go#L14"" rel=""nofollow noreferrer"">notify()</a> function to pass along the repository name?</p>",<p>I use github app triggered builds and use <code>build.substitutions.REPO_NAME</code></p>
"Unable to copy hidden files using gcloud scp in cloud build - remote builder<p>I'm running cloud build using <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/remote-builder"" rel=""nofollow noreferrer"">remote builder</a>, able to copy all file in the workspace to my own VM but, unable to copy hidden files</p>

<p><strong>Command used to copy files</strong></p>

<pre><code>gcloud compute scp --compress --recurse '/workspace/*' [username]@[instance_name]:/home/myfolder --ssh-key-file=my-key --zone=us-central1-a
</code></pre>

<p>so, this copies only non-hidden files.</p>

<p><strong>Also used dot operator to copy hidden files</strong></p>

<pre><code>gcloud compute scp --compress --recurse '/workspace/.' [username]@[instance_name]:/home/myfolder --ssh-key-file=my-key --zone=us-central1-a
</code></pre>

<p>Still not able to copy and got below error</p>

<pre><code>scp: error: unexpected filename: .
</code></pre>

<p>Can anyone suggest to me how to copy hidden files to VM using gcloud scp.</p>

<p>Thanks in advance</p>","<p>If you remove the trailing character after the slash, it may work. For example, this worked for me:</p>

<pre><code>gcloud compute scp --compress --recurse 'test/' [username]@[instance_name]:/home/myfolder
</code></pre>"
"How to enable cloudbuild.yaml for zip-based CloudFunction deployments?<p>Given some generic Python code, structured like ...</p>

<pre><code>cloudbuild.yaml
requirements.txt
functions/
    folder_a/
        test/
            main_test.py
        main.py
</code></pre>

<p>If I'm ...</p>

<ul>
<li>creating a .zip from above folder and</li>
<li>using either Terraform's <a href=""https://www.terraform.io/docs/providers/google/r/cloudfunctions_function.html"" rel=""nofollow noreferrer""><code>google_cloudfunctions_function</code></a> resource or <a href=""https://cloud.google.com/functions/docs/deploying/repo"" rel=""nofollow noreferrer""><code>gcloud functions deploy</code></a> to upload/deploy the function</li>
</ul>

<p>... it seems the build configuration for cloudbuild (<code>cloudbuild.yaml</code>) included in the .zip is never considered during build (i.e. while / prior to resolving <code>requirements.txt</code>).</p>

<p>I've set up <code>cloudbuild.yaml</code> to grant access to a private github repository (which contains a dependency listed in <code>requirements.txt</code>). Unfortunately, build fails with (terraform output):</p>

<blockquote>
  <p>Error: Error waiting for Updating CloudFunctions Function: Error code 3, message: Build failed: {""error"": {""canonicalCode"": ""INVALID_ARGUMENT"", ""errorMessage"": ""<code>pip_download_wheels</code> had stderr output:\nCommand \""git clone -q ssh://git@github.com/SomeWhere/SomeThing.git /tmp/pip-req-build-a29nsum1\"" failed with error code 128 in None\n\nerror: <code>pip_download_wheels</code> returned code: 1"", ""errorType"": ""InternalError"", ""errorId"": ""92DCE9EA""}}</p>
</blockquote>

<p>According to <a href=""https://cloud.google.com/cloud-build/docs/access-private-github-repos#configure_the_build"" rel=""nofollow noreferrer"">cloud build docs</a>, a <code>cloudbuild.yaml</code> can be specified using <code>gcloud builds submit --config=cloudbuild.yaml .</code> -- is there any way to supply that parameter to <code>gcloud functions deploy</code> (or even Terraform), too? I'd like to stay with the current, ""transparent"" code build, i.e. I do not want to set up code build separately but just upload my zip and have the code be built and deployed ""automatically"", while respecting <code>codebuild.yaml</code>.</p>","<p>It looks like you're trying to authenticate to a private Git repo via SSH. <a href=""https://cloud.google.com/functions/docs/writing/specifying-dependencies-python#using_private_dependencies"" rel=""nofollow noreferrer"">This is unfortunately not currently supported by Cloud Functions.</a></p>

<p>The alternative would be to vendor your private dependency into the directory before creating your <code>.zip</code> file.</p>"
"GCP Cloud Build ubuntu step runs successfully but without expected results<p>ubuntu returns expected results from <code>ls</code> until I add a <code>cd</code> command then it returns nothing.</p>

<p>Here's a truncated outline of my project structure:</p>

<pre><code>gcp_cicd_workflow
  |-- src
     | my_module.py
  |-- tests
     | test_my_module.py
</code></pre>

<p>The log messages I'm providing below are ordered with the newest on top. Note that there's a lot of ubuntu log messages related to pulling the image that I have excluded from the log messages.</p>

<p>Code: </p>

<pre><code>  # Step 4
- name: 'ubuntu'
  entrypoint: '/bin/bash'
  args: ['-c', 'ls']
</code></pre>

<p>Log The <code>ls</code> command returns the expected results - it returns all the folders and files in the workspace folder:    </p>

<pre><code>Finished Step #4
Step #4: tests
Step #4: src
Step #4: setup.py
Step #4: requirements.txt
Step #4: python_cloud_builder
Step #4: gcp_cicd
Step #4: gcp.egg-info
Step #4: gcp-cicd-workflow
Step #4: cloudbuild.yaml
Step #4: __init__.py
Step #4: README.md
</code></pre>

<p>Code:</p>

<pre><code>  # Step 4
- name: 'ubuntu'
  entrypoint: '/bin/bash'
  args: ['-c', 'cd tests', 'ls']
</code></pre>

<p>Log: No results returned after the <code>cd</code> command:  </p>

<pre><code>Finished Step #4
</code></pre>

<p>Code - cd to nonexistent folder:</p>

<pre><code>  # Step 4
- name: 'ubuntu'
  entrypoint: '/bin/bash'
  args: ['-c', 'cd foo']
</code></pre>

<p>Log - Get expected ""no such file ... ""result):  </p>

<pre><code>Finished Step #4
Step #4: /bin/bash: line 0: cd: foo: No such file or directory
</code></pre>","<p>Try below code:</p>

<pre><code>- name: 'ubuntu'
  entrypoint`enter code here`: '/bin/bash'
  args: ['-c', 'cd tests; ls']
</code></pre>

<p><code>$ bash -c</code> runs passed arguments as a single bash script. It doesn't separate passed arguments into separate scripts.</p>

<p><code>;</code> in bash ends a script line, and the following after semicolon is interpreted as a new script line.</p>"
"How to pass private repository credentials to maven docker image when using Google Cloud Build<p>I am trying to use Google Cloud Build to build my Java app. It allows to use so called cloud builders - docker images of different builders. I am using Maven. So the problem is that I have to use a private repository (artifactory) to deploy artifacts. This rep is password protected and I do not know how to pass these credentials to GC maven docker container.</p>

<p>I see that the only possible way is: </p>

<ol>
<li><p>To run the shell script which will update the maven container settings.xml with something like:</p>

<pre><code>&lt;servers&gt;
    &lt;server&gt;
        &lt;id&gt;myRepoName&lt;/id&gt;
        &lt;username&gt;${server.username}&lt;/username&gt;
        &lt;password&gt;${server.password}&lt;/password&gt;
    &lt;/server&gt;
&lt;/servers&gt;
</code></pre></li>
<li><p>set env variables in the cloudbuild.yml</p></li>
</ol>

<p>Are there any other elegant ways to achieve what I'm trying to?</p>","<p>I solved this by doing the following:</p>

<ol>
<li><p>Create a Google Cloud Storage bucket and upload your desired <code>settings.xml</code>. I'm using GitHub Packages, following <a href=""https://help.github.com/en/github/managing-packages-with-github-packages/configuring-apache-maven-for-use-with-github-packages#authenticating-to-github-packages"" rel=""noreferrer"">their documentation</a></p></li>
<li><p>Setup your cloudbuild.yaml with the following:</p></li>
</ol>

<pre><code>steps:
  - name: gcr.io/cloud-builders/gsutil
    args: ['cp', 'gs://ci-maven/settings.xml', 'settings.xml']
  - name: maven:3.6.3-jdk-11-openj9
    entrypoint: 'mvn'
    args: ['--settings', '/workspace/settings.xml', 'install']
images: ['gcr.io/schemata-np/scheduler']
</code></pre>

<p>First, it copies the settings.xml to the current directory (<code>/workspace</code>). Then, using the Docker Maven image directly, we add <code>--settings /workspace/settings.xml</code> to our args to specify the settings.xml location. From there, Google Cloud Build was able to pull my private GitHub package to properly install my project.</p>

<p>It may be possible to copy to <code>/usr/share/maven/ref/</code> in the first step to allow the default Maven Docker behavior, but I was not able to get this to work. If anyone does, let me know!</p>

<p>Based on <a href=""https://stackoverflow.com/a/55800624/2693009"">this answer to a slightly different question about caching artifacts</a> and <a href=""https://cloud.google.com/cloud-build/docs/speeding-up-builds#caching_directories_with_google_cloud_storage"" rel=""noreferrer"">Google Cloud Build documentation</a></p>"
"Failed to get existing workspaces: querying Cloud Storage failed: storage: bucket doesn't exist<p>I was using terraform in cloud build, but it fails at this step</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>steps:
  # Terraform
  - id: 'configure_terraform'
    name: node:10.16.3
    entrypoint: ""node""
    args: [""./create_terraform_config.js"",
           ""../terraform/override.tf"",
           ""${_TERRAFORM_BUCKET_NAME}"",
           ""${_TERRAFORM_BUCKET_PATH}""]
    dir: ""app/scripts""
  - id: 'init_terraform'
    name: hashicorp/terraform:light
    args: [""init""]
    dir: ""app/terraform""</code></pre>
</div>
</div>
</p>
<pre><code>Initializing the backend...

Successfully configured the backend &quot;gcs&quot;! Terraform will automatically
use this backend unless the backend configuration changes.

Error: Failed to get existing workspaces: querying Cloud Storage failed: storage: bucket doesn't exist
</code></pre>","<p>This will fix the issue</p>

<pre><code>terraform init -reconfigure
</code></pre>

<p>reference: <a href=""https://github.com/hashicorp/terraform/issues/23532#issuecomment-560493391"" rel=""noreferrer"">https://github.com/hashicorp/terraform/issues/23532#issuecomment-560493391</a></p>"
"What are the risks to Google Container Registry if the project name is changed?<p>I have a CI/CD pipeline configured where Google Cloud Build automatically builds containers from code pushed to a GitHub repo if tagged with a specific tag. The containers are automatically deposited into Google Container Registry. Each container in the registry is tagged with a tag in the form <code>us.gcr.io/project_name/container_name:tag_name</code></p>

<p>My question is if I change the project name, how will this affect the containers currently sitting in Google Container Registry that I have already tagged with the current project name? Do I need to change the tag on each container with the intended project name before updating the project name itself? Is this handled automatically by GCP? </p>","<p><a href=""https://cloud.google.com/container-registry/docs/overview#registry_name"" rel=""nofollow noreferrer"">Registries in Container Registry are named by the host and project ID</a>. To work with images (for example push, pull, delete) identify the image using the following format:</p>

<p>[HOSTNAME]/[PROJECT-ID]/[IMAGE]:[TAG] </p>

<p>Or [HOSTNAME]/[PROJECT-ID]/[IMAGE]@[IMAGE_DIGEST]</p>

<p>Even if you change your PROJECT NAME, PROJECT ID can’t be changed, because <a href=""https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects"" rel=""nofollow noreferrer"">Project ID: a unique identifier for your project, composed of the project name and a randomly assigned number</a>. So your images keep with the same registries in Container Registry.  us.gcr.io/project_id/container_name:tag_name</p>"
"How to setup a upstream and downstream trigger in google cloud build?<p>I have to run cloud build trigger one after another and only if first trigger pass it has to move to next trigger. 
All the triggers are for different repos.</p>

<p>How can i achieve it?</p>

<p>Thanks in advance!</p>","<p>Cloud Build publishes messages on a Google Pub/Sub topic when your build's state changes, such as when your build is created, when your build transitions to a working state, and when your build completes. You can have a look at the doc <a href=""https://cloud.google.com/cloud-build/docs/send-build-notifications"" rel=""nofollow noreferrer"">here</a> for more info.</p>

<p>You could set up a <a href=""https://cloud.google.com/functions/docs/calling/pubsub"" rel=""nofollow noreferrer"">PubSub-triggered Cloud Function</a> to process these events and programmatically launch the next Cloud Build via the API (see the API tab <a href=""https://cloud.google.com/cloud-build/docs/running-builds/start-build-manually"" rel=""nofollow noreferrer"">here</a>).  This is a but cumbersome since you have to define your builds in the API call's body but as of now there's no chaining capability in Cloud Build.</p>"
"Error response from daemon: pull access denied for buildcontainer, repository does not exist or may require 'docker login'<p><strong>dockerfile:</strong></p>

<pre><code>FROM node:10
ADD . /app
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 3000
CMD [ ""node"", ""index.js"" ]
</code></pre>

<p><strong>cloudbuild.yaml:</strong></p>

<pre><code>steps:
- name: buildcontainer
args: ['build', '-t', 'gcr.io/$PROJECT_ID/coffee2goserver:${SHORT_SHA}', '.']
- name: 'pushcontainer'
args: [""push"", ""gcr.io/$PROJECT_ID/coffee2goserver""]
</code></pre>

<p><strong>error:</strong></p>

<blockquote>
  <p>Error response from daemon: pull access denied for buildcontainer,
  repository does not exist or may require 'docker login'</p>
</blockquote>","<p>Google Cloud Build doesn't provide any <code>buildcontainer</code> or <code>pushcontainer</code> images you can use in your builds (unless you have created them yourself).</p>

<p>You may use the official Docker image in your step like so:</p>

<pre><code>steps:
- name: docker
args: ['build', '-t', 'gcr.io/$PROJECT_ID/coffee2goserver:${SHORT_SHA}', '.']
- name: docker
args: [""push"", ""gcr.io/$PROJECT_ID/coffee2goserver:${SHORT_SHA}""]
</code></pre>

<p>The Docker build step is automatically set up with credentials for your <a href=""https://cloud.google.com/cloud-build/docs/permissions"" rel=""nofollow noreferrer"">Cloud Build Service Account</a>. These permissions are sufficient to interact directly with GCR.</p>

<p>Also, as David Maze@ mentioned in his comment, be careful to push your image with the same name it was build with, including the <code>${SHORT_SHA}</code> in this case.</p>"
"How do I attach a GPU to a Google Cloud build?<p>I am trying to run a CI/CD on my codebase, but in order to run my tests, I need a GPU-enabled VM (to produce deep learning results). </p>

<p>However, the only configurable machine option I see is the machine type (number of cores and memory). I don't see an option for adding an accelerator type (GPU).</p>

<p>Is there a way to attach a GPU to the build VM, and if not, is there another method for triggering a test on another GPU enabled VM?</p>

<p>Thanks!</p>","<p>Google Cloud Build doesn't provide machine types equipped with GPUs at the moment. One option though is to use the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/remote-builder"" rel=""nofollow noreferrer"">remote-builder</a> cloud builder. It allows you to run your builds on a Compute Engine instances running in your project. You can use the <code>INSTANCE_ARGS</code> option to customize the instance to fit your specific needs, adding one or more GPUs in this case. You can have a look <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/remote-builder/examples/test-configs"" rel=""nofollow noreferrer"">here</a> for some example configs. You can use any flag available with the <code>gcloud compute instances create</code> <a href=""https://cloud.google.com/sdk/gcloud/reference/compute/instances/create"" rel=""nofollow noreferrer"">command</a>, including the <code>--accelerator</code> flag for GPUs.</p>"
"Gcloud with cloudbuild and Django Postgres cause psycopg2 ImportError<p>I am building a Django based application on App Engine. I have created a Postres CloudSql instance. I created a cloudbuild.yaml file with a Cloud Build Trigger.</p>

<p>django = v2.2</p>

<p>psycopg2 = v2.8.4</p>

<p>GAE runtime: python37</p>

<p>The cloudbuild.yaml:</p>

<pre><code>steps:
- name: 'python:3.7'
  entrypoint: python3
  args: ['-m', 'pip', 'install', '-t', '.', '-r', 'requirements.txt']
- name: 'python:3.7'
  entrypoint: python3
  args: ['./manage.py', 'migrate', '--noinput']
- name: 'python:3.7'
  entrypoint: python3
  args: ['./manage.py', 'collectstatic', '--noinput']
- name: ""gcr.io/cloud-builders/gcloud""
  args: [""app"", ""deploy""]
timeout:   ""3000s""
</code></pre>

<p>The deploymnet is going alright, the app can connect to the database. But when I try load a page I get the next error:
""<strong>...import psycopg2 as Database File ""/srv/psycopg2/__init__.py"", line 50, in  from psycopg2._psycopg import ( # noqa ImportError: libpython3.7m.so.1.0: cannot open shared object file: No such file or directory</strong>""</p>

<p>Another interesting thing is if I deploy my app with 'gcloud app deploy' (not through Cloud Build), everything is alright I am not getting the error above, my app can communicate with the database.</p>

<p>I am pretty new with gcloud, so maybe I missed some basic here. 
But my questions are: </p>

<p>-What is missing from my cloudbuild.yaml to make it work?</p>

<p>-Do I pip install my dependencies to the correct place?</p>

<p>-The prospective of this error what is the difference with the Cloud Build based deployment and the manual one?</p>","<p>From what I see you're using Cloud Build to run <code>gcloud app deploy</code>. </p>

<p>This command commits your code and configuration files to App Engine. As explained <a href=""https://cloud.google.com/appengine/docs/standard/python3/runtime"" rel=""nofollow noreferrer"">here</a> App engine runs in a Google managed environment that automatically handles the installation of the dependencies specified in the <code>requirements.txt</code> file and executes the entrypoint you defined in your <code>app.yaml</code>. This has the benefit of not having to manually trigger the instalation of dependencies. The first two steps of your cloudbuild are not affecting the App Engine's runtime, since the configuration of it is managed by the aforementioned files once they're deployed.</p>

<p>The purpose of <a href=""https://cloud.google.com/cloud-build/docs/overview"" rel=""nofollow noreferrer"">Cloud Build</a> is to import source code from a variety of repositories and build binaries or images according to your specifications. It could be used to <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/create-basic-configuration#creating_a_build_config"" rel=""nofollow noreferrer"">build Docker images and push them to a repository</a>, <a href=""https://cloud.google.com/cloud-build/docs/create-custom-build-steps#creating_a_custom_build_step"" rel=""nofollow noreferrer"">download a file to be included in Docker build</a> or <a href=""https://cloud.google.com/cloud-build/docs/quickstart-go"" rel=""nofollow noreferrer"">package a Go binary an upload it to Cloud Storage</a>. Furthermore the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/tree/master/gcloud"" rel=""nofollow noreferrer"">gcloud builder</a> is aimed to run gcloud commands through a build pipeline for example to create account permissions or configure firewall rules when these are required steps for another operation to succeed.</p>

<p>Since you're not automatizing a build pipeline but trying to deploy an App Engine application Cloud build is not the product you should be using. The way to go when deploying to App Engine is to simply run <code>gcloud app deploy</code> command and let Google's environment take care of the rest for you.</p>"
"How the triggers are added to Queue in google cloud build?<p>Build trigger should wait untill the previous trigger for a same repos finishes its execution.</p>

<p>If i push twice to the repo, trigger executed twice at same time.
I don't want this to be happen.</p>

<p>How to make the cloud trigger to wait for the previous trigger job?</p>

<p>Thanks in advance!</p>","<p>You would have to implement this configuration and logic. It would have to evaluate if the build has been triggered and running and then wait for it to finish. </p>

<p>Maybe something can be done in order to check with the buildSteps and a <a href=""https://cloud.google.com/cloud-build/docs/configuring-builds/configure-build-step-order"" rel=""nofollow noreferrer"">waitFor</a> or with a <a href=""https://cloud.google.com/cloud-build/docs/cloud-builders#writing_your_own_custom_builder"" rel=""nofollow noreferrer"">custom builder</a>.</p>

<p>A combination between a call to the API and listing the Cloud Builds and they're statuses, implementing maybe waitFor in your config so it checks and then proceed with the build once the other is finished. </p>

<p>The thing is depending on how many builds you are submitting I don't have a clear idea on how you can assign a priority in the queue as it would go FIFO. </p>

<p>Someone else added a similar question <a href=""https://stackoverflow.com/questions/55905120/how-to-prevent-cloud-build-from-running-builds-in-parallel"">here</a></p>

<p>Hope this helps.</p>"
"Can I delete container images from Google Cloud Storage artifacts bucket?<p>I have a Google App Engine app, which connects to Google Cloud Storage. </p>

<p>I noticed that the amount of data stored was unreasonably high (4.01 GB, when it should be 100MB or so). </p>

<p>So, I looked at how much each bucket was storing, and I found that there was an automatically created bucket called <code>us.artificats.</code> that was taking up most of the space.</p>

<p>I looked inside, and all it has is one folder: <code>containers/images/</code>. </p>

<p>From what I've Googled, it seems like these images come from Google Cloud Build.</p>

<p>My question is, can I delete them without compromising my entire application?</p>","<p>For those of you seeing this later on, I ended up deleting the folder, and everything was fine.</p>
<p>When I ran Google Cloud Build again, it added items back into the bucket, which I had to delete later on.</p>
<p>As @HarshitG mentioned, this can be set up to happen automatically via deletion rules in cloud storage. As for myself, I added a deletion step to my deployment GitHub action.</p>"
"gcloud builds submit fails while docker push + gcloud run deploy work just fine?<p>EDIT: The so called duplicate question was way off since 1. I could push another image and 2. I could not push a build image.  Finally, point #3 is the solution was totally different and ONLY related to pushing build images via cloudbuild.  ie. I beg to differ that this question WAS different.</p>

<p>Running into some more google cloud security stuff.  We currently deploy to cloud run like so</p>

<pre><code>docker build . --tag gcr.io/myproject/authservice
docker push gcr.io/myproject/authservice

gcloud run deploy staging-admin --region us-west1 --image gcr.io/myproject/authservice --platform managed
</code></pre>

<p>I did the quick start for google builds but I am getting permission errors.  I did this command</p>

<pre><code>https://cloud.google.com/cloud-build/docs/quickstart-build
</code></pre>

<p>The command I ran was</p>

<pre><code>gcloud builds submit --tag gcr.io/myproject/quickstart-image
</code></pre>

<p>This is all the same project but submitting builds gets this same error over and over and over(I am not sure why it doesn't just exit on first error.</p>

<pre><code>The push refers to repository [gcr.io/myproject/quickstart-image]
e3831abe9997: Preparing
60664c29ef5a: Preparing
denied: Token exchange failed for project 'myproject'. Caller does not have permission 'storage.buckets.get'. To configure permissions, follow instructions at: https://cloud.google.com/container-registry/docs/access-control
</code></pre>

<p>Any ideas how to fix so I can use google cloud build?</p>","<p>BE WARNED: I read the duplicate question post but in my case </p>

<ol>
<li>I can push items</li>
<li>only the build one is failing AND the solution I found is different than any of the other question answers</li>
</ol>

<p>This was a VERY weird issue.  The storage permission MUST be a red herring because these permissions fixed the issue</p>

<p><a href=""https://i.stack.imgur.com/BaOrL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BaOrL.png"" alt=""enter image description here""></a></p>

<p>I found some documentation somewhere that I can't seem to find on a google github repo about adding these permissions AND a document on the TWO @cloudbuild.gserviceaccount.com accouts AND you must add the permissions to the correct one!!!!  One is owned by google and you should not touch.</p>"
"GCR - image missing all but two files, local image is complete<p>I have a container I'm deploying to Kubernetes (GKE), and the image I have built locally is good, and runs as expected, but it appears that the image being pulled from Google Container Registry, when the run command is changed to <code>pwd &amp;&amp; ls</code> returns the output shown here:</p>

<pre><code>I 2020-06-17T16:24:54.222382706Z /app
I 2020-06-17T16:24:54.226108583Z lost+found
I 2020-06-17T16:24:54.226143620Z package-lock.json
</code></pre>

<p>and the output of the same commands when running in the container locally, with <code>docker run -it &lt;container:tag&gt; bash</code> is this:</p>

<pre><code>#${API_CONTAINER} resolves to gcr.io/&lt;project&gt;/container: I.E. tag gets appended

.../# docker run -it ${API_CONTAINER}latest bash   
root@362737147de4:/app# pwd
/app
root@362737147de4:/app# ls
Dockerfile       dist          files  node_modules       package.json  ssh.bat      stop_forever.bat  test      tsconfig.json
cloudbuild.yaml  environments  log    package-lock.json  src           startApi.sh  swagger.json      test.pdf  tsconfig.test.json
root@362737147de4:/app# 
</code></pre>

<p>My thoughts on this start with, either the push to the registry is literally failing to work, or I'm not pulling the right one, i.e. pulling some off <code>latest</code> tag that was build by cloud build in a previous attempt to get this going.</p>

<p>What could be the potential issue? what could potentially fix this issue?</p>

<p>Edit: After using differing tags in deployment, using <code>--no-cache</code> during build, and pulling from the registry on another machine, my inclination is that GKE is having an issue pulling the image from GCR. Is there a way I can put this somewhere else, or get visibility on what's going on with the pull?</p>

<p>EDIT 2:</p>

<p>So Yes, I have a docker file I can share, but please be aware that I have inherited it, and don't understand the process that went into building this, or why some steps were necessary to the other developer. (I am definitely interested in refactoring this as much as possible.</p>

<pre><code>FROM node:8.12.0

RUN mkdir /app
WORKDIR /app

ENV PATH /app/node_modules/.bin:$PATH

RUN apt-get update &amp;&amp; apt-get install snmp -y

RUN npm install --unsafe-perm=true
RUN apt-get update \
    &amp;&amp; apt-get install -y \
    gconf-service \
    libasound2 \
    libatk1.0-0 \
    libatk-bridge2.0-0 \
    libc6 \
    libcairo2 \
    libcups2 \
    libdbus-1-3 \
    libexpat1 \
    libfontconfig1 \
    libgcc1 \
    libgconf-2-4 \
    libgdk-pixbuf2.0-0 \
    libglib2.0-0 \
    libgtk-3-0 \
    libnspr4 \
    libpango-1.0-0 \
    libpangocairo-1.0-0 \
    libstdc++6 \
    libx11-6 \
    libx11-xcb1 \
    libxcb1 \
    libxcomposite1 \
    libxcursor1 \
    libxdamage1 \
    libxext6 \
    libxfixes3 \
    libxi6 \
    libxrandr2 \
    libxrender1 \
    libxss1 \
    libxtst6 \
    ca-certificates \
    fonts-liberation \
    libappindicator1 \
    libnss3 \
    lsb-release \
    xdg-utils \
    wget

COPY . /app


# Installing puppeteer and chromium for generating PDF of the invoices.


# Install latest chrome dev package and fonts to support major charsets (Chinese, Japanese, Arabic, Hebrew, Thai and a few others)
# Note: this installs the necessary libs to make the bundled version of Chromium that Puppeteer
# installs, work.
RUN apt-get update \
    &amp;&amp; apt-get install -y wget gnupg libpam-cracklib \
    &amp;&amp; wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \
    &amp;&amp; sh -c 'echo ""deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main"" &gt;&gt; /etc/apt/sources.list.d/google.list' \
    &amp;&amp; apt-get update \
    &amp;&amp; apt-get install -y google-chrome-unstable fonts-ipafont-gothic fonts-wqy-zenhei fonts-thai-tlwg fonts-kacst fonts-freefont-ttf \
      --no-install-recommends \
    &amp;&amp; rm -rf /var/lib/apt/lists/*


# Uncomment to skip the chromium download when installing puppeteer. If you do,
# you'll need to launch puppeteer with:
#     browser.launch({executablePath: 'google-chrome-unstable'})
# ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD true

# Install puppeteer so it's available in the container.
RUN npm i puppeteer \
    # Add user so we don't need --no-sandbox.
    # same layer as npm install to keep re-chowned files from using up several hundred MBs more space
    &amp;&amp; groupadd -r pptruser &amp;&amp; useradd -r -g pptruser -G audio,video pptruser \
    &amp;&amp; mkdir -p /home/pptruser/Downloads \
    &amp;&amp; chown -R pptruser:pptruser /home/pptruser \
    &amp;&amp; chown -R pptruser:pptruser /app/node_modules

#build the api, and move into place.... framework options are limited with the build.
RUN npm i puppeteer kiwi-server-cli &amp;&amp; kc build -e prod
RUN rm -Rf ./environments &amp; rm -Rf ./src &amp;&amp; cp -R ./dist/prod/* .


# Run everything after as non-privileged user.
# USER pptruser

CMD [""google-chrome-unstable""] # I have tried adding this here as well ""&amp;&amp;"", ""node"", ""src/server.js""
</code></pre>

<p>For pushing the image I'm using this command: 
<code>docker push gcr.io/&lt;projectid&gt;/api:latest-&lt;version&gt;</code> and I have the credentials setup with <code>cloud auth configure-docker</code> and here's a sanitized version of the yaml manifest:</p>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kompose.cmd: kompose convert -f ./docker-compose.yml
    kompose.version: 1.21.0 ()
  creationTimestamp: null
  labels:
    io.kompose.service: api
  name: api
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: api
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        kompose.cmd: kompose convert -f ./docker-compose.yml
        kompose.version: 1.21.0 ()
      creationTimestamp: null
      labels:
        io.kompose.service: api
    spec:
      containers:
      - args:
        - bash
        - -c
        - node src/server.js
        env:
        - name: NODE_ENV
          value: production
        - name: TZ
          value: America/New_York
        image: gcr.io/&lt;projectId&gt;/api:latest-0.0.9
        imagePullPolicy: Always
        name: api
        ports:
        - containerPort: 8087
        resources: {}
        volumeMounts:
        - mountPath: /app
          name: api-claim0
        - mountPath: /files
          name: api-claim1
      restartPolicy: Always
      serviceAccountName: """"
      volumes:
      - name: api-claim0
        persistentVolumeClaim:
          claimName: api-claim0
      - name: api-claim1
        persistentVolumeClaim:
          claimName: api-claim1
status: {}

</code></pre>","<p>The solution comes from the original intent of the <code>docker-compose.yml</code> file which was converted into a kubernetes manifest via a tool called <code>kompose</code>. The original docker-compose file was intended for development and as such had overrides in place to push the local development environment into the running container.</p>

<p>This was because of this in the yml file:</p>

<pre><code>services:
  api:
    build: ./api
    volumes:
      - ./api:/app
      - ./api/files:/files
</code></pre>

<p>which translates to this on the kubernetes manifest:</p>

<pre><code>      volumeMounts:
      - mountPath: /app
        name: api-claim0
      - mountPath: /files
        name: api-claim1
      volumes:
      - name: api-claim0
        persistentVolumeClaim:
          claimName: api-claim0
      - name: api-claim1
        persistentVolumeClaim:
          claimName: api-claim1
</code></pre>

<p>Which Kubernetes has no files to supply, and the app is essentially overwritten with an empty volume, so the file is not found.</p>

<p>removal of the directives in the kubernetes manifest resulted in success.</p>

<p>Reminder to us all to be mindful.</p>"
"Proper permission for Cloud Build to deploy to Firebase?<p>Permissions recommended for the Cloud Build service account in the <a href=""https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-firebase#required_iam_permissions"" rel=""nofollow noreferrer"">official Google documentation</a> and the <a href=""https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/firebase#with-iam-roles"" rel=""nofollow noreferrer"">Firebase CLI community builder docs</a> are insufficient:</p>
<blockquote>
<ol start=""3"">
<li>In the permissions table, locate the email ending with @cloudbuild.gserviceaccount.com, and click on the pencil icon.</li>
<li>Add <code>Cloud Build Service Account</code>, <code>Firebase Admin</code> and <code>API Keys Admin roles</code>.</li>
</ol>
</blockquote>
<p>I still get the following error in Cloud Build when I do <code>firebase deploy</code>:</p>
<blockquote>
<p>Error: HTTP Error: 403, The caller does not have permission</p>
</blockquote>
<p>What I've tried is different Firebase IAM roles, <code>Editor</code>, and <code>Owner</code>. So far only the <code>Owner</code> role works. That is way too much privilege for a Cloud Build service account, and violates the least-privilege model.</p>
<p>Everything is in the same Google Cloud project.</p>
<p>Anyone know how to troubleshoot this? Or know which role/permission is missing?</p>","<p><strong>tl;dr</strong> seems like it was &quot;an accidental permission expansion&quot; that has been corrected.</p>
<p>I am able restrict the roles to:</p>
<ol>
<li><code>Cloud Build Service Account</code></li>
<li><code>Firebase Admin</code></li>
<li><code>API Keys Admin</code></li>
</ol>"
"invalid cloud build timeout?<p>We have a build that takes anywhere from 1 minute to 15 minutes(monobuild that is not parallized yet so it may build 8 servers or 1).  It was timing out so I modified the build file to</p>
<pre><code>steps:
- name: gcr.io/$PROJECT_ID/continuous-deploy
  timeout: 1200s
</code></pre>
<p>I also ran these commands(the last one failed though even though I got that from another post so it worked for them somehow)...</p>
<pre><code>Deans-MacBook-Pro:orderly dean$ gcloud config set app/cloud_build_timeout 1250
Updated property [app/cloud_build_timeout].
Deans-MacBook-Pro:orderly dean$ gcloud config set builds/timeout 1300
Updated property [builds/timeout].
Deans-MacBook-Pro:orderly dean$ gcloud config set container/build-timeout 1350
ERROR: (gcloud.config.set) Section [container] has no property [build-timeout].
Deans-MacBook-Pro:orderly dean$
</code></pre>
<p>I get the following error that anything greater than 10 minutes is invalid on google</p>
<pre><code>invalid build: invalid timeout in build step #0: build step timeout &quot;20m0s&quot; must be &lt;= build timeout &quot;10m0s&quot;
</code></pre>
<p>Why MUST it be less than 10m0s?  I really need our builds to be about 20 minutes.</p>
<p>I was going off of</p>
<p><a href=""https://stackoverflow.com/questions/60070274/why-cant-i-override-the-timeout-on-my-google-cloud-build"">Why can&#39;t I override the timeout on my Google Cloud Build?</a></p>
<p>and</p>
<p><a href=""https://stackoverflow.com/questions/60431732/gcp-cloud-build-ignores-timeout-settings"">GCP Cloud build ignores timeout settings</a></p>
<p>thanks,
Dean</p>","<p>The timeout of the steps should be less or equal than the timeout of the whole task.</p>
<p>By setting the timeout at the step level to 20 minutes it is causing the error as the default timeout for the whole task is 10 minutes by default.</p>
<p>The way to avoid this happenning is to set the timeout of the full task to be grater or equal to the the timeout of the specific steps.</p>
<p>I added a small example on how to define this.</p>
<pre><code>steps:
- name: gcr.io/$PROJECT_ID/continuous-deploy
  timeout: 1200s # Step Timeout

timeout: 1200s  # Full Task Timeout

</code></pre>"
"Why the history in gcb is not able to sort by branchname?<p>I'm able to sort by source but, unable to sort by branch, it shows only few branches for all sources.</p>
<p>I used it like
<code>Source:[source_name] OR Branch:[branch_name]</code>
in the history tab.</p>
<p><a href=""https://i.stack.imgur.com/NX1A7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NX1A7.png"" alt=""Console"" /></a></p>
<p>can anyone tell me how to sort based on the branch?</p>","<p>I think that is normal, I have the same thing on my side.</p>
<p>My history:
<a href=""https://i.stack.imgur.com/2Ptv1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2Ptv1.png"" alt=""enter image description here"" /></a></p>
<p>2 branches: Master and Open-source. However when I filter on the branch, even master I have nothing:</p>
<p><a href=""https://i.stack.imgur.com/puRMf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/puRMf.png"" alt=""enter image description here"" /></a></p>
<p>Why?? Because I have any trigger on the master or open-source branch. My trigger is on any branch.</p>
<p><a href=""https://i.stack.imgur.com/KDHC2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KDHC2.png"" alt=""enter image description here"" /></a></p>
<p>Thus the build branch param is not set and thus you can search/filter on it. On others project, when I have a filter per branch, the history is correct and I can filter on the branch name.</p>
<p>An alternative is to use the gcloud command and the filter param like this</p>
<pre><code>gcloud builds list --filter=&quot;substitutions.BRANCH_NAME=&lt;YourBranchName&gt;&quot;
</code></pre>
<p>More detail on the <a href=""https://cloud.google.com/sdk/gcloud/reference/topic/filters"" rel=""nofollow noreferrer"">filter capabilities and expression</a></p>"
"Access a file in one bucket by reading the path from another file using gsutil<p>What is the best way to access a file in one bucket whose path is stored in another file in different bucket?</p>
<p>Most multi fold steps suggest using Cloud Storage for persisting values between steps and writing values of variables into files, but after many attempts, we are unable to achieve this even for a simple use case in Cloud Build setup.</p>
<p>Few of the many things we have tried:</p>
<p>Concatenating steps in Cloud Shell which works in Cloud Shell, but fails in Cloud Build steps:</p>
<pre><code>gsutil cp gs://bucket_1/filepath.txt filepath.txt &amp;&amp; filepath=$(&lt;filepath.txt) &amp;&amp; gsutil cp gs://bucket_2/filepath local_dest.txt
</code></pre>
<p>Cloud Build fails as it doesn't recognize the command &quot;filepath=$(&lt;filepath.txt)&quot;.</p>
<p>Cloudbuild.yaml steps (simplified to test). Step1 succeeds, but Step2 fails</p>
<pre><code>- name: gcr.io/cloud-builders/gsutil
  id: 'step1'
  entrypoint: 'gsutil'
  args: ['cp', 'gs://bucket_1/filepath.txt', 'filepath.txt']
  volumes:
  - name: data
    path: /persistent_volume

- name: gcr.io/cloud-builders/gsutil
  id: 'step2'
  entrypoint: 'gsutil'
  args: ['filepath=$(&lt;filepath.txt)', 'gsutil cp gs://anc-android-builds/$BRANCH_NAME/filepath local_dest.txt']
  volumes:
  - name: data
    path: /persistent_volume
</code></pre>
<p>Error:
CommandException: Invalid command &quot;filepath=$(&lt;filepath.txt)&quot;.</p>
<p>We've tried different ways of pushing this, and breaking it down into multiple steps as well, but nothing seems to work.</p>
<p>This must be a simple answer, but we can't seem to figure this out. Please help and advise.</p>","<p>In order to achieve what you're looking for, you need to modify your 2nd step, as for now the entrypoint is expecting a gsutil command but not receiving it straight away. Therefore you need to change to something like:</p>
<pre><code>- name: gcr.io/cloud-builders/gsutil
  id: 'step2'
  entrypoint: 'bash'
  args:
      - '-c'
      - |
      - filepath=$(&lt;filepath.txt) &amp;&amp; gsutil cp gs://anc-android-builds/$BRANCH_NAME/filepath local_dest.txt
  volumes:
  - name: data
    path: /persistent_volume
</code></pre>
<p>You may need to tweak it a bit more, depending on you exact scenario but this should be the correct path to achieve what you are intending to.</p>"
"Cloud Build docker image unable to write files locally - fail to open file... permission denied<p>Using Service Account credentials, I am successful at running Cloud Build to spin up gsutil, move files from gs into the instance, then copy them back out.  All is good.</p>
<p>One of the Cloud Build steps successfully loads a docker image from outside source, it loads fine and reports its own help info successfully.  But when run, it fails with the error message:
&quot;fail to open file &quot;..intermediary_work_product_file.&quot; permission denied.</p>
<p>For the app I'm running in this step, this error is typically produced when the file cannot be written to its default location.  I've set dir = &quot;/workspace&quot; to confirm the default.</p>
<p>So how do I grant read/write permissions to the app running inside a Cloud Build step to write its own intermediary work product to the local folders?   The Cloud Build itself is running fine using Service Account credentials.  Have tried adding more permissions including with Storage, Cloud Run, Compute Engine, App Engine admin roles.  But the same error.</p>
<p>I assume that the credentials used to create the instance are passed to the run time.  Have dug deep into the GCP CloudBuild documentation and examples, but found no answers.</p>
<p>There must be something fundamental I'm overlooking.</p>","<p>This problem was resolved by changing the Dockerfile USER as suggested by <a href=""https://github.com/PRAJINPRAKASH"" rel=""nofollow noreferrer"">@PRAJINPRAKASH</a> in this helpful answer  <a href=""https://stackoverflow.com/a/62218160/4882696"">https://stackoverflow.com/a/62218160/4882696</a></p>
<p>Tried to solve this by systematically testing GCP services and role permissions.  All Service Account credentials tested were able to create container instances, and run gcloud or gutil fine.  However, the custom apps created containers but failed when doing local write even to the default shared /workspace.</p>
<p>When using GCP Cloud Build, local read/write permissions do not &quot;pass through&quot; from the default service account to the runtime instance.  The documentation is not clear on this.</p>"
"In Google cloudbuild.yaml, what is the - | argument?<p>In Google cloudbuild tutorial the example cloudbuild.yaml uses - | as one of the arguments.</p>
<pre><code>  args:
  - '-c'
  - |
      if [ -d &quot;environments/$BRANCH_NAME/&quot; ]; then
...
</code></pre>
<p>What is the purpose of '- |'</p>","<p>That character is called &quot;Literal Block Scalar&quot; and it is used to span values across multiple lines. Spanning with <code>|</code> will include the newlines and any trailing spaces. You can also span with <code>&gt;</code> but this will fold new lines to spaces.</p>
<p>Example:</p>
<pre><code>include_newlines: |
        exactly as you see
        will appear these three
        lines of poetry     

fold_newlines: &gt;
        this is really a
        single line of text
        despite appearances
</code></pre>
<p>If you want to know more about the yaml syntax, you can visit <a href=""https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html"" rel=""nofollow noreferrer"">this</a> for information that may not be included in the cloud build <a href=""https://cloud.google.com/cloud-build/docs/build-config"" rel=""nofollow noreferrer"">documentation</a>.</p>"
"Cloud Build could not create an image and publish it to Container Registry<p>My objective is to create a CI/CD for a Django Rest Framework project. I am using <code>Google Source Repository</code> for version control and <code>Google Compute Engine VM</code> instance for deployment. <code>Dockerfile</code> of this project is able to create image and I am also able to run that image.</p>
<p>Now, on <code>Source Repository</code> when I merge other branches with master branch the cloud trigger should be able to create a new image, push it to the <code>Container Repository</code> and update an existing instance of <code>Google Compute Engine VM</code> with the new container.</p>
<p>So far on every push to master branch <code>Cloud Build - Trigger</code> are able create an image and push it over <code>Container Registry</code>. But <code>Cloud Build - Trigger</code> uses <code>Dockerfile</code> not <code>cloudbuild.yaml</code>.</p>
<p>This is <code>cloudbuild.ymal</code> which I crated.</p>
<pre><code>steps:
    - name: 'gcr.io/myproject-100/docker'
      args: [ 'build', '-t', 'gcr.io/myproject-100/dropoff:', '.' ]

    - name: 'gcr.io/cloud-builders/gcloud'
      args: [ 'compute', 'instance', 'update-container', 'dropoff-staging-v3', --zone='northeast1-a' ]
    
    images:
    - 'gcr.io/myproject-100/dropoff'
</code></pre>
<p>The <code>Dockerfile</code> and <code>cloudbuild.yaml</code> are in the root directory of project folder. When I run command <code>gcloud builds submit --config cloudbuild.yaml</code> I get this error -</p>
<pre><code>ERROR: (gcloud.builds.submit) parsing cloudbuild.yaml: while parsing a block collection
  in &quot;cloudbuild.yaml&quot;, line 2, column 5
expected &lt;block end&gt;, but found u'?'
  in &quot;cloudbuild.yaml&quot;, line 4, column 5
</code></pre>
<ol>
<li>What has caused this error and how can I fix this.</li>
<li>Do you think the way I wrote <code>cloudbuild.yaml</code> will help me achieve my objective.</li>
</ol>","<p>That <code>cloudbuild.yaml</code> seems completely unrelated,</p>
<p>because line 2 and 4, column 5 has no &quot;u&quot;; the whole line doesn't have one.</p>
<p>See <a href=""https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/getting_started_with_kubernetes/yaml_in_a_nutshell"" rel=""nofollow noreferrer"">YAML in a Nutshell</a> for how to get your indentation right; it should look more alike this:</p>
<pre><code>steps:
  - name: 'gcr.io/dropoff-280002/docker'
    args: [ 'build', '-t', 'gcr.io/dropoff-280002/dropoff:', '.' ]
  - name: 'gcr.io/cloud-builders/gcloud'
    args: [ 'compute', 'instance', 'update-container', 'dropoff-staging-v3', --zone='northeast1-a' ]    
images:
  - 'gcr.io/dropoff-280002/dropoff'
</code></pre>
<p>Or see this one <a href=""https://github.com/syslogic/cloudbuild-android/blob/master/cloudbuild.yaml"" rel=""nofollow noreferrer""><code>cloudbuild.yaml</code></a> ...which is known to be working.</p>"
"CloudBuild Bash-style string manipulation<p>I need to extract a string from the TAG_NAME default variable. but i could not get this to work.</p>
<pre><code>- name: 'gcr.io/cloud-builders/git'
  id: find-folder-name
  dir: ${_DIR}
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      if [ ${_STRATEGY} = &quot;tag&quot; ]; echo &quot;tag name &quot; $TAG_NAME; echo ${TAG_NAME%\.np\.v\.*};fi
  volumes:
  - name: 'ssh'
    path: /root/.ssh
  secretEnv: ['GCLOUD_SERVICE_KEY']
</code></pre>
<p>The regex works perfect fine if i just run it in gitbash locally.
The output is follow, i am expecting it will also print &quot;test&quot; as well, but it is empty. here is the output</p>
<pre><code>tag name  test.np.v.1.1.7
</code></pre>","<p>Fixed by assigning to a variable</p>
<pre><code>release=$TAG_NAME;echo ${release%\.np\.v\.*}
</code></pre>"
"google api key gets 401<p>I am trying to call some endpoints on google cloud build but I just get 401 and I am wondering how to fix this?  In the end, I want to curl but if the website is not working, there is truly something wrong!  Here is the picture</p>

<p><a href=""https://i.stack.imgur.com/rSZvd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rSZvd.png"" alt=""enter image description here""></a></p>","<p>Only a group of API services/products on Google Cloud Platform support API keys without more authentication methods (OAuth). You can find these specific services <a href=""https://cloud.google.com/docs/authentication/api-keys"" rel=""nofollow noreferrer"">here</a>. In the same page that you posted, if you go down on the documentation there should be a ""Authorization Scopes"" section listing the required OAuth scopes that the API requires, if so, it means OAuth is needed in order to use the service.</p>

<p>I look at the <a href=""https://cloud.google.com/cloud-build/docs/api/reference/rest/v1/projects.builds/list"" rel=""nofollow noreferrer"">Cloud Build Rest Api Doc</a> and indeed I can see OAuth is required, in addition to not being listed <a href=""https://cloud.google.com/docs/authentication/api-keys"" rel=""nofollow noreferrer"">here</a>. You need to implement OAuth for this specific Service (Cloud Build API).</p>"
"Delimit BigQuery REGEXP_EXTRACT strings in Google Cloud Build YAML script<p>I have a complex query that creates a View within the BigQuery console.
I have simplified it to the following to illustrate the issue</p>
<pre><code>SELECT
REGEXP_EXTRACT(FIELD1,  r&quot;[\d]*&quot;) as F1,
REGEXP_REPLACE(FIELD2, r&quot;\'&quot;, &quot;&quot;) AS F2,
FROM `project.mydataset.mytable`
</code></pre>
<p>Now I am trying to automate the creation of the view with cloud build.
I cannot workout how to delimit the strings inside the regex to work with both yaml and SQL.</p>
<pre><code>- name: 'gcr.io/cloud-builders/gcloud'
entrypoint: 'bq'
args: [
'mk',
'--use_legacy_sql=false',
'--project_id=${_PROJECT_ID}',
'--expiration=0',
'--view= 
    REGEXP_EXTRACT(FIELD1,  r&quot;[\d]*&quot;) as F1 ,
    REGEXP_REPLACE(FIELD2, r&quot;\'&quot;, &quot;&quot;) AS F2,
    REGEXP_EXTRACT(FIELD3,  r&quot;\[(\d{3,12}).*\]&quot;) AS F3
    FROM `project.mydataset.mytable`&quot; 
    '${_TARGET_DATASET}.${_TARGET_VIEW}'
  ]
</code></pre>
<p>I get the following error</p>
<blockquote>
<p>Failed to trigger build: failed unmarshalling build config
cloudbuild/build-views.yaml: json: cannot unmarshal number into Go
value of type string</p>
</blockquote>
<p>I have tried using Cloud Build substitution parameters, and as many combinations of <a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/lexical"" rel=""nofollow noreferrer"">SQL</a> and <a href=""https://stackoverflow.com/questions/37427498/how-to-escape-double-and-single-quotes-in-yaml-within-the-same-string"">YAML</a> escape sequences as I can think of to find a working solution.</p>","<p>Generally, you want to use <a href=""https://yaml.org/spec/1.2/spec.html#id2793652"" rel=""nofollow noreferrer"">block scalars</a> in such cases, as they do not process any special characters inside them and are terminated via indentation.</p>
<p>I have no idea how the command is <em>supposed</em> to look, but here's something that's at least valid YAML:</p>
<pre><code>- name: 'gcr.io/cloud-builders/gcloud'
  entrypoint: 'bq'
  args:
  - 'mk'
  - '--use_legacy_sql=false'
  - '--project_id=${_PROJECT_ID}'
  - '--expiration=0'
  - &gt;- # folded block scalar; newlines are folded into spaces
    --view= 
    REGEXP_EXTRACT(FIELD1,  r&quot;[\d]*&quot;) as F1,
    REGEXP_REPLACE(FIELD2, r&quot;\'&quot;, &quot;&quot;) AS F2,
    REGEXP_EXTRACT(FIELD3,  r&quot;\[(\d{3,12}).*\]&quot;) AS F3
    FROM `project.mydataset.mytable`&quot; 
    '${_TARGET_DATASET}.${_TARGET_VIEW}'
  - dummy value to show that the scalar ends here
</code></pre>
<p>A folded block scalar is started with <code>&gt;</code>, the following minus tells YAML to not append the final newline to its value.</p>"
"How to deploy an Angular8 application to google app engine using CI-CD process?<p>I am trying to deploy my UI application to <code>google app engine</code> using the <code>CI-CD</code> process. This is totally new to me. Below are the steps that I followed:</p>

<ol>
<li>Mirroed my bitbucket repository.</li>
<li>Created <code>cloudbuild.yaml</code> and <code>app.yaml</code> files.</li>
<li>Created a cloud build trigger.</li>
</ol>

<p>Here is my cloudbuild.yaml file:</p>

<pre><code>steps:
# Install npm
- name: 'node:10.10.0'
  args: ['npm', 'install']
  dir: './UI'
# Build productive file
- name: 'node:10.10.0'
  args: ['npm', 'run', 'build', '--prod']
  dir: './UI'
# Deploy UI to CP-D
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['app', 'deploy', './']
  dir: './UI'
</code></pre>

<p>app.yaml:</p>

<pre><code>runtime: python27
threadsafe: true

handlers:
- url:  /(.*\.js)
  mime_type: text/javascript
  static_files: EPortal/\1
  upload: EPortal/(.*\.js)

- url:  /favicon.ico
  static_files: EPortal/favicon.ico
  upload: EPortal/assets/favicon.ico

- url:  /(.*\.(gif|png|jpg|css|js|json)(|\.map))$
  static_files: EPortal/\1
  upload: EPortal/(.*)(|\.map)

- url:  /(.*\.svg)
  static_files: EPortal/\1
  upload: EPortal/(.*\.svg)
  mime_type: image/svg+xml

- url:  /.*
  secure: always
  redirect_http_response_code: 301
  static_files: EPortal/index.html
  upload: EPortal/index\.html
  http_headers:
    Strict-Transport-Security: max-age=31536000; includeSubDomains
    X-Frame-Options: DENY
</code></pre>

<p>Below are the logs generated. Pasting only few logs due to security reasons:</p>

<pre><code>Step #2: Do you want to continue (Y/n)?  
Step #2: Beginning deployment of service [default]...
Step #2: ERROR: (gcloud.app.deploy) Cannot upload file [/workspace/UI/node_modules/canvas/build/Release/librsvg-2.so.2], which has size [47123185] (greater than maximum allowed size of [33554432]). Please delete the file or add to the skip_files entry in your application .yaml file and try again.
Finished Step #2
ERROR
ERROR: build step 2 ""gcr.io/cloud-builders/gcloud"" failed: step exited with non-zero status: 1
</code></pre>

<p>Everything is working fine except my <code>deployment</code>. Not deploying due to <code>size</code> issue. It would be of a great help if you could help me fix this.</p>

<p>Thank you.</p>","<p>Do you need <code>node_modules</code> in the Python application? It looks like that is just for building a static site. You would want to ignore <code>node_modules</code> in this case either:</p>

<ul>
<li>Using <code>skip_files</code> entry in your application .yaml file just like the error suggests. See <a href=""https://cloud.google.com/appengine/docs/standard/python/config/appref#skip_files"" rel=""nofollow noreferrer"">https://cloud.google.com/appengine/docs/standard/python/config/appref#skip_files</a> for information on it.</li>
<li>Adding <code>node_modules</code> to an ignore file. Information on this at <a href=""https://cloud.google.com/sdk/gcloud/reference/topic/gcloudignore"" rel=""nofollow noreferrer"">https://cloud.google.com/sdk/gcloud/reference/topic/gcloudignore</a></li>
</ul>

<p>Given your current application configuration it might look like:</p>

<pre><code>runtime: python27
threadsafe: true
skip_files:
  - node_modules/

handlers:
- url:  /(.*\.js)
  mime_type: text/javascript
  static_files: EPortal/\1
  upload: EPortal/(.*\.js)

- url:  /favicon.ico
  static_files: EPortal/favicon.ico
  upload: EPortal/assets/favicon.ico

- url:  /(.*\.(gif|png|jpg|css|js|json)(|\.map))$
  static_files: EPortal/\1
  upload: EPortal/(.*)(|\.map)

- url:  /(.*\.svg)
  static_files: EPortal/\1
  upload: EPortal/(.*\.svg)
  mime_type: image/svg+xml

- url:  /.*
  secure: always
  redirect_http_response_code: 301
  static_files: EPortal/index.html
  upload: EPortal/index\.html
  http_headers:
    Strict-Transport-Security: max-age=31536000; includeSubDomains
    X-Frame-Options: DENY
</code></pre>"
"How to google oauth to an api? My example is not working<p>I am trying to do this article for google cloud build</p>

<pre><code>https://cloud.google.com/endpoints/docs/openapi/service-account-authentication
</code></pre>

<p>I am guessing to use the service account email I generated the key from in that example AND for Audient, I put """" (which is probably the reason it's not working?).  I have no idea and can't find what in the world to put for audience.  </p>

<p><strong>In addition to code below, I tried setting audience to '<a href=""https://cloudbuild.googleapis.com"" rel=""nofollow noreferrer"">https://cloudbuild.googleapis.com</a>' which also did not work</strong></p>

<p>My code is the following...</p>

<pre><code>public class GenToken {
    public static void main(String[] args) throws IOException {
        Duration d = Duration.ofDays(365);
        String tok = generateJwt(""/Users/dean/workspace/order/java/googleBuild/orderly-gcp-key.json"",
                ""mycloudbuilder@order-gcp.iam.gserviceaccount.com"", """", d.toSeconds());

        System.out.println(""tok=""+tok);

        URL url = new URL(""https://cloudbuild.googleapis.com/v1/projects/order-gcp/builds"");
        makeJwtRequest(tok, ""GET"", url);

    }

    public static String generateJwt(final String saKeyfile, final String saEmail,
                                     final String audience, final long expiryLength)
            throws FileNotFoundException, IOException {

        Date now = new Date();
        Date expTime = new Date(System.currentTimeMillis() + TimeUnit.SECONDS.toMillis(expiryLength));

        // Build the JWT payload
        JWTCreator.Builder token = JWT.create()
                .withIssuedAt(now)
                // Expires after 'expiraryLength' seconds
                .withExpiresAt(expTime)
                // Must match 'issuer' in the security configuration in your
                // swagger spec (e.g. service account email)
                .withIssuer(saEmail)
                // Must be either your Endpoints service name, or match the value
                // specified as the 'x-google-audience' in the OpenAPI document
                .withAudience(audience)
                // Subject and email should match the service account's email
                .withSubject(saEmail)
                .withClaim(""email"", saEmail);

        // Sign the JWT with a service account
        FileInputStream stream = new FileInputStream(saKeyfile);
        ServiceAccountCredentials cred = ServiceAccountCredentials.fromStream(stream);
        RSAPrivateKey key = (RSAPrivateKey) cred.getPrivateKey();
        Algorithm algorithm = Algorithm.RSA256(null, key);
        return token.sign(algorithm);
    }

    /**
     * Makes an authorized request to the endpoint.
     */
    public static String makeJwtRequest(final String signedJwt, String method, final URL url)
            throws IOException, ProtocolException {

        HttpURLConnection con = (HttpURLConnection) url.openConnection();
        con.setRequestMethod(method);
        con.setRequestProperty(""Content-Type"", ""application/json"");
        con.setRequestProperty(""Authorization"", ""Bearer "" + signedJwt);

        InputStreamReader reader = new InputStreamReader(con.getInputStream());
        BufferedReader buffReader = new BufferedReader(reader);

        String line;
        StringBuilder result = new StringBuilder();
        while ((line = buffReader.readLine()) != null) {
            result.append(line);
        }
        buffReader.close();
        return result.toString();
    }
}
</code></pre>

<p>The orderly-gcp-key.json has these attributes in it</p>

<pre><code>{
    ""type"": ""service_account"",
    ""project_id"": ""myproj"",
    ""private_key_id"": ""xxxxxxxx"",
    ""private_key"": ""-----BEGIN PRIVATE KEY-----\nasdfsd\n-----END PRIVATE KEY-----\n"",
    ""client_email"": ""build-ci-mine@myproj.iam.gserviceaccount.com"",
    ""client_id"": ""1167333552"",
    ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",
    ""token_uri"": ""https://oauth2.googleapis.com/token"",
    ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",
    ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/build-ci-mine%40myproj.iam.gserviceaccount.com""
}
</code></pre>

<p>oops, my edit didn't get posted :(.  Here is the error</p>

<pre><code>Exception in thread ""main"" java.io.IOException: Server returned HTTP response code: 401 for URL: https://cloudbuild.googleapis.com/v1/projects/orderly-gcp/builds
at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1919)
at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1515)
at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:250)
at com.orderlyhealth.auth.websecure.GenToken.makeJwtRequest(GenToken.java:71)
at com.orderlyhealth.auth.websecure.GenToken.main(GenToken.java:26)
</code></pre>","<p>I hope that I better understood!!</p>

<p>When you try to reach a Google API, you have to use an access token. I have 2 code snippets for you.</p>

<p>Use Google Http client</p>

<pre><code>        GoogleCredentials credentials = GoogleCredentials.getApplicationDefault();
        HttpRequestFactory factory = new NetHttpTransport().createRequestFactory(new HttpCredentialsAdapter(credentials));
        HttpRequest request = factory.buildGetRequest(new GenericUrl(""https://cloudbuild.googleapis.com/v1/projects/gbl-imt-homerider-basguillaueb/builds""));
        HttpResponse httpResponse = request.execute();
        System.out.println(CharStreams.toString(new InputStreamReader(httpResponse.getContent(), Charsets.UTF_8)));
</code></pre>

<p>Use pure java connection</p>

<pre><code>        GoogleCredentials credentials = GoogleCredentials.getApplicationDefault();

        HttpURLConnection con = (HttpURLConnection) new URL(""https://cloudbuild.googleapis.com/v1/projects/gbl-imt-homerider-basguillaueb/builds"").openConnection();
        con.setRequestMethod(""GET"");
        con.setRequestProperty(""Content-Type"", ""application/json"");
        con.setRequestProperty(""Authorization"", ""Bearer "" + credentials.refreshAccessToken().getTokenValue());

        InputStreamReader reader = new InputStreamReader(con.getInputStream());
        BufferedReader buffReader = new BufferedReader(reader);

        String line;
        StringBuilder result = new StringBuilder();
        while ((line = buffReader.readLine()) != null) {
            result.append(line);
        }
        buffReader.close();
        System.out.println(result.toString());

</code></pre>

<p>You can rely on the platform environment. In local, perform a <code>gcloud auth application-default login</code> to set your credential as default default credential. On GCP, the component identity (the default service account or the service account that you define when you create the component), is used thanks to the method <code>GoogleCredentials.getApplicationDefault();</code></p>

<p>Your dependency management need this (here in maven)</p>

<pre><code>        &lt;dependency&gt;
            &lt;groupId&gt;com.google.auth&lt;/groupId&gt;
            &lt;artifactId&gt;google-auth-library-oauth2-http&lt;/artifactId&gt;
            &lt;version&gt;0.20.0&lt;/version&gt;
        &lt;/dependency&gt;
</code></pre>

<p>Does this solve your issue?</p>"
"Google Cloud Build - Terraform Self-Destruction on Build Failure<p>I'm currently facing an issue with my Google Cloud Build for CI/CD.</p>
<ul>
<li>First, I build new docker images of multiple microservices and use Terraform to create the GCP infrastructure for the containers that they will also live in production.</li>
<li>Then I perform some Integration / System Tests and if everything is fine I push new versions of the microservice images to the container registry for later deployment.</li>
</ul>
<p>My problem is, that the Terraformed infrastructure doesn't get destroyed if the cloud build fails.</p>
<p>Is there a way to always execute a cloud build step even if some previous steps have failed, here I would want to always execute &quot;terraform destroy&quot;?
Or specifically for Terraform, is there a way to define a self-destructive Terraform environment?</p>
<h3>cloudbuild.yaml example with just one docker container</h3>
<pre><code>steps:

    # build fresh ...
    - id: build 
      name: 'gcr.io/cloud-builders/docker'
      dir: '...'
      args: ['build', '-t', 'gcr.io/$PROJECT_ID/staging/...:latest', '-t', 'gcr.io/$PROJECT_ID/staging/...:$BUILD_ID', '.', '--file', 'production.dockerfile']

    # push
    - id: push
      name: 'gcr.io/cloud-builders/docker'
      dir: '...'
      args: ['push', 'gcr.io/$PROJECT_ID/staging/...']
      waitFor: [build]

    # setup terraform
    - id: terraform-init
      name: 'hashicorp/terraform:0.12.28'
      dir: '...'
      args: ['init']
      waitFor: [push]

    # deploy GCP resources
    - id: terraform-apply 
      name: 'hashicorp/terraform:0.12.28'
      dir: '...'
      args: ['apply', '-auto-approve']
      waitFor: [terraform-init]

    # tests
    - id: tests
      name: 'python:3.7-slim'
      dir: '...'
      waitFor: [terraform-apply]
      entrypoint: /bin/sh
      args:
       - -c
       - 'pip install -r requirements.txt &amp;&amp; pytest ... --tfstate terraform.tfstate'

    # remove GCP resources
    - id: terraform-destroy
      name: 'hashicorp/terraform:0.12.28'
      dir: '...'
      args: ['destroy', '-auto-approve']
      waitFor: [tests]
</code></pre>","<p>Google Cloud Build doesn't yet support <code>allow_failure</code> or some similar mechanism as mentioned in <a href=""https://github.com/GoogleCloudPlatform/cloud-builders/issues/253"" rel=""nofollow noreferrer"">this unsolved but closed issue</a>.</p>
<p>What you can do, and as mentioned in the linked issue, is to chain shell conditional operators.</p>
<p>If you want to run a command on failure then you can do something like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>    - id: tests
      name: 'python:3.7-slim'
      dir: '...'
      waitFor: [terraform-apply]
      entrypoint: /bin/sh
      args:
       - -c
       - pip install -r requirements.txt &amp;&amp; pytest ... --tfstate terraform.tfstate || echo &quot;This failed!&quot;
</code></pre>
<p>This would run your test as normal and then echo <code>This failed!</code> to the logs if the tests fail. If you want to run <code>terraform destroy -auto-approve</code> on the failure then you would need to replace the <code>echo &quot;This failed!&quot;</code> with <code>terraform destroy -auto-approve</code>. Of course you will also need the Terraform binaries in the Docker image you are using so will need to use a custom image that has both Python and Terraform in it for that to work.</p>
<pre class=""lang-yaml prettyprint-override""><code>    - id: tests
      name: 'example-customer-python-and-terraform-image:3.7-slim-0.12.28'
      dir: '...'
      waitFor: [terraform-apply]
      entrypoint: /bin/sh
      args:
       - -c
       - pip install -r requirements.txt &amp;&amp; pytest ... --tfstate terraform.tfstate || terraform destroy -auto-approve ; false&quot;
</code></pre>
<p>The above job also runs <code>false</code> at the end of the command so that it will return a non 0 exit code and mark the job as failed still instead of only failing if <code>terraform destroy</code> failed as well.</p>
<p>An alternative to this would be to use something like <a href=""https://kitchen.ci/"" rel=""nofollow noreferrer"">Test Kitchen</a> which will automatically stand up infrastructure, run the necessary verifiers and then destroy it at the end all in a single <code>kitchen test</code> command.</p>
<p>It's probably also worth mentioning that your pipeline is entirely serial so you don't need to use <code>waitFor</code>. This is mentioned in the <a href=""https://cloud.google.com/cloud-build/docs/build-config#build_steps"" rel=""nofollow noreferrer"">Google Cloud Build documentation</a>:</p>
<blockquote>
<p>A build step specifies an action that you want Cloud Build to perform.
For each build step, Cloud Build executes a docker container as an
instance of <code>docker run</code>. Build steps are analogous to commands in a
script and provide you with the flexibility of executing arbitrary
instructions in your build. If you can package a build tool into a
container, Cloud Build can execute it as part of your build. By
default, Cloud Build executes all steps of a build serially on the
same machine. If you have steps that can run concurrently, use the
<a href=""https://cloud.google.com/cloud-build/docs/build-config#waitfor"" rel=""nofollow noreferrer""><code>waitFor</code></a> option.</p>
</blockquote>
<p><a href=""https://cloud.google.com/cloud-build/docs/build-config#waitfor"" rel=""nofollow noreferrer"">and</a></p>
<blockquote>
<p>Use the <code>waitFor</code> field in a build step to specify which steps must run
before the build step is run. If no values are provided for <code>waitFor</code>,
the build step waits for all prior build steps in the build request to
complete successfully before running. For instructions on using
<code>waitFor</code> and <code>id</code>, see Configuring build step order.</p>
</blockquote>"
"Why am I seeing this error: 'ERROR: (gcloud.run.deploy) PERMISSION_DENIED: The caller does not have permission' while deploying container?<p>Assume I have a <code>cloudbuild.yaml</code> file like the one below. Also assume that I can run and deploy the container in question manually when using <code>gcloud</code> for the separate functionalities (building and running).</p>
<p>When deploying, the third step is resulting in the error <code>ERROR: (gcloud.run.deploy) PERMISSION_DENIED: The caller does not have permission</code></p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
# Build the container image
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/[PROJECT_ID]/[IMAGE]:$COMMIT_SHA', '.']
# Push the image to Container Registry
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/[PROJECT_ID]/[IMAGE]:$COMMIT_SHA']
# Deploy image to Cloud Run
- name: 'gcr.io/cloud-builders/gcloud'
  args:
  - 'run'
  - 'deploy'
  - '[SERVICE_NAME]'
  - '--image'
  - 'gcr.io/[PROJECT_ID]/[IMAGE]:$COMMIT_SHA'
  - '--region'
  - '[REGION]'
  - '--platform'
  - 'managed'
images:
- gcr.io/[PROJECT_ID]/[IMAGE]

</code></pre>","<h3>See the docs at:</h3>
<p><a href=""https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-cloud-run#before_you_begin"" rel=""noreferrer"">https://cloud.google.com/cloud-build/docs/deploying-builds/deploy-cloud-run#before_you_begin</a></p>
<br>
<h3>You need to follow the steps available there:</h3>
<blockquote>
<ol>
<li><p>Grant the Cloud Run Admin role to the Cloud Build service account:</p>
<ul>
<li><p>In the Cloud Console, go to the Cloud Build Settings page:</p>
</li>
<li><p>Open the Settings page</p>
</li>
<li><p>Locate the row with the Cloud Run Admin role and set its Status to ENABLED.</p>
</li>
<li><p>In the Additional steps may be required pop-up, click Skip.</p>
</li>
</ul>
</li>
<li><p>Grant the IAM Service Account User role to the Cloud Build service account on the Cloud Run runtime service account:</p>
<ul>
<li><p>In the Cloud Console, go to the Service Accounts page:</p>
</li>
<li><p>Open the Service Accounts page</p>
</li>
<li><p>In the list of members, locate and select [PROJECT_NUMBER]-compute@developer.gserviceaccount.com. This is the
Cloud Run runtime service account.</p>
</li>
<li><p>Click SHOW INFO PANEL in the top right corner.</p>
</li>
<li><p>In the Permissions panel, click the Add Member button.</p>
</li>
<li><p>In the New member field, enter the email address of the Cloud Build service account. This is of the form
[PROJECT_NUMBER]@cloudbuild.gserviceaccount.com.
Note: The email address of Cloud Build service account is different from that of Cloud Run runtime service account.</p>
</li>
<li><p>In the Role dropdown, select Service Accounts, and then Service Account User.</p>
</li>
<li><p>Click Save.</p>
</li>
</ul>
</li>
</ol>
</blockquote>
<hr>
<p>In my case, the @cloudbuild account wasn't showing up in the IAM suggestions in step 2, but if you perform step 1, and run your build, the error message will change to something similar to the redacted message below, which contains the account you need.</p>
<pre><code>ERROR: (gcloud.run.deploy) User [&lt;SOME_NUMBER_HERE&gt;@cloudbuild.gserviceaccount.com] does not have permission to access namespace [&lt;YOUR_PROJECT_ID&gt;] (or it may not exist): Permission 'iam.serviceaccounts.actAs' denied on service account &lt;SOME_OTHER_NUMBER_HERE&gt;-compute@developer.gserviceaccount.com (or it may not exist).
</code></pre>"
"cloud-build + gke-deploy app name or label are ignored<p>I have this config on a cloudbuild.yaml file:</p>
<pre><code>- name: 'gcr.io/cloud-builders/gke-deploy'
args:
  - run
  - --app=doc-io
  - --namespace=frontend
  - --cluster=cluster-dev
  - --location=europe-west1-b
  - --image=gcr.io/${PROJECT_ID}/github.com/ourprojet/docs-io:dev-${SHORT_SHA}
  - --version=dev-${SHORT_SHA}
</code></pre>
<p>But this creates a new workload in GKE named &quot;docs-io&quot; instead of deploying my latest docker image to the existing workload &quot;doc-io&quot;.</p>
<p>No matter what I do on the cloud-build side, even with adding the env variables <code>_K8S_APP_NAME</code> or <code>_K8S_LABELS</code> directly in cloud-build config - it creates a new workload on GKE named <code>docs-io</code>.</p>
<p>I haven't been able to find anywhere what is the default &quot;workloads&quot; name taken by cloud-build or gke-deploy nor how to override it.</p>
<p>Has anyone encountered this issue? Any clue how to indicate where the docker deployment has to happen?</p>","<p>Alright, it's easy to miss from the documentation but what one has to do is to keep the yaml file generated at the creation of a workload (easy to recreate otherwise).</p>
<p>Otherwise, as mentioned by Gari Singh as a comment of my question, a new workload will be generated (or replacing the previous one).</p>
<p>Thus, in the workload config file, no need for any specific tag (image level) and simply put it in a Google Storage folder.
Then:</p>
<pre><code># GKE run deploy
- name: 'gcr.io/cloud-builders/gke-deploy'
args:
  - run
  - --filename=gs://path/to/config/myserviceconfig.yml
  - ...
</code></pre>
<p>And this way you get a clean deployment on your GKE cluster.</p>
<p>The doc is a bit miss leading as to what happen without the filename argument, hope it helps.</p>"
"When deploying a service on Cloud Run, the latest build is not being used<p>I'm using this command to submit the build:</p>
<pre><code>gcloud builds submit --tag gcr.io/[my-project]/[my-service]
</code></pre>
<p>This successfully submits the build and everything is correct in the Cloud Build tab in the GCP interface.</p>
<p>And this command to deploy:</p>
<pre><code>gcloud beta run deploy [my-service]
--service-account [service-account]@[my-project].iam.gserviceaccount.com
--image gcr.io/[my-project]/[my-service]:latest --cpu 2 --memory 8Gi
--timeout 59m59s --vpc-connector=pyston-vpc-connector
--vpc-egress=private-ranges-only 
--set-cloudsql-instances=[my-project]:europe-west2:[my-instance]
</code></pre>
<p>This does 'successfully' deploy the service, as in there are no errors. However, it is not deploying using the latest build, it uses the last build submitted by someone else in my team.</p>
<p>I have tried deploying in the GCP interface as well but this led to the same outcome.</p>
<p>Any ideas on how t solve this?</p>
<p>Thanks!</p>",<p>I was specifying the incorrect Container Registry Repository when submitting the build meaning the deployment couldn't find the latest image.</p>
"Why does the Google Cloud Build configuration file not use an entrypoint for docker build step(s)?<p>In general I see cloud build config examples like the one below. Why does the cloud-sdk build step use an entrypoint when the docker build steps do not?</p>
<pre><code>steps:
# Build the container image
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '--tag', 'gcr.io/project-id/project-name','.']
# Push the container image to Container Registry
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/project-id/project-name']
# Deploy container image to Cloud Run
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: 'gcloud'
  args: ['run', 'deploy', 'project-name', '--image', 'gcr.io/project-id/project-name', '--region', 'us-central1']
</code></pre>
<p>My running theory is that it has to do with the last word of the <code>name</code> being the same as the <code>cmd</code> keyword arg. eg. <code>gcr.io/cloud-builders/docker</code> ends in <code>docker</code> but the <code>cloud-sdk</code> requires the <code>cmd</code> keyword <code>gcloud</code> to run.</p>","<blockquote>
<p>Why does the cloud-sdk build step use an entrypoint when the docker build steps do not?</p>
</blockquote>
<p>This is because entrypoint <code>gcloud</code> will be used to invoke the <code>gcloud run deploy</code> command to deploy it in Cloud Run.</p>
<p>On the other hand, Dockerfile has its own configuration for being open source so that it would work on other platforms aside from gcloud.</p>
<p>Additional information regarding <code>args</code> and <code>entrypoint</code> is available in this link on <a href=""https://cloud.google.com/build/docs/build-config-file-schema#args"" rel=""nofollow noreferrer"">Structure of a build config file</a>.</p>"
"GCP artifact registry cannot find a version that satisfies the requirement when using artifact registry in different GCP project<p>I am trying to add a custom package to a cloud function on GCP,
I have the the right permission, and can import packages when I follow <a href=""https://cloud.google.com/artifact-registry/docs/python/store-python"" rel=""nofollow noreferrer"">this tutorial</a> in the same GCP project.</p>
<p>I cannot add artifact from an external GCP project, even if I give manually the role: <strong>Artifact Registry Reader</strong> to the service account deploying the cloud function.</p>
<p>I also posted an issue on the
<a href=""https://issuetracker.google.com/issues/231345778"" rel=""nofollow noreferrer"">google cloud platform issue tracker</a></p>","<p>I updated the Google issue tracker with my findings while investigating and answering  <a href=""https://stackoverflow.com/q/67490412/9267296"">this question</a></p>
<p>I'll put a short summary from that answer below, but see my answer there for more details.</p>
<h2>Summary</h2>
<p>So, to summarize, the first authentication to the repo is done with whatever SA you use.<br />
Stupidly enough, the download itself is done with the inbuilt SA for Cloud Build from the project you are deploying the Cloud Function to. IMHO this should be done by the same SA as the first.</p>
<p>Note that the format for the inbuilt SA for Cloud Build is <code>&lt;PROJECT-NUMBER&gt;@cloudbuild.gserviceaccount.com</code></p>"
"ERROR: (gcloud.builds.submit) INVALID_ARGUMENT: could not parse service account URL<p>I want to use a custom Service Account to build my docker container with Cloud Build. Using <code>gcloud iam service-accounts list</code> and <code>gcloud config list</code> I have confirmed that the service account is in the project environment and I used <code>gcloud services list --enabled</code> to check that <code>cloudbuild.googleapis.com</code> is enabled. I get the error: <code>ERROR: (gcloud.builds.submit) INVALID_ARGUMENT: could not parse service account URL</code>. I tried all of the available service accounts and I tried with and without the prefix path. What is the correct URL or config after steps to get the service account working?</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/my-project-id/my-app']
images: ['gcr.io/my-project-id/my-app']
serviceAccount: 'projects/my-project-id/serviceAccount/my-sa@my-project-id.iam.gserviceaccount.com'
options:
  logging: CLOUD_LOGGING_ONLY
</code></pre>","<p>The build config for <a href=""https://cloud.google.com/build/docs/build-config-file-schema#serviceaccount"" rel=""nofollow noreferrer"">serviceAccount</a> references <a href=""https://cloud.google.com/build/docs/securing-builds/configure-user-specified-service-accounts#running_builds_from_the_command_line"" rel=""nofollow noreferrer"">this</a> page and there's an example that shows the structure:</p>
<pre><code>projects/{project-id}/serviceAccounts/{service-account-email}
</code></pre>
<p>So, it follows Google's API convention of a plural noun (i.e. <code>serviceAccounts</code>) followed by the unique identifier.</p>
<p>Another way to confirm this is via <a href=""https://developers.google.com/apis-explorer"" rel=""nofollow noreferrer"">APIs Explorer</a> for <a href=""https://cloud.google.com/build/docs/api/reference/rest"" rel=""nofollow noreferrer"">Cloud Build</a>.</p>
<p>The service's <a href=""https://cloud.google.com/build/docs/api/reference/rest/v1/projects.builds#Build"" rel=""nofollow noreferrer"">Build</a> resource defines <a href=""https://cloud.google.com/build/docs/api/reference/rest/v1/projects.builds#Build.FIELDS.service_account"" rel=""nofollow noreferrer"">serviceAccount</a> too.</p>"
"Any practical difference between using `docker build` & `docker push` together vs. `cloud build submit` in cloud build config files?<p><a href=""https://cloud.google.com/build/docs/building/build-containers"" rel=""nofollow noreferrer"">Google Cloud docs</a> use the first code block but I'm wondering why they don't use the second one. As far as I can tell they achieve the same result. Is there any practical difference?</p>
<pre><code># config 1
steps:
# Build the container image
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/project-id/project-name','.']
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/project-id/project-name']
# Deploy container image to Cloud Run
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['run', 'deploy', 'project-name', '--image', 'gcr.io/project-id/project-name', '--region', 'us-central1']
images: ['gcr.io/project-id/project-name']
</code></pre>
<pre><code># config 2
steps:
# Build the container image
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['builds', 'submit', '--region', 'us-central1', '--tag', 'gcr.io/project-id/project-name','.']
# Deploy container image to Cloud Run
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['run', 'deploy', 'project-name', '--image', 'gcr.io/project-id/project-name', '--region', 'us-central1']
</code></pre>
<p>I run this with <code>gcloud builds submit --config cloudbuild.yaml</code></p>","<p>In the second config, you call a Cloud Build from inside a Cloud Build, that means you pay twice the docker build/push process in the second config.</p>
<p>That time of timeline in fact</p>
<ul>
<li>Cloud Build 1
<ul>
<li>Cloud build 2</li>
<li>Docker Build</li>
<li>Docker Push</li>
</ul>
</li>
<li>Deploy on cloud run</li>
</ul>
<p>In addition, the number of concurrent build are limited and with the config 2 you use 2 times more quotas.</p>
<p>And the result is the same (it should be slightly faster with the config 1 because you haven't a new Cloud Build to spin up)</p>"
"Cannot install private dependency from artifact registry inside docker build<p>I am trying to install a private python package that was uploaded to an artifact registry inside a docker container (to deploy it on cloudrun).</p>
<p>I have sucessfully used that package in a cloud function in the past, so I am sure the package works.</p>
<p><strong>cloudbuild.yaml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  args: [ 'build', '-t', 'gcr.io/${_PROJECT}/${_SERVICE_NAME}:$SHORT_SHA', '--network=cloudbuild', '.', '--progress=plain']
</code></pre>
<p><strong>Dockerfile</strong></p>
<pre><code>FROM python:3.8.6-slim-buster

ENV APP_PATH=/usr/src/app
ENV PORT=8080

# Copy requirements.txt to the docker image and install packages
RUN apt-get update &amp;&amp; apt-get install -y cython 

RUN pip install --upgrade pip

# Set the WORKDIR to be the folder
RUN mkdir -p $APP_PATH

COPY / $APP_PATH

WORKDIR $APP_PATH

RUN pip install -r requirements.txt --no-color
RUN pip install --extra-index-url https://us-west1-python.pkg.dev/my-project/my-package/simple/ my-package==0.2.3 # This line is where the bug occurs


# Expose port 
EXPOSE $PORT

# Use gunicorn as the entrypoint
CMD exec gunicorn --bind 0.0.0.0:8080 app:app
</code></pre>
<p>The permissions I added are:</p>
<ul>
<li><strong>cloudbuild default service account</strong> (project-number@cloudbuild.gserviceaccount.com): <strong>Artifact Registry Reader</strong></li>
<li><strong>service account running the cloudbuild</strong> : <strong>Artifact Registry Reader</strong></li>
<li><strong>service account running the app</strong>: <strong>Artifact Registry Reader</strong></li>
</ul>
<p><strong>The cloudbuild error:</strong></p>
<pre class=""lang-sh prettyprint-override""><code>Step 10/12 : RUN pip install --extra-index-url https://us-west1-python.pkg.dev/my-project/my-package/simple/ my-package==0.2.3
---&gt; Running in b2ead00ccdf4
Looking in indexes: https://pypi.org/simple, https://us-west1-python.pkg.dev/muse-speech-devops/gcp-utils/simple/
User for us-west1-python.pkg.dev: [91mERROR: Exception:
Traceback (most recent call last):
File &quot;/usr/local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py&quot;, line 167, in exc_logging_wrapper
status = run_func(*args)
File &quot;/usr/local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py&quot;, line 205, in wrapper
return func(self, options, args)
File &quot;/usr/local/lib/python3.8/site-packages/pip/_internal/commands/install.py&quot;, line 340, in run
requirement_set = resolver.resolve(
File &quot;/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/resolvelib/resolver.py&quot;, line 94, in resolve
result = self._result = resolver.resolve(
File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py&quot;, line 481, in resolve
state = resolution.resolve(requirements, max_rounds=max_rounds)
File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py&quot;, line 348, in resolve
self._add_to_criteria(self.state.criteria, r, parent=None)
File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/resolvelib/resolvers.py&quot;, line 172, in _add_to_criteria
if not criterion.candidates:
File &quot;/usr/local/lib/python3.8/site-packages/pip/_vendor/resolvelib/structs.py&quot;, line 151, in __bool__
</code></pre>","<p>From your traceback log, we can see that Cloud Build doesn't have the credentials to authenticate to the private repo:</p>
<pre><code>Step 10/12 : RUN pip install --extra-index-url https://us-west1-python.pkg.dev/my-project/my-package/simple/ my-package==0.2.3
---&gt; Running in b2ead00ccdf4
Looking in indexes: https://pypi.org/simple, https://us-west1-python.pkg.dev/muse-speech-devops/gcp-utils/simple/
User for us-west1-python.pkg.dev: [91mERROR: Exception: //&lt;-ASKING FOR USERNAME
</code></pre>
<p>I uploaded a simple package to a private Artifact Registry repo to test this out when building a container and also received the same message. Since you seem to be authenticating with a <a href=""https://cloud.google.com/artifact-registry/docs/python/authentication#sa-key"" rel=""nofollow noreferrer"">service account key</a>, the username and password will need to be stored inside <code>pip.conf</code>:</p>
<p><strong>pip.conf</strong></p>
<pre><code>[global]
extra-index-url = https://_json_key_base64:KEY@LOCATION-python.pkg.dev/PROJECT/REPOSITORY/simple/
</code></pre>
<p>This file therefore needs to be available during the build process. <a href=""https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds"" rel=""nofollow noreferrer"">Multi-stage</a> docker builds are very useful here to ensure the configuration keys are not exposed, since we can choose what files make it into the final image (configuration keys would only be present while used to download the packages from the private repo):</p>
<p><strong>Sample Dockerfile</strong></p>
<pre class=""lang-dockerfile prettyprint-override""><code># Installing packages in a separate image
FROM python:3.8.6-slim-buster as pkg-build

# Target Python environment variable to bind to pip.conf
ENV PIP_CONFIG_FILE /pip.conf

WORKDIR /packages/
COPY requirements.txt /

# Copying the pip.conf key file only during package downloading
COPY ./config/pip.conf /pip.conf

# Packages are downloaded to the /packages/ directory
RUN pip download -r /requirements.txt
RUN pip download --extra-index-url https://LOCATION-python.pkg.dev/PROJECT/REPO/simple/ PACKAGES

# Final image that will be deployed
FROM python:3.8.6-slim-buster

ENV PYTHONUNBUFFERED True
ENV APP_HOME /app

WORKDIR /packages/
# Copying ONLY the packages from the previous build
COPY --from=pkg-build /packages/ /packages/

# Installing the packages from the copied files
RUN pip install --no-index --find-links=/packages/ /packages/*

WORKDIR $APP_HOME
COPY ./src/main.py ./

# Executing sample flask web app 
CMD exec gunicorn --bind :$PORT --workers 1 --threads 8 --timeout 0 main:app
</code></pre>
<p>I based the dockerfile above on this <a href=""https://stackoverflow.com/questions/69704683/"">related thread</a>, and I could confirm the packages were correctly downloaded from my private Artifact Registry repo, and also that the <code>pip.conf</code> file was not present in the resulting image.</p>"
"Google Cloud Build Error: build step 0 ""gcr.io/cloud-builders/docker"" failed: step exited with non-zero status: 1<p>Note: There is a similar <a href=""https://stackoverflow.com/questions/61312014/google-cloud-build-trigger-failing-with-error-build-step-0-gcr-io-cloud-build"">post</a> regarding this issue but it involves a CI/CD workflow and a considerably more complicated Dockerfile. The solutions presented do not seem to apply to my situation.</p>
<p>Per <a href=""https://cloud.google.com/run/docs/quickstarts/build-and-deploy/deploy-python-service"" rel=""nofollow noreferrer"">Google documentation</a> I am attempting to build an image by running <code>gcloud run deploy</code> in the directory where the files mentioned in my Dockerfile are located. The Dockerfile appears as:</p>
<pre><code>FROM python:3.9-alpine
WORKDIR /app
COPY main.py /app/main.py
COPY requirements.txt /tmp/requirements.txt
RUN pip3 install -r /tmp/requirements.txt
CMD [&quot;python3&quot;, &quot;main.py&quot;]
</code></pre>
<p>I receive a message that the build failed, and when checking the logs I see the following:</p>
<pre><code>starting build &quot;...&quot;

FETCHSOURCE
Fetching storage object: gs://my-app_cloudbuild/source/....
Copying gs://my-app_cloudbuild/source/...
/ [0 files][    0.0 B/  1.5 KiB]                                                
/ [1 files][  1.5 KiB/  1.5 KiB]                                                
Operation completed over 1 objects/1.5 KiB.                                      
BUILD
Already have image (with digest): gcr.io/cloud-builders/docker
unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /workspace/Dockerfile: no such file or directory
ERROR
ERROR: build step 0 &quot;gcr.io/cloud-builders/docker&quot; failed: step exited with non-zero status: 1
</code></pre>
<p>Can anyone explain the reason for this error? I suspect it has to do with how files are copied to the image, but I was able to build and run this container without problem on my local machine. Any idea why this fails in Cloud Run Build?</p>
<p>Running <code>ls -la</code> in the directory where I ran <code>gcloud run deploy</code> returns:</p>
<pre><code>drwxr-xr-x   9 user  staff       288 May 20 16:04 .
drwxr-xr-x   6 user  staff       192 May 20 13:35 ..
drwxr-xr-x  14 user  staff       448 May 20 16:06 .git
-rw-r--r--   1 user  staff        27 May 20 15:06 .gitignore
-rw-r--r--   1 user  staff       424 May 20 16:54 Dockerfile
-rw-r--r--   1 user  staff      3041 May 20 15:55 main.py
-rw-r--r--   1 user  staff       144 May 19 09:42 requirements.txt
drwxr-xr-x   6 user  staff       192 May 19 09:09 venv
</code></pre>
<p>Contents of <code>.gitignore</code>:</p>
<pre><code>Dockerfile
venv
*.gz
*.tar
*.pem
</code></pre>
<p>Full console output when attempting two-step build (see comments):</p>
<pre><code>user@users-MacBook-Pro TwitterBotAQI % gcloud builds submit  --tag gcr.io/missoula-aqi/aqi
Creating temporary tarball archive of 2 file(s) totalling 3.1 KiB before compression.
Some files were not included in the source upload.

Check the gcloud log [/Users/user/.config/gcloud/logs/2022.05.20/18.40.53.921436.log] to see which files and the contents of the
default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn
more).

Uploading tarball of [.] to [gs://missoula-aqi_cloudbuild/source/1653093653.998995-48d4ba15b3274455a21e16b7abc7d65b.tgz]
Created [https://cloudbuild.googleapis.com/v1/projects/missoula-aqi/locations/global/builds/0c22d976-171e-4e7b-92d8-ec91704d6d52].
Logs are available at [https://console.cloud.google.com/cloud-build/builds/0c22d976-171e-4e7b-92d8-ec91704d6d52?project=468471228522].
------------------------------------------------------------------------------------ REMOTE BUILD OUTPUT -------------------------------------------------------------------------------------
starting build &quot;0c22d976-171e-4e7b-92d8-ec91704d6d52&quot;

FETCHSOURCE
Fetching storage object: gs://missoula-aqi_cloudbuild/source/1653093653.998995-48d4ba15b3274455a21e16b7abc7d65b.tgz#1653093655000531
Copying gs://missoula-aqi_cloudbuild/source/1653093653.998995-48d4ba15b3274455a21e16b7abc7d65b.tgz#1653093655000531...
/ [1 files][  1.5 KiB/  1.5 KiB]                                                
Operation completed over 1 objects/1.5 KiB.
BUILD
Already have image (with digest): gcr.io/cloud-builders/docker
unable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /workspace/Dockerfile: no such file or directory
ERROR
ERROR: build step 0 &quot;gcr.io/cloud-builders/docker&quot; failed: step exited with non-zero status: 1
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

BUILD FAILURE: Build step failure: build step 0 &quot;gcr.io/cloud-builders/docker&quot; failed: step exited with non-zero status: 1
ERROR: (gcloud.builds.submit) build 0c22d976-171e-4e7b-92d8-ec91704d6d52 completed with status &quot;FAILURE&quot;
</code></pre>",<p>I had added <code>Dockerfile</code> to <code>.gitignore</code> as it contained API keys stored as environment variables. Removing <code>Dockerfile</code> from <code>.gitignore</code> resolved the issue.</p>
"How to make Coogle Cloud Build trigger changes from all commits in branch<p>We use Google Cloud Build to run different tests for our Github repository when we push a branch. Our problem is that only the last commit in the branch seems to affect which triggers are run.</p>
<p>E.g. say our repository looks like this:</p>
<pre><code>./client/&lt;client code&gt;
./server/&lt;server code&gt;
</code></pre>
<p>and in our two triggers we specify the <a href=""https://cloud.google.com/build/docs/automating-builds/create-manage-triggers#build_trigger"" rel=""nofollow noreferrer"">Included files filter</a> like this:</p>
<ul>
<li>trigger 1: Included files: <code>client/**</code></li>
<li>trigger 2: Included files: <code>server/**</code></li>
</ul>
<p>the problem is that only the last commit in the branch that should trigger builds affects which triggers to run.</p>
<p>To illustrate the problem: say we push a branch with the following commits:</p>
<ul>
<li>Commit 1: &lt;Changes to client/foo.js&gt;</li>
<li>Commit 2: &lt;Changes to server/bar.scala&gt;</li>
</ul>
<p>only &quot;trigger 2&quot; is run. And what we want is that both triggers are run, since we want to run tests for all the changes introduced by our branch.</p>
<p>Is there any way to get GCB to &quot;see&quot; all the changes in the branch that is pushed, when deciding which triggers to run? The obvious quick-fix is to create branches with single commits, which makes all the triggers run, but is less than ideal from the perspective of our git workflow.</p>","<p>With the trigger type set to &quot;push to a branch&quot;, git is not able to track the changes made to all the branch but only to the last commit as you described.</p>
<p>To accomplish this, you must use Pull Requests to trigger the build. When creating a pull request, git will than be able to track the changes appropriately and your tests will be run as you expected. So change the event that triggers your build to &quot;Pull Request&quot; and than create a pull request from your branch to trigger the build and therefore your tests.</p>"
"How to run docker-compose on google cloud run?<p>I'm new to GCP, and I'm trying to deploy my spring boot web service using docker-compose.
In my <code>docker-compose.yml</code> file, I have 3 services: my app service, mysql service and Cassandra service.
Locally, It works like a charm. I added also a <code>cloudbuild.yaml</code> file :</p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
- name: 'docker/compose:1.28.2'
  args: ['up', '-d']
- name: 'gcr.io/cloud-builders/docker'
  args: ['tag', 'workspace_app:latest', 'gcr.io/$PROJECT_ID/$REPO_NAME:$COMMIT_SHA']
images: ['gcr.io/$PROJECT_ID/$REPO_NAME:$COMMIT_SHA']
</code></pre>
<p>The build on Google cloud build is made with success. But, when I try to run the image on google cloud run, It doesn't call the docker-compose.
How do I must process to use docker-compose on production?</p>","<p>Finally, I deployed my solution with docker-compose on the google virtual machine instance.
First, we must clone our git repository on our virtual machine instance.
Then, on the cloned repository containing of course the docker-compose.yml, the dockerfile and the war file, we executed this command:</p>
<pre class=""lang-sh prettyprint-override""><code>docker run --rm \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v &quot;$PWD:$PWD&quot; \
    -w=&quot;$PWD&quot; \
    docker/compose:1.29.1 up
</code></pre>
<p>And, voila, our solution is working on production with docker-compose</p>"
"Spring batch worker pods getting scheduled only 2 worker nodes<p>I am running Spring Batch application in Kubernetes environment. The k8s cluster have one master and three worker nodes. I am testing spring batch under high load, which is spawning around 100 worker pods. However, all the 100 pods are coming up only on two out of three worker nodes. No node selector or additional labeling has been done on the nodes.</p>
<p>I have used Spring cloud deployer Kubernetes to create worker pods in Kubernetes.</p>
<p>The versions involved are:</p>
<ul>
<li>Spring Boot: 2.1.9.RELEASE</li>
<li>Spring Cloud: 2020.0.1</li>
<li>Spring Cloud Deployer: 2.5.0</li>
<li>Spring Cloud Task: 2.1.1.RELEASE</li>
<li>Kubernetes: 1.21</li>
</ul>
<p>How can I ensure that worker pods get scheduled on all available worker nodes evenly?</p>
<p>Following is the partition handler implementation responsible for launching the tasks.</p>
<pre><code>@Bean
public PartitionHandler partitionHandler(TaskLauncher taskLauncher, JobExplorer jobExplorer) {

    Resource resource = this.resourceLoader.getResource(resourceSpec);

    DeployerPartitionHandler partitionHandler = new DeployerPartitionHandler(taskLauncher, jobExplorer, resource,
        &quot;worker&quot;);

    commandLineArgs.add(&quot;--spring.profiles.active=worker&quot;);
    commandLineArgs.add(&quot;--spring.cloud.task.initialize.enable=false&quot;);
    commandLineArgs.add(&quot;--spring.batch.initializer.enabled=false&quot;);
    commandLineArgs.add(&quot;--spring.cloud.task.closecontext_enabled=true&quot;);
    commandLineArgs.add(&quot;--logging.level.root=DEBUG&quot;);

    partitionHandler.setCommandLineArgsProvider(new PassThroughCommandLineArgsProvider(commandLineArgs));
    partitionHandler.setEnvironmentVariablesProvider(environmentVariablesProvider());
    partitionHandler.setApplicationName(appName + &quot;worker&quot;);
    partitionHandler.setMaxWorkers(maxWorkers);

    return partitionHandler;
}

@Bean
public EnvironmentVariablesProvider environmentVariablesProvider() {
    return new SimpleEnvironmentVariablesProvider(this.environment);
}
</code></pre>","<p>Posting this out of comments as a community wiki for better visibility, feel free to edit and expand.</p>
<hr />
<p>There are scheduling mechanics which can prevent scheduling pods on some nodes:</p>
<ul>
<li><p><a href=""https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/"" rel=""nofollow noreferrer"">Taints and tolerations</a></p>
</li>
<li><p><a href=""https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/"" rel=""nofollow noreferrer"">Node selector</a></p>
</li>
</ul>
<p>If nothing is set, it's worth trying to rejoin the node. For instance it might not be registered correctly (this solved the issue above).</p>"
"""Cloud Run error: Container failed to start. Failed to start and then listen on the port defined by the PORT environment variable.""<p>I'm trying to build a container image that I will later use to update the code inside of a virtual machine. The docker image works fine as I can build and run it inside of my terminal. However, I keep getting an error when I try to deploy it to cloud run: &quot;Cloud Run error: Container failed to start. Failed to start and then listen on the port defined by the PORT environment variable.&quot; How can I fix this error?</p>
<p>The build log contains this:</p>
<pre><code>Deploying container to Cloud Run service [SERVICE] in project [PROJECT_ID] region [REGION]
Deploying...
Creating Revision.......................................................................................................................................................................failed
Deployment failed
ERROR: (gcloud.run.deploy) Cloud Run error: Container failed to start. Failed to start and then listen on the port defined by the PORT environment variable. Logs for this revision might contain more information.
</code></pre>
<p>The revision log contains this:</p>
<pre><code>{
  &quot;protoPayload&quot;: {
    &quot;@type&quot;: &quot;type.googleapis.com/google.cloud.audit.AuditLog&quot;,
    &quot;status&quot;: {
      &quot;code&quot;: 9,
      &quot;message&quot;: &quot;Ready condition status changed to False for Revision {REVISION_NAME} with message: Cloud Run error: Container failed to start. Failed to start and then listen on the port defined by the PORT environment variable. Logs for this revision might contain more information.\n\nLogs URL:{URL_LINK}&quot;
    },
    &quot;serviceName&quot;: &quot;run.googleapis.com&quot;,
    &quot;resourceName&quot;: &quot;{REVISION_NAME}&quot;,
    &quot;response&quot;: {
      &quot;metadata&quot;: {
        &quot;name&quot;: &quot;{REVISION_NAME}&quot;,
        &quot;namespace&quot;: &quot;{NAMESPACE}&quot;,
        &quot;selfLink&quot;: &quot;{SELFLINK}&quot;,
        &quot;uid&quot;: &quot;{UID}&quot;,
        &quot;resourceVersion&quot;: &quot;{RESOURCEVER}&quot;,
        &quot;generation&quot;: 1,
        &quot;creationTimestamp&quot;: &quot;{TIMESTAMP}&quot;,
        &quot;labels&quot;: {
          &quot;serving.knative.dev/route&quot;: &quot;{SERVICE}&quot;,
          &quot;serving.knative.dev/configuration&quot;: &quot;{SERVICE}&quot;,
          &quot;serving.knative.dev/configurationGeneration&quot;: &quot;15&quot;,
          &quot;serving.knative.dev/service&quot;: &quot;{SERVICE}&quot;,
          &quot;serving.knative.dev/serviceUid&quot;: &quot;{SERVICE_UID}&quot;,
          &quot;cloud.googleapis.com/location&quot;: &quot;{REGION}&quot;
        },
        &quot;annotations&quot;: {
          &quot;run.googleapis.com/client-name&quot;: &quot;gcloud&quot;,
          &quot;serving.knative.dev/creator&quot;: &quot;{NAMESPACE}@cloudbuild.gserviceaccount.com&quot;,
          &quot;client.knative.dev/user-image&quot;: &quot;gcr.io/{PROJECT_ID}/{IMAGE}&quot;,
          &quot;run.googleapis.com/client-version&quot;: &quot;357.0.0&quot;,
          &quot;autoscaling.knative.dev/maxScale&quot;: &quot;100&quot;
        },
        &quot;ownerReferences&quot;: [
          {
            &quot;kind&quot;: &quot;Configuration&quot;,
            &quot;name&quot;: &quot;{SERVICE}&quot;,
            &quot;uid&quot;: &quot;{UID}&quot;,
            &quot;apiVersion&quot;: &quot;serving.knative.dev/v1&quot;,
            &quot;controller&quot;: true,
            &quot;blockOwnerDeletion&quot;: true
          }
        ]
      },
      &quot;apiVersion&quot;: &quot;serving.knative.dev/v1&quot;,
      &quot;kind&quot;: &quot;Revision&quot;,
      &quot;spec&quot;: {
        &quot;containerConcurrency&quot;: 80,
        &quot;timeoutSeconds&quot;: 300,
        &quot;serviceAccountName&quot;: &quot;{NAMESPACE}-compute@developer.gserviceaccount.com&quot;,
        &quot;containers&quot;: [
          {
            &quot;image&quot;: &quot;gcr.io/{PROJECT_ID}/{IMAGE}&quot;,
            &quot;ports&quot;: [
              {
                &quot;name&quot;: &quot;h2c&quot;,
                &quot;containerPort&quot;: 8080
              }
            ],
            &quot;resources&quot;: {
              &quot;limits&quot;: {
                &quot;cpu&quot;: &quot;1000m&quot;,
                &quot;memory&quot;: &quot;512Mi&quot;
              }
            }
          }
        ]
      },
      &quot;status&quot;: {
        &quot;observedGeneration&quot;: 1,
        &quot;conditions&quot;: [
          {
            &quot;type&quot;: &quot;Ready&quot;,
            &quot;status&quot;: &quot;False&quot;,
            &quot;reason&quot;: &quot;HealthCheckContainerError&quot;,
            &quot;message&quot;: &quot;Cloud Run error: Container failed to start. Failed to start and then listen on the port defined by the PORT environment variable. Logs for this revision might contain more information.\n\nLogs URL:{LOG_LINK}&quot;,
            &quot;lastTransitionTime&quot;: &quot;{TIME}&quot;
          },
          {
            &quot;type&quot;: &quot;Active&quot;,
            &quot;status&quot;: &quot;Unknown&quot;,
            &quot;reason&quot;: &quot;Reserve&quot;,
            &quot;lastTransitionTime&quot;: &quot;{TIME}&quot;,
            &quot;severity&quot;: &quot;Info&quot;
          },
          {
            &quot;type&quot;: &quot;ContainerHealthy&quot;,
            &quot;status&quot;: &quot;False&quot;,
            &quot;reason&quot;: &quot;HealthCheckContainerError&quot;,
            &quot;message&quot;: &quot;Cloud Run error: Container failed to start. Failed to start and then listen on the port defined by the PORT environment variable. Logs for this revision might contain more information.\n\nLogs URL:{LOG_LINK}&quot;,
            &quot;lastTransitionTime&quot;: &quot;{TIME}&quot;
          },
          {
            &quot;type&quot;: &quot;ResourcesAvailable&quot;,
            &quot;status&quot;: &quot;True&quot;,
            &quot;lastTransitionTime&quot;: &quot;{TIME}&quot;
          },
          {
            &quot;type&quot;: &quot;Retry&quot;,
            &quot;status&quot;: &quot;True&quot;,
            &quot;reason&quot;: &quot;ImmediateRetry&quot;,
            &quot;message&quot;: &quot;System will retry after 0:00:00 from lastTransitionTime for attempt 0.&quot;,
            &quot;lastTransitionTime&quot;: &quot;{TIME}&quot;,
            &quot;severity&quot;: &quot;Info&quot;
          }
        ],
        &quot;logUrl&quot;: &quot;{LOG_LINK}&quot;,
        &quot;imageDigest&quot;: &quot;gcr.io/{PROJECT_ID}/{IMAGE_SHA}&quot;
      },
      &quot;@type&quot;: &quot;type.googleapis.com/google.cloud.run.v1.Revision&quot;
    }
  },
  &quot;insertId&quot;: &quot;{ID}&quot;,
  &quot;resource&quot;: {
    &quot;type&quot;: &quot;cloud_run_revision&quot;,
    &quot;labels&quot;: {
      &quot;location&quot;: &quot;{REGION}&quot;,
      &quot;configuration_name&quot;: &quot;{SERVICE}&quot;,
      &quot;service_name&quot;: &quot;{SERVICE}&quot;,
      &quot;project_id&quot;: &quot;{PROJECT_ID}&quot;,
      &quot;revision_name&quot;: &quot;{REVISION_NAME}&quot;
    }
  },
  &quot;timestamp&quot;: &quot;{TIME}&quot;,
  &quot;severity&quot;: &quot;ERROR&quot;,
  &quot;logName&quot;: &quot;projects/{PROJECT_ID}/logs/cloudaudit.googleapis.com%2Fsystem_event&quot;,
  &quot;receiveTimestamp&quot;: &quot;{TIME}&quot;
}
</code></pre>
<p>This is my cloudbuild.yaml:</p>
<pre><code>steps:
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/PROJECT_ID/IMAGE', '.']
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/PROJECT_ID/IMAGE']
- name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  entrypoint: gcloud
  args: ['run', 'deploy', 'SERVICE-NAME', '--image', 'gcr.io/PROJECT_ID/IMAGE', '--region', 'REGION', '--port', '8080']
images:
- gcr.io/PROJECT_ID/IMAGE
</code></pre>
<p>This is my Dockerfile:</p>
<pre><code>FROM python:3.9.7-slim-buster

WORKDIR /app

COPY . .

CMD [ &quot;python3&quot;, &quot;hello.py&quot; ]
</code></pre>
<p>This is the code in hello.py:</p>
<pre><code>print(&quot;Hello World&quot;)
</code></pre>","<p>When Cloud Run starts your container, a health check is sent to the container. Your container is not responding to the health check. Therefore, Cloud Run determines that your service is failing.</p>
<p>Cloud Run requires that a container provide service/process/program that listens for and responds to HTTP requests.</p>
<p>Your hello.py file only prints a message to stdout. Your program does not start a process to listen for requests.</p>
<p>A very simple example that converts your example into a working program:</p>
<pre><code>import os

from flask import Flask

app = Flask(__name__)

@app.route('/')
def home():
        return &quot;Hello world&quot;

if __name__ == '__main__':
        app.run(debug=True, host='0.0.0.0', port=int(os.environ.get('PORT', 8080)))
</code></pre>
<p>Note: You will need to add a file <strong>requirements.txt</strong> to your build to include Flask. Create <strong>requirements.txt</strong> in the same location as <strong>Dockerfile</strong>.</p>
<p>requirements.txt:</p>
<pre><code>Flask==2.0.1
</code></pre>"
"How can I use a non-exec entrypoint for Kaniko in Google Cloud Build (to enable build arg definition)<p>The <a href=""https://cloud.google.com/blog/products/gcp/introducing-kaniko-build-container-images-in-kubernetes-and-google-container-builder-even-without-root-access"" rel=""nofollow noreferrer"">instructions</a> for using Kaniko in GCB use the <code>exec</code> form of the kaniko project builder, like this:</p>
<pre><code>  - id: 'Build (with Kaniko Cache)'
    name: 'gcr.io/kaniko-project/executor:latest'
    args:
      - --destination=$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME:$SHORT_SHA
      - --cache=true
      - --cache-ttl=6h
</code></pre>
<p>But I'm using it to replace a docker build, in which I circumvent the <code>exec</code> form of usage in order to inject a build arg (an access token from the Secret Manager) as described <a href=""https://stackoverflow.com/questions/65302542/how-do-i-use-google-secrets-manager-to-create-a-docker-arg-in-google-cloud-build"">here</a> and <a href=""https://stackoverflow.com/questions/56732084/google-cloud-build-docker-build-arg-not-respected"">here</a>.</p>
<pre><code>  - id: 'Build'
    name: gcr.io/cloud-builders/docker
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        docker build --cache-from $_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME:$SHORT_SHA --build-arg PERSONAL_ACCESS_TOKEN_GITHUB=$(cat decrypted-pat.txt) -t $_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME:$SHORT_SHA .
</code></pre>
<p>I've tried defining a <code>bash</code> entrypoint but that's not found so I'm stuck. Is it even possible to run the non-exec form?</p>
<p><em>Note: It is possible to access the secret in a file within the container instead of via a build arg, but that would mean changing the setup for my developers to all have that secret file in order to build their development images locally, which I could, but really don't want, to do.</em></p>","<p>I solved it using <code>docker run</code>:</p>
<pre class=""lang-yaml prettyprint-override""><code>- id: Build
  name: gcr.io/cloud-builders/docker
  entrypoint: /bin/bash
  args:
  - -c
  - |
   docker run \
      --network=cloudbuild \
      -v /workspace:/workspace \
        gcr.io/kaniko-project/executor:latest \
          --dockerfile /workspace/Dockerfile \
          --build-arg=GITHUBTOKEN=$$GITHUBTOKEN \
          --destination=gcr.io/$PROJECT_ID/myapp:$SHORT_SHA \
          --cache=true \
          --context dir:///workspace/
  secretEnv: ['GITHUBTOKEN']

availableSecrets:
  secretManager:
    - versionName: projects/$PROJECT_ID/secrets/github_machine_user_pat/versions/latest
      env: GITHUBTOKEN
</code></pre>"
"getting commit of last push/merge in google cloud build<p>I'm using Cloud Build to trigger docker builds whenever someone pushes changes to a branch on cloud source repo. I'm trying to change this behavior to only build the image only when there's a change to the dockerfile.</p>
<p>Doing something like that on github actions would require me to run something like <code>git diff --name-only ${{ github.event.before }}..${{ github.sha }}</code> to get a list of files that were changed between the last push/merge and the latest commit, however there doesn't seem to be something equivalent to <code>{{ github.event.before }}</code> for cloudbuild. I'd be happy to hear your suggestions on how this can be achieved.</p>
<p>update: it appears using the <strong>included files filter</strong> is a viable solution here</p>
<blockquote>
<p>Included files (optional): Changes affecting at least one of these files will invoke a build. You can use glob strings to specify multiple files with wildcard characters. Acceptable wildcard characters include the characters supported by Go Match, **, and alternation[0].</p>
</blockquote>
<p>[0] <a href=""https://cloud.google.com/build/docs/automating-builds/create-manage-triggers"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/automating-builds/create-manage-triggers</a></p>","<p>it appears using the <strong>included files filter</strong> is a viable solution here</p>
<blockquote>
<p>Included files (optional): Changes affecting at least one of these files will invoke a build. You can use glob strings to specify multiple files with wildcard characters. Acceptable wildcard characters include the characters supported by Go Match, **, and alternation[0].</p>
</blockquote>
<p>[0] <a href=""https://cloud.google.com/build/docs/automating-builds/create-manage-triggers"" rel=""nofollow noreferrer"">https://cloud.google.com/build/docs/automating-builds/create-manage-triggers</a></p>"
"Google Cloud Build - Custom machine types not working<p>I already set up custom machine types in the cloud build file.</p>
<p>But, google tall me a warning (Long build duration).</p>
<p>Custom machine types not applied!</p>
<p>What did I do wrong?</p>
<p><strong>cloudbuild.yaml</strong></p>
<pre class=""lang-yaml prettyprint-override""><code>steps:
  - name: &quot;gcr.io/cos-cloud/cos-customizer&quot;
    args:
      [
        &quot;start-image-build&quot;,
        &quot;-image-project=cos-cloud&quot;,
        &quot;-image-name=${_BASE_IMAGE_NAME}&quot;,
        &quot;-gcs-bucket=${_CLOUD_BUILD_BUCKET_NAME}&quot;,
        &quot;-gcs-workdir=cloud-build-${BUILD_ID}&quot;,
      ]
  - name: &quot;gcr.io/cos-cloud/cos-customizer&quot;
    args:
      [
        &quot;run-script&quot;,
        &quot;-env=_MIX_ENV=${_MIX_ENV},_RELEASE_NAME=${_RELEASE_NAME}&quot;,
        &quot;-script=cloudbuild.sh&quot;,
      ]
  - name: &quot;gcr.io/cos-cloud/cos-customizer&quot;
    args:
      [
        &quot;finish-image-build&quot;,
        &quot;-zone=us-central1-a&quot;,
        &quot;-project=${PROJECT_ID}&quot;,
        &quot;-image-project=${PROJECT_ID}&quot;,
        &quot;-image-family=${_IMAGE_FAMILY_NAME}&quot;,
        &quot;-image-name=${_BASE_IMAGE_NAME}-${_IMAGE_FAMILY_NAME}-${SHORT_SHA}&quot;,
      ]
options:
  machineType: &quot;E2_HIGHCPU_8&quot;
timeout: 3600s
</code></pre>
<p><strong>Google's build duration warning</strong></p>
<p><a href=""https://i.stack.imgur.com/16H07.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/16H07.png"" alt=""enter image description here"" /></a></p>","<p>This is not a CloudBuild problem.</p>
<p><a href=""https://github.com/GoogleCloudPlatform/cos-customizer"" rel=""nofollow noreferrer"">cos-customizer</a> configures the machine type is impossible.</p>
<p><a href=""https://github.com/GoogleCloudPlatform/cos-customizer/issues/79"" rel=""nofollow noreferrer"">https://github.com/GoogleCloudPlatform/cos-customizer/issues/79</a>
<a href=""https://i.stack.imgur.com/Grufz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Grufz.png"" alt=""Answer"" /></a></p>"
"How to fully automate the Google Cloud Build trigger creation<p>i try to fully automate the cloud build trigger creation via sh script</p>
<p>As source I use Github.</p>
<p>So far it's possible to create the trigger</p>
<pre><code>gcloud beta builds triggers create github \
  --repo-name=organisation/repo \
  --repo-owner=organisation \
  --branch-pattern=&quot;^main$&quot; \
  --build-config=cloudbuild.yaml
</code></pre>
<p>BUT each repo has to be authorized manually before otherwise you get the Error:</p>
<pre><code>ERROR: (gcloud.beta.builds.triggers.create.github) FAILED_PRECONDITION: Repository mapping does not exist. Please visit https://console.cloud.google.com/cloud-build/triggers/connect?project=********* to connect a repository to your project
</code></pre>
<p>Which links me to the UI to create the authorization manually</p>
<p>Is there a way to also automate that step?</p>","<p>Currently there is no way to connect to external repositories using the API, but there is an ongoing <a href=""https://issuetracker.google.com/142550612"" rel=""nofollow noreferrer"">feature request</a> for this to be implemented.</p>
<p>There are two options you can adopt now:</p>
<ol>
<li><p><strong>Connect all the repositories at once</strong> from the Cloud Console. This way, you will be able to automate the creation of triggers for those repositories.</p>
</li>
<li><p>Use <strong>Cloud Source Repositories</strong>, which are connected to Cloud Build by default, as indicated <a href=""https://cloud.google.com/build/docs/automating-builds/create-manage-triggers#connect_repo"" rel=""nofollow noreferrer"">here</a>. Check this  <a href=""https://cloud.google.com/source-repositories/docs/adding-repositories-as-remotes"" rel=""nofollow noreferrer"">documentation</a> on how to create a remote repository in CSR from a local git repository.</p>
</li>
</ol>"
"Google Cloud Build Step Logs Not Viewable in Console<p>I am not able to view Google Cloud Build logs in the console. For each step that I click on I cannot see the associated logs in the Build Log window on the right (see picture). This occurs with both the Build Summary and each detail step. The only way to view these logs is to click View Raw, but that is only a great workaround.</p>
<p><a href=""https://i.stack.imgur.com/RbjLw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RbjLw.png"" alt=""Cloud Build Console Screenshot"" /></a></p>
<p>Another issue is that each build step status (Success/Failure) is only populated at the end of the entire build process, as opposed to updating after each step.</p>
<p>Is anybody also experiencing this or have suggestions to rememdy this issue? My browser is Google Chrome Version 93.0.4577.82 (Official Build) (x86_64)</p>","<p>Experience shows that there can be adverse interactions between Chrome Plugins and a variety of websites that have rich content or streaming (such as Google's Console).  If something seems odd, try and create a new Chrome profile or try running in incognito mode and see if that resolves the issue.  If it does, you can incrementally add (or remove) the plugins until you find the one that is causing problems.  If you do find the culprit plugin, consider posting that as a comment to others on what you find.</p>"
"How Can I pass secret manager secret using cloudbuild to app engine environment variable in app.yaml<p><strong>Below is my app.yaml</strong></p>
<pre><code>runtime: python39
entrypoint: gunicorn -b :$PORT main:app
runtime_config:
  python_version: 3

env_variables:
     SEC: %sec%

manual_scaling:
  instances: 1
resources:
  cpu: 1
  memory_gb: 0.5
  disk_size_gb: 10
</code></pre>
<p><strong>This is my cloudbuild.yaml for app engine trying to pass a secrete vale to app.yaml as env veriable</strong></p>
<pre><code>   steps:
    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
      entrypoint: 'bash'
      args: ['-c', &quot;export _VAL=$(echo $$SEC) 
              &amp;&amp; echo $$SEC;echo $_VAL  &amp;&amp; sed -i 's/%sec%/'$$SEC'/g' app.yaml
              &amp;&amp; gcloud config set app/cloud_build_timeout 1600 &amp;&amp; gcloud app deploy&quot;
      ]
      secretEnv: [&quot;SEC&quot;]
    availableSecrets:
      secretManager:
      - versionName: projects/$PROJECT_ID/secrets/db_secret/versions/3
        env: 'SEC'
    
    timeout: '1600s'
</code></pre>","<p>I've done this in the past by just using &quot;&gt;&gt;&quot; to append the env vars to the bottom of my app.yaml. With this method, the env_variables section of your app.yaml needs to be last.</p>
<p>I don't use this method anymore though, the secrets show up in your cloudbuild log. I just import the secret manager package to grab my secrets inside the application these days.</p>
<p>cloudbuild.yaml</p>
<pre><code>steps:
- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  secretEnv: ['SECRET_ONE','SECRET_TWO']
  entrypoint: 'bash'
  args: 
  - -c
  - |
    echo $'\n  SECRET_ONE: '$$SECRET_ONE &gt;&gt; app.yaml
    echo $'\n  SECRET_TWO: '$$SECRET_TWO &gt;&gt; app.yaml
    gcloud -q app deploy
availableSecrets:
  secretManager:
  - versionName: projects/012345678901/secrets/SECRET_ONE
    env: 'SECRET_ONE'
  - versionName: projects/012345678901/secrets/SECRET_TWO
    env: 'SECRET_TWO'
</code></pre>
<p>app.yaml</p>
<pre><code>runtime: go116
main: cmd
service: serviceone
env_variables:
  PROJECT_ID: project-a0a00
  PORT: 8080
</code></pre>"
"How to shutdown a Virtual Machine in GCP after successful execution of code?<h1>I have following</h1>
<ul>
<li>A repository which contains a code(python) to be executed</li>
<li>A VM at Google Cloud (debian/ubuntu)</li>
<li>A scheduler + cloud function to start VM every hour</li>
</ul>
<h1>What I require</h1>
<ul>
<li>I want to run the code present in repository which will be pulled every hour and   runs 15-25 mins and finally after its execution the machine shuts down.</li>
</ul>
<h1>What I already tried:</h1>
<h2>Using Crontab in machine.</h2>
<ol>
<li>used <code>@reboot run_my_script &amp;&amp; shutdown -h now</code></li>
<li>Once this is implemented I am not able to access VM anymore because at every start the vm runs the script and shut itself down.</li>
</ol>
<p><strong>This method not works Example what if my code runs to error or my system requires new dependency</strong></p>
<h2>Syncing Cloud scheduler and Crontab</h2>
<ol>
<li>used <code>5 * * * * run_my_script &amp;&amp; shutdown -h now</code></li>
<li>Cloud scheduler is set to start my vm at every hour exactly at hour:00:00</li>
<li>Crontab starts running the code at hour:05:00 and shutdown after completion (5 minutes to start vm)</li>
</ol>
<p><strong>This method is running fine But I require a more robust solution(Industry accepted way)</strong></p>
<h1>Code Snippets</h1>
<h2>VM</h2>
<h4>Name</h4>
<pre><code>test_vm
</code></pre>
<h4>Crontab</h4>
<pre><code>5 * * * * sh /home/danish_bansal/workflow.sh
</code></pre>
<h4>workflow.sh:</h4>
<pre><code>#!/bin/sh
sudo rm -r repoName/ || true
sudo git clone https://&lt;token&gt;@github.com/Repo/repoName.git
cd repoName
/usr/bin/python3 Script.py
sudo shutdown -h now
</code></pre>
<h2>Cloud Scheduler</h2>
<pre><code>At 0 * * * * calls cloud function
</code></pre>
<h2>Cloud Function</h2>
<pre><code>from googleapiclient import discovery

def startInstance(r):
    service = discovery.build('compute', 'v1')
    print('VM Instance starting')

    # Project ID for this request.
    project = 'project-name' 

    # The name of the zone for this request.
    zone = 'us-central1-a'  

    # Name of the instance resource to start.
    instance = 'test-vm'

    request = service.instances().start(project=project, zone=zone, instance=instance)
    response = request.execute()

    print(response,'VM Instance started')
</code></pre>","<p>I tried to use a different approach to obtain a similar result as you were asking, using <strong>Cloud Build Trigger</strong> that will get the code directly from the <strong>Github repository</strong> and <strong>Cloud Scheduler</strong> to run it at the frequency I prefer.</p>
<p>I begin creating the Trigger following the official documentation <a href=""https://cloud.google.com/build/docs/automating-builds/build-repos-from-github"" rel=""nofollow noreferrer"">“Building repositories from GitHub”</a>:</p>
<p>As is said in the documentation before creating the trigger you must:</p>
<ul>
<li>Allow the <a href=""https://console.cloud.google.com/flows/enableapi?apiid=cloudbuild.googleapis.com&amp;redirect=https://cloud.google.com/build/docs/automating-builds/build-repos-from-github&amp;_ga=2.121596749.432488135.1633329806-829935075.1631776757&amp;_gac=1.50397787.1633504466.CjwKCAjw7--KBhAMEiwAxfpkWNuxqfWNFpuE2XbCByJYLhRH8enGpyX_cE2VvVeLBT0GgOzHgpANixoCdCMQAvD_BwE"" rel=""nofollow noreferrer"">Cloud Build API</a></li>
<li>Have your source ready in your Github repository.</li>
<li>and be aware to have admin-level <a href=""https://docs.github.com/en/github/setting-up-and-managing-organizations-and-teams/repository-permission-levels-for-an-organization#permission-levels-for-repositories-owned-by-an-organization"" rel=""nofollow noreferrer"">permissions in your repository</a>.</li>
</ul>
<p>Once that you have the prerequisites we can go for the first step, connect the <strong>Cloud Build</strong> from your <strong>GCP project</strong> to your <strong>Github repository</strong>.</p>
<p>You’ll be able to do it by going to the <strong><a href=""https://console.cloud.google.com/cloud-build/triggers?_ga=2.267742155.432488135.1633329806-829935075.1631776757&amp;_gac=1.57870680.1633504466.CjwKCAjw7--KBhAMEiwAxfpkWNuxqfWNFpuE2XbCByJYLhRH8enGpyX_cE2VvVeLBT0GgOzHgpANixoCdCMQAvD_BwE"" rel=""nofollow noreferrer"">Trigger</a></strong> section from your <strong>Cloud Build</strong> and clicking on <strong><a href=""https://cloud.google.com/build/docs/automating-builds/build-repos-from-github#installing_gcb_app"" rel=""nofollow noreferrer"">“connect repository”</a></strong>, it will open a window where you’ll have to choose between your repositories and connect them. <a href=""https://cloud.google.com/build/docs/automating-builds/build-repos-from-github#installing_gcb_app"" rel=""nofollow noreferrer"">The official documentation</a> also explains how to sign and authorize the Google Cloud Build App to connect to Google Cloud if you haven’t done it before.</p>
<p>To create the <strong>GitHub trigger</strong> this time in the <strong><a href=""https://console.cloud.google.com/cloud-build/triggers?_ga=2.267742155.432488135.1633329806-829935075.1631776757&amp;_gac=1.57870680.1633504466.CjwKCAjw7--KBhAMEiwAxfpkWNuxqfWNFpuE2XbCByJYLhRH8enGpyX_cE2VvVeLBT0GgOzHgpANixoCdCMQAvD_BwE"" rel=""nofollow noreferrer"">Trigger</a></strong> section, instead of choosing <strong>“connect repository”</strong> you have to select <strong>“create trigger”</strong>, in this part you will be able to <a href=""https://cloud.google.com/build/docs/automating-builds/build-repos-from-github#creating_github_triggers"" rel=""nofollow noreferrer"">create and set the trigger</a> to be able to run the code from your <strong>Github repository</strong>. The following steps are the ones that worked for me:</p>
<ol>
<li><p>Add Name, Description and Tags (as you wish).</p>
</li>
<li><p>In the Event part I selected <strong>“Manual invocation”</strong>.</p>
</li>
<li><p>In <strong>”Source”</strong> I selected my Github repository from the drop-down and in the <strong>”Revision”</strong> part I selected <strong>“Branch”</strong> (for me it was set automatically once I selected the repository).</p>
</li>
<li><p>For the <strong>”Configuration”</strong> for <strong>“Type”</strong> I used <strong>“Cloud Build configuration file (yaml or json)”</strong> and for the <strong>“Location”</strong> I used the <strong>“Inline”</strong> option where in the button that appears as an <strong>editor</strong> I added the next code:</p>
</li>
</ol>
<pre class=""lang-yaml prettyprint-override""><code>            Steps:
            - id: &quot;run&quot;
              name: python:3-alpine
              entrypoint: python3
              args:
               - hello.py
</code></pre>
<p>With this code what I am setting is the language I need to run my code (Python) and the name of the file where is my code that I will want to run, in this case, <code>hello.py</code>.</p>
<ol start=""5"">
<li>Then I clicked on <strong>“create”</strong>.</li>
</ol>
<p>Once it is created, to test it, you can go again to the <strong><a href=""https://console.cloud.google.com/cloud-build/triggers?_ga=2.267742155.432488135.1633329806-829935075.1631776757&amp;_gac=1.57870680.1633504466.CjwKCAjw7--KBhAMEiwAxfpkWNuxqfWNFpuE2XbCByJYLhRH8enGpyX_cE2VvVeLBT0GgOzHgpANixoCdCMQAvD_BwE"" rel=""nofollow noreferrer"">Trigger</a></strong> section and in the list you’ll be able to find the trigger that you just have created, you can click the <strong>“RUN”</strong> button and to check if it works you can go to the <strong>Cloud Build History</strong>.</p>
<p>In this section you will find a list with all the builds created, choosing the one related to our trigger and opening it you’ll be able to see that the code that was named in the code present in your repository, in my case <code>hello.py</code>, was successfully run.</p>
<p>Why using <strong>Cloud Build</strong>?</p>
<p>The reason, a part of being able to get the code directly from the Github repository, is because of how <strong>Cloud Build</strong> works itself, it allows you to stop worrying about having to stop the code, because once the code is executed it will stop by itself.</p>
<p>Now that we demonstrate that we can run our code directly from the <strong>Github repository</strong> creating a <strong>Cloud Build Trigger</strong>, we can move on to set it to be able to run it at every hour.</p>
<p>For this we can use <strong><a href=""https://cloud.google.com/build/docs/automating-builds/create-scheduled-triggers"" rel=""nofollow noreferrer"">Schedule triggers</a></strong>, you’ll be able to set it in the same <strong><a href=""https://console.cloud.google.com/cloud-build/triggers?_ga=2.267742155.432488135.1633329806-829935075.1631776757&amp;_gac=1.57870680.1633504466.CjwKCAjw7--KBhAMEiwAxfpkWNuxqfWNFpuE2XbCByJYLhRH8enGpyX_cE2VvVeLBT0GgOzHgpANixoCdCMQAvD_BwE"" rel=""nofollow noreferrer"">Trigger</a></strong> section, selecting the trigger that you just have created and going to the three dots menu that is next to the <strong>“RUN”</strong> button, there you'll find <strong>“Run on schedule”</strong> and once you select it, it will open a window where you’ll have to enable the <strong>“Cloud Scheduler API”</strong>, select a <strong>“Service Account”</strong> and enter the <strong>“Frequency”</strong> that you will need.</p>
<p>With all these settings I was able to get my code from my repository to run every hour.</p>"
"Google Cloud Builder for Angular fails during Node SASS installation using Cloud Builders Community image<p>Steps I've done so far:</p>
<ul>
<li>I've created a <code>Google Cloud Repository</code></li>
<li>I've created a <code>Cloud Build Trigger</code> and linked it to my GitHub account and repo and matched the branch name exactly <code>^staging$</code></li>
</ul>
<p>And now per <a href=""https://digizoo.com.au/1403/how-to-deploy-angular-8-app-to-google-app-engine/"" rel=""nofollow noreferrer"">the guide</a> I'm trying to initialize an angular builder with the following code:</p>
<pre><code>git clone https://github.com/GoogleCloudPlatform/cloud-builders-community
cd cloud-builders-community/ng
gcloud builds submit --config cloudbuild.yaml .  **** ERROR HERE ****
gcloud container images list --filter ng
</code></pre>
<p>When I try to submit the new builder I get an error initializing node-sass. Now GCC isn't stingy with the build errors but I've condensed it here as I don't believe it's all relevant.</p>
<pre><code>BUILD
Starting Step #0
Step #0: Already have image (with digest): gcr.io/cloud-builders/docker
Step #0: Sending build context to Docker daemon  8.704kB
Step #0: Step 1/4 : FROM node:current
Step #0: current: Pulling from library/node
Step #0: Digest: sha256:a1118930ecc77da1ce4b19ac8c17adf386b7bb36b348111437f1cfca5a5c9fd7
Step #0: Status: Downloaded newer image for node:current
Step #0:  ---&gt; 9c23a8242f8b
Step #0: Step 2/4 : ARG ng_version=latest
Step #0:  ---&gt; Running in 8571f9c93b72
Step #0: Removing intermediate container 8571f9c93b72
Step #0:  ---&gt; ae862a663d5c
Step #0: Step 3/4 : RUN npm install -g @angular/cli@$ng_version --unsafe-perms &amp;&amp;     ng version
Step #0:  ---&gt; Running in 8b6eabc3c04e
...
...
lots of npm install deprecated warnings
...
...
Step #0: npm ERR! code 1
Step #0: npm ERR! path /usr/local/lib/node_modules/@angular/cli/node_modules/node-sass
Step #0: npm ERR! command failed
Step #0: npm ERR! command sh -c node scripts/build.js
Step #0: npm ERR! Building: /usr/local/bin/node /usr/local/lib/node_modules/@angular/cli/node_modules/node-gyp/bin/node-gyp.js rebuild --verbose --libsass_ext= --libsass_cflags= --libsass_ldflags= --libsass_library=
Step #0: npm ERR! make: Entering directory '/usr/local/lib/node_modules/@angular/cli/node_modules/node-sass/build'
Step #0: npm ERR!   g++ '-DNODE_GYP_MODULE_NAME=libsass' '-DUSING_UV_SHARED=1' '-DUSING_V8_SHARED=1' '-DV8_DEPRECATION_WARNINGS=1' '-DV8_DEPRECATION_WARNINGS' '-DV8_IMMINENT_DEPRECATION_WARNINGS' '-D_GLIBCXX_USE_CXX11_ABI=1' '-D_LARGEFILE_SOURCE' '-D_FILE_OFFSET_BITS=64' '-D__STDC_FORMAT_MACROS' '-DOPENSSL_NO_PINSHARED' '-DOPENSSL_THREADS' '-DLIBSASS_VERSION=&quot;3.5.5&quot;' -I/root/.node-gyp/16.10.0/include/node -I/root/.node-gyp/16.10.0/src -I/root/.node-gyp/16.10.0/deps/openssl/config -I/root/.node-gyp/16.10.0/deps/openssl/openssl/include -I/root/.node-gyp/16.10.0/deps/uv/include -I/root/.node-gyp/16.10.0/deps/zlib -I/root/.node-gyp/16.10.0/deps/v8/include -I../src/libsass/include  -fPIC -pthread -Wall -Wextra -Wno-unused-parameter -m64 -O3 -fno-omit-frame-pointer -std=gnu++14 -std=c++0x -fexceptions -frtti -MMD -MF ./Release/.deps/Release/obj.target/libsass/src/libsass/src/ast.o.d.raw   -c -o Release/obj.target/libsass/src/libsass/src/ast.o ../src/libsass/src/ast.cpp
...
...
a lot more GCC make errors
...
...
Step #0: npm ERR! make: *** [binding.target.mk:133: Release/obj.target/binding/src/binding.o] Error 1
Step #0: npm ERR! gyp ERR! build error 
Step #0: npm ERR! gyp ERR! stack Error: `make` failed with exit code: 2
Step #0: npm ERR! gyp ERR! stack     at ChildProcess.onExit (/usr/local/lib/node_modules/@angular/cli/node_modules/node-gyp/lib/build.js:262:23)
Step #0: npm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:390:28)
Step #0: npm ERR! gyp ERR! stack     at Process.ChildProcess._handle.onexit (node:internal/child_process:290:12)
Step #0: npm ERR! gyp ERR! System Linux 5.4.0-1052-gcp
Step #0: npm ERR! gyp ERR! command &quot;/usr/local/bin/node&quot; &quot;/usr/local/lib/node_modules/@angular/cli/node_modules/node-gyp/bin/node-gyp.js&quot; &quot;rebuild&quot; &quot;--verbose&quot; &quot;--libsass_ext=&quot; &quot;--libsass_cflags=&quot; &quot;--libsass_ldflags=&quot; &quot;--libsass_library=&quot;
Step #0: npm ERR! gyp ERR! cwd /usr/local/lib/node_modules/@angular/cli/node_modules/node-sass
Step #0: npm ERR! gyp ERR! node -v v16.10.0
Step #0: npm ERR! gyp ERR! node-gyp -v v3.8.0
Step #0: npm ERR! gyp ERR! not ok 
Step #0: npm ERR! Build failed with error code: 1
Step #0: 
Step #0: npm ERR! A complete log of this run can be found in:
Step #0: npm ERR!     /root/.npm/_logs/2021-10-05T13_37_53_770Z-debug.log
Step #0: The command '/bin/sh -c npm install -g @angular/cli@$ng_version --unsafe-perms &amp;&amp;     ng version' returned a non-zero code: 1
Finished Step #0
ERROR
ERROR: build step 0 &quot;gcr.io/cloud-builders/docker&quot; failed: step exited with non-zero status: 1
Step #0: 
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
ERROR: (gcloud.builds.submit) build c9c6d17a-e87f-449a-9901-44b786e8dd39 completed with status &quot;FAILURE&quot;
</code></pre>
<p>I'm pretty new to angular and super new to cloud building so not sure exactly where to proceed next. If I had to guess the preconfigured build image has an incompatibility between the node version and the node-sass version - at least that's what I've seen cause these kinds of issues in the past locally. But since it's a remote image I feel like it should be in a known good state.</p>","<p>Posting @anon58192932 solution for visibility:</p>
<p>You can follow this <a href=""https://shashankvivek-7.medium.com/continuous-deployment-of-angular-app-on-google-app-engine-with-git-c83d4e14e84"" rel=""nofollow noreferrer"">Medium guide</a> that uses a generic <code>npm</code> image.</p>
<blockquote>
<p><code>cloudbuild.yaml</code>:</p>
<pre><code>steps:

  # Install node packages
  - name: 'gcr.io/cloud-builders/npm'
    args: [ 'install' ]

  # Build productive files
  - name: 'gcr.io/cloud-builders/npm'
    args: [ 'run', 'build', '--prod' ]

  # Deploy to google cloud app egnine
  - name: 'gcr.io/cloud-builders/gcloud'
    args: ['app', 'deploy', '--version=prod']
</code></pre>
</blockquote>"
"why do cloud build private pools have a long queue time<p>I have set up a private worker pool and I had expected that the queued time for a build would go down. Previously queue time (when only one build is queued) was about 1 minute. I had assumed because I was using shared machines inside GCP to do the build. I therefore expected that a private worker pool would have no queue since I would be the only one building anything. I was surprised to see it took about 1 minute also. I then thought that perhaps the first build would have to spin up a VM and that's why it took so long so I carried out a second build after the first had finished but that also had a queue time of about 1 minute. I don't understand what is going on, 1 minute is quite a long time.</p>","<p>When you use Cloud Build shared pool, you use machine provisioned by Google, up and running and paid by Google. Therefore, when you have a build to run, you pick one active machine in the shared pool and you run your build on it.</p>
<p>With private pool, it's different. The machine are still managed by Google, but the pool is private, dedicated to you. Therefore, Google won't keep VM up and running (and use CPU/Memory) if you run nothing on them (because you pay only when a job is running). So, Google stop the VM.</p>
<p>When you run a job on Cloud Build, a VM is started and your job can start. Similar to Compute Engine, it takes about 1 minutes to provision a VM and to start it.</p>
<hr />
<p>That being said, your requirement could be a nice feature request: keep warm a number of VMs to prevent this on demand provisioning. Of course, it won't be free, but it will be faster!</p>
<p>You can open a feature request <a href=""https://issuetracker.google.com/"" rel=""nofollow noreferrer"">here</a></p>"
"How to install python package into a docker service inside of a monorepo<p>I have a monorepo setup such as the following:</p>
<pre><code>/serviceA
/serviceB
/packageA
/packageB
</code></pre>
<p>I would like to install <code>packageA</code> inside of <code>serviceA</code>, which is docker container with a python service inside.</p>
<p>Inside of my <code>serviceA</code> <code>requirements.txt</code> file I can reference the local package: <code>file:///Users/me/dev/platform/packageA</code>, this fails when submitting to <code>gcloud builds</code> as the package is not on that machine.</p>
<p>My question is, what are my options here and am I going about this setup the correct/best way?</p>
<p>My thinking is that I could try and copy the code into the docker, but this seems a little hacky - as I am not sure where the version would go..</p>
<p>The other option is to push to GH first and reference from there, but this would mean I need to somehow grant access to the repo to cloud build.</p>","<p>You can use <a href=""https://docs.docker.com/develop/develop-images/multistage-build/"" rel=""nofollow noreferrer"">Docker multi stage build</a> strategy:</p>
<pre><code>FROM base as package-builder
COPY your_package_A_code
RUN your_package_build_stuff

FROM base
COPY --from=package-builder /path/builded/package_A /path/to/package_A
</code></pre>
<p>This way you don't expose the code of your package A inside the service A. Then, reference your package_A where you decide to store (in my example:<code>/path/to/package_A</code>) in your <code>requirements.txt</code>.</p>"
"Cloud Run Deploy fails: Container failed to start. Failed to start and then listen on the port defined by the PORT environment variable<p>I have a project which I had previously successfully deployed to Google Cloud Run, and set up with a trigger such that upon pushing to the repo's main branch on Github, it would automatically deploy. It worked great.</p>
<p>Then I tried to rename the github repo, which meant deleting and creating a new trigger, and now I cannot get it working again.</p>
<p>Everytime, the build succeeds but deployment fails with this error in Cloud Build:</p>
<p><strong>Step #2 - &quot;Deploy&quot;: ERROR: (gcloud.run.services.update) Cloud Run error: Container failed to start. Failed to start and then listen on the port defined by the PORT environment variable. Logs for this revision might contain more information.</strong></p>
<p>I have not changed anything other than the repo name, leading me to believe the fix is not with my code, but I tried some changes there anyway.</p>
<p>I have looked into the solutions set forth in <a href=""https://stackoverflow.com/questions/55662222/container-failed-to-start-failed-to-start-and-then-listen-on-the-port-defined-b"">this</a> post. However, I believe I am listening on the correct port.</p>
<p>My app is using Python and Flask, and contains this:</p>
<pre><code>if __name__ == &quot;__main__&quot;:
    app.run(debug=False, host=&quot;0.0.0.0&quot;, port=int(os.environ.get(&quot;PORT&quot;, 8080)))
</code></pre>
<p>Which should use the ENV var Port (8080) and otherwise default to 8080. I also tried just using port=8080.</p>
<p>I tried explicitly exposing the port in the Dockerfile, which also did not work:</p>
<pre><code>FROM python:3.7
   
#Copy files into docker image dir, and make that the current working dir
COPY . /docker-image
WORKDIR /docker-image

RUN pip install -r requirements.txt

CMD [&quot;flask&quot;, &quot;run&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;]

EXPOSE 8080
</code></pre>
<p>Cloud Run does seem to be using port 8080 - if I dig into the response, I see this nested under Response.spec.container.0 :</p>
<pre><code>ports: [
  0: {
    containerPort: 8080
    name: &quot;http1&quot;
  }
]
</code></pre>
<p>All that said, if I look at the logs, it shows &quot;Now running on Port 5000&quot;.</p>
<p><a href=""https://i.stack.imgur.com/syDHo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/syDHo.png"" alt=""enter image description here"" /></a></p>
<p>I have no idea where that Port 5000 is coming from or being set, but trying to change the ports in Python/Flask and the Dockerfile to 5000 leads to the same errors.</p>
<p>How do I get it to run on Port 8080? It's very strange to me that this was working FINE prior to renaming the repo and creating a new trigger. How is this setup different? The Trigger does not give an option to set the port so I'm not sure how that caused this error.</p>","<p>You have mixed things. Flask command default port is effectively 5000. If you want to change it, you need to change your flask run command with the <code>--port=</code> parameter</p>
<pre><code>CMD [&quot;flask&quot;, &quot;run&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;,&quot;--port&quot;,&quot;8080&quot;]
</code></pre>
<p>In addition, your flask run command, is a flask runtime and totally ignore the standard python entrypoint <code>if __name__ == &quot;__main__&quot;:</code>. If you want to use this entrypoint, use the Python runtime</p>
<pre><code>CMD [&quot;python&quot;, &quot;&lt;main file&gt;.py&quot;]
</code></pre>"
"Firebase Cloud messaging - permissions error for cloud build deployed app<p>I have a java spring boot backend app, that I am trying to hook up to Firebase Cloud Messages.
I have an android app that uses firebase and I am trying to use this backend to push notifications.</p>
<p>I've generated a private key from firebase console project settings, placed the json file - and the following worked LOCALLY perfectly:</p>
<pre><code>try {
  FirebaseOptions options = new FirebaseOptions.Builder()
    .setCredentials(GoogleCredentials.fromStream(new ClassPathResource(&quot;PATH_TO_GENERATED_JSON&quot;).
  getInputStream())).build();
  if (FirebaseApp.getApps().isEmpty()) {
    FirebaseApp.initializeApp(options);
    logger.info(&quot;Firebase application has been initialized&quot;);
  }
} catch (IOException e) {
  logger.error(e.getMessage());
}
...
response = FirebaseMessaging.getInstance().send(message);
</code></pre>
<p>I have set up google cloud build to automatically trigger and build from github.
But I cannot commit the json credentials file (right?), so for cloud deployment I have changed the initialization part to:</p>
<pre><code>if (FirebaseApp.getApps().isEmpty()) {
  FirebaseApp.initializeApp();
  logger.info(&quot;Firebase application has been initialized&quot;);
}
</code></pre>
<p>But I have received errors information about project id not set, so I have also edited the cloud build trigger inline YAML with:</p>
<p><code>--update-env-vars=GOOGLE_CLOUD_PROJECT=XXXXXXXXXXX</code></p>
<p>But now I am getting the following error when trying to send the message:</p>
<p><code>com.google.firebase.messaging.FirebaseMessagingException: Permission 'cloudmessaging.messages.create' denied on resource '//cloudresourcemanager.googleapis.com/projects/XXXXXXXXXXX' (or it may not exist).</code>
(XXXXXXXXXXX being my project id)</p>
<p>I've started giving &quot;Firebase Cloud Messaging Admin&quot; role left and right on <a href=""https://console.cloud.google.com/iam-admin/iam?project="" rel=""nofollow noreferrer"">https://console.cloud.google.com/iam-admin/iam?project=</a> but that didn't help :(</p>
<p>Can anyone help?</p>
<p>Adding stack trace:</p>
<pre><code>com.google.firebase.messaging.FirebaseMessagingException: Permission 'cloudmessaging.messages.create' denied on resource '//cloudresourcemanager.googleapis.com/projects/our-shield-329019' (or it may not exist).
    at com.google.firebase.messaging.FirebaseMessagingException.withMessagingErrorCode(FirebaseMessagingException.java:51)
    at com.google.firebase.messaging.FirebaseMessagingClientImpl$MessagingErrorHandler.createException(FirebaseMessagingClientImpl.java:293)
    at com.google.firebase.messaging.FirebaseMessagingClientImpl$MessagingErrorHandler.createException(FirebaseMessagingClientImpl.java:282)
    at com.google.firebase.internal.AbstractHttpErrorHandler.handleHttpResponseException(AbstractHttpErrorHandler.java:57)
    at com.google.firebase.internal.ErrorHandlingHttpClient.send(ErrorHandlingHttpClient.java:108)
    at com.google.firebase.internal.ErrorHandlingHttpClient.sendAndParse(ErrorHandlingHttpClient.java:72)
    at com.google.firebase.messaging.FirebaseMessagingClientImpl.sendSingleRequest(FirebaseMessagingClientImpl.java:127)
    at com.google.firebase.messaging.FirebaseMessagingClientImpl.send(FirebaseMessagingClientImpl.java:113)
    at com.google.firebase.messaging.FirebaseMessaging$1.execute(FirebaseMessaging.java:137)
    at com.google.firebase.messaging.FirebaseMessaging$1.execute(FirebaseMessaging.java:134)
    at com.google.firebase.internal.CallableOperation.call(CallableOperation.java:36)
    at com.google.firebase.messaging.FirebaseMessaging.send(FirebaseMessaging.java:104)
    at com.google.firebase.messaging.FirebaseMessaging.send(FirebaseMessaging.java:86)
    at com.miloszdobrowolski.investobotbackend.InvestobotAPIs.testNotification(InvestobotAPIs.java:120)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
    at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
    at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
    at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
    at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
    at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1067)
    at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:963)
    at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
    at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:898)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:655)
    at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:764)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:227)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
    at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:189)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:162)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:197)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:97)
    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:540)
    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:135)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78)
    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:357)
    at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:382)
    at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65)
    at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:893)
    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1726)
    at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49)
    at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
    at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
    at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: com.google.api.client.http.HttpResponseException: 403 Forbidden
</code></pre>","<p>As listed in the <a href=""https://cloud.google.com/iam/docs/understanding-roles#firebase-roles"" rel=""nofollow noreferrer"">Understanding Roles</a> documentation, <strong>Firebase Cloud Messaging Admin</strong> does not have a <code>cloudmessaging.messages.create</code> permission. In order to add this permission, use one of the following roles:</p>
<ol>
<li>Firebase Admin (roles/firebase.admin)</li>
<li>Firebase Grow Admin (roles/firebase.growthAdmin)</li>
<li>Firebase Admin SDK Administrator Service Agent (roles/firebase.sdkAdminServiceAgent)</li>
<li>Firebase SDK Provisioning Service Agent (roles/firebase.sdkProvisioningServiceAgent)</li>
</ol>"
"Kubectl patch with json fails on Cloud Build<p>I can patch my ingress resource from cli with kubectl running following command:</p>
<pre><code>kubectl patch ingress ingress-resource --type=json -p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/rules/0/http/paths/0/backend/service/name&quot;, &quot;value&quot;:&quot;node-app-blue-helm-chart&quot;}]' 
</code></pre>
<p>When I add following step to my cloudbuild.yaml and execute, it fails with following error.</p>
<pre><code>Step #3: Running: kubectl patch ingress ingress-resource --type=json -p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/rules/0/http/paths/0/backend/service/name&quot;, &quot;value&quot;:&quot;node-app-blue-helm-chart&quot;}]' 
Step #3: Error from server (BadRequest): json: cannot unmarshal string into Go value of type jsonpatch.Patch
Finished Step #3
</code></pre>
<p>Step I used:</p>
<pre><code>  - name: 'gcr.io/cloud-builders/kubectl'
    args:
    - 'patch'
    - 'ingress'
    - 'ingress-resource'
    - '--type=json'
    - '-p=''[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/rules/0/http/paths/0/backend/service/name&quot;, &quot;value&quot;:&quot;node-app-green-helm-chart&quot;}]'' '
    env:
    - 'CLOUDSDK_COMPUTE_ZONE=----'
    - 'CLOUDSDK_CONTAINER_CLUSTER=----'
</code></pre>
<p>What can be missing?</p>","<p>To solve this problem I tried to escape all &quot; as mentioned here (<a href=""https://stackoverflow.com/questions/48650734/passing-a-json-string-command-arg-in-a-yaml-file"">Passing a json-string command arg in a YAML file</a>). It didn't solve problem. But then I also divided &quot;-p&quot; and the rest and also used &quot; on second part. So working step became like this:</p>
<pre><code>  - name: 'gcr.io/cloud-builders/kubectl'
    args:
    - &quot;patch&quot;
    - &quot;ingress&quot;
    - &quot;ingress-resource&quot;
    - &quot;--type=json&quot;
    - &quot;-p&quot;
    - &quot;[{\&quot;op\&quot;: \&quot;replace\&quot;, \&quot;path\&quot;: \&quot;/spec/rules/0/http/paths/0/backend/service/name\&quot;, \&quot;value\&quot;:\&quot;node-app-green-helm-chart\&quot;}]&quot;
    env:
    - 'CLOUDSDK_COMPUTE_ZONE=----'
    - 'CLOUDSDK_CONTAINER_CLUSTER=----'
</code></pre>"
"Spring Cloud Data Flow : Asynchronous DeploymentPartitionHanlder<p><strong>TL;DR</strong></p>
<p>I used <a href=""https://dataflow.spring.io/docs/feature-guides/batch/partitioning/"" rel=""nofollow noreferrer"">this example</a> to build a simple application that uses Spring Batch (remote partitioning) and Spring Cloud data flow to deploy worker pods on Kubernetes.</p>
<p>Looking at the logs for the &quot;partitionedJob&quot; pod created on Kubernetes, I see that the worker steps (pods) are getting launched sequentially. The time taken to launch one worker pod is roughly 10-15 seconds (Sometimes this is as high as 2 minutes as well as shown below). As a result, worker pods are getting launched at a gap of 10-15 seconds one by one.</p>
<hr />
<p><strong>Logs :</strong></p>
<pre><code>[info 2021/06/26 14:30:29.089 UTC &lt;main&gt; tid=0x1] Job: [SimpleJob: [name=job]] launched with the following parameters: [{maxWorkers=40, chunkSize=5000, run.id=13, batch.worker-app=docker://docker-myhost.artifactrepository.net/my-project/myjob:0.1, grideSize=40}]

[info 2021/06/26 14:30:29.155 UTC &lt;main&gt; tid=0x1] The job execution id 26 was run within the task execution 235

[info 2021/06/26 14:30:29.184 UTC &lt;main&gt; tid=0x1] Executing step: [masterStep]

2021-06-26 14:30:29 INFO  AuditRecordPartitioner:51 - Creating partitions. [gridSize=40]

[info 2021/06/26 14:32:41.128 UTC &lt;main&gt; tid=0x1] Using Docker entry point style: exec

[info 2021/06/26 14:34:51.560 UTC &lt;main&gt; tid=0x1] Using Docker image: docker-myhost.artifactrepository.net/myproject/myjob:0.1

[info 2021/06/26 14:34:51.560 UTC &lt;main&gt; tid=0x1] Using Docker entry point style: exec

[info 2021/06/26 14:36:39.464 UTC &lt;main&gt; tid=0x1] Using Docker image: docker-myhost.artifactrepository.net/myproject/myjob:0.1

[info 2021/06/26 14:36:39.464 UTC &lt;main&gt; tid=0x1] Using Docker entry point style: exec

[info 2021/06/26 14:38:34.203 UTC &lt;main&gt; tid=0x1] Using Docker image: docker-myhost.artifactrepository.net/myproject/myjob:0.1

[info 2021/06/26 14:38:34.203 UTC &lt;main&gt; tid=0x1] Using Docker entry point style: exec

[info 2021/06/26 14:40:44.544 UTC &lt;main&gt; tid=0x1] Using Docker image: docker-myhost.artifactrepository.net/myproject/myjob:0.1

[info 2021/06/26 14:40:44.544 UTC &lt;main&gt; tid=0x1] Using Docker entry point style: exec
</code></pre>
<p><strong>It takes roughly 7-8 minutes for 40 pods to be created on Kubernetes</strong>. (Sometimes this number is as high as 20 minutes)  What would be ideal is for all the partitioned steps (worker pods) to be launched asynchronously in one go.</p>
<p><strong>Question</strong> : How can we configure Spring Cloud Data Flow /Spring Batch to launch worker pods (partitioned steps) asynchronously/parallelly instead of sequentially? If SCDF is indeed creating 40 partitions in one go, why is that in reality, the master job is creating these partitions one by one at a very slow rate? (As seen in the logs). I don't believe it is an infra issue because I am able to launch tasks at a rapid speed using the <a href=""https://dataflow.spring.io/docs/feature-guides/batch/java-dsl/"" rel=""nofollow noreferrer"">Task DSL</a></p>
<p><strong>Relevant code:</strong></p>
<pre><code>@EnableTask
@EnableBatchProcessing
@SpringBootApplication
public class BatchApplication {

    public static void main(String[] args) {
        SpringApplication.run(BatchApplication.class, args);
    }
}


/**
 * 
 * Main job controller
 * 
 * 
 */
@Profile(&quot;master&quot;)
@Configuration
public class MasterConfiguration {

    private static final Logger LOGGER = LoggerFactory.getLogger(MasterConfiguration.class);

    @Autowired
    private ApplicationArguments applicationArguments;

    @Bean
    public Job job(JobBuilderFactory jobBuilderFactory) {
        LOGGER.info(&quot;Creating job...&quot;);
        SimpleJobBuilder jobBuilder = jobBuilderFactory.get(&quot;job&quot;).start(masterStep(null, null, null));

        jobBuilder.incrementer(new RunIdIncrementer());

        return jobBuilder.build();
    }

    @Bean
    public Step masterStep(StepBuilderFactory stepBuilderFactory, Partitioner partitioner,
            PartitionHandler partitionHandler) {
        LOGGER.info(&quot;Creating masterStep&quot;);
        return stepBuilderFactory.get(&quot;masterStep&quot;).partitioner(&quot;workerStep&quot;, partitioner)
                .partitionHandler(partitionHandler).build();
    }

    @Bean
    public DeployerPartitionHandler partitionHandler(@Value(&quot;${spring.profiles.active}&quot;) String activeProfile,
            @Value(&quot;${batch.worker-app}&quot;) String resourceLocation,
            @Value(&quot;${spring.application.name}&quot;) String applicationName, ApplicationContext context,
            TaskLauncher taskLauncher, JobExplorer jobExplorer, ResourceLoaderResolver resolver) {
        ResourceLoader resourceLoader = resolver.get(resourceLocation);
        Resource resource = resourceLoader.getResource(resourceLocation);
        DeployerPartitionHandler partitionHandler = new DeployerPartitionHandler(taskLauncher, jobExplorer, resource,
                &quot;workerStep&quot;);

        List&lt;String&gt; commandLineArgs = new ArrayList&lt;&gt;();
        commandLineArgs.add(&quot;--spring.profiles.active=&quot; + activeProfile.replace(&quot;master&quot;, &quot;worker&quot;));
        commandLineArgs.add(&quot;--spring.cloud.task.initialize.enable=false&quot;);
        commandLineArgs.add(&quot;--spring.batch.initializer.enabled=false&quot;);

        commandLineArgs.addAll(Arrays.stream(applicationArguments.getSourceArgs()).filter(
                x -&gt; !x.startsWith(&quot;--spring.profiles.active=&quot;) &amp;&amp; !x.startsWith(&quot;--spring.cloud.task.executionid=&quot;))
                .collect(Collectors.toList()));
        commandLineArgs.addAll(applicationArguments.getNonOptionArgs());

        partitionHandler.setCommandLineArgsProvider(new PassThroughCommandLineArgsProvider(commandLineArgs));
        partitionHandler.setEnvironmentVariablesProvider(new NoOpEnvironmentVariablesProvider());

        List&lt;String&gt; nonOptionArgs = applicationArguments.getNonOptionArgs();

        partitionHandler.setMaxWorkers(Integer.valueOf(getNonOptionArgValue(nonOptionArgs, 3)));
        partitionHandler.setGridSize(Integer.valueOf(getNonOptionArgValue(nonOptionArgs, 4)));
        partitionHandler.setApplicationName(applicationName);

        return partitionHandler;
    }

    @Bean(&quot;auditRecordPartitioner&quot;)
    public Partitioner auditRecordPartitioner() {
        
        return new AuditRecordPartitioner&lt;&gt;());
    }
    
    private String getNonOptionArgValue(List&lt;String&gt; nonOptionArgs, int index)  {
        return nonOptionArgs.get(index).split(&quot;=&quot;)[1];
    }
}


@Profile(&quot;worker&quot;)
@Configuration
public class WorkerConfiguration {

    private static final Logger LOGGER = LoggerFactory.getLogger(WorkerConfiguration.class);

    @Autowired
    public JobBuilderFactory jobBuilderFactory;

    @Autowired
    public StepBuilderFactory stepBuilderFactory;

    @Autowired
    private ApplicationArguments applicationArguments;

    @Bean
    public DeployerStepExecutionHandler stepExecutionHandler(ApplicationContext context, JobExplorer jobExplorer,
            JobRepository jobRepository) {
        LOGGER.info(&quot;stepExecutionHandler...&quot;);
        return new DeployerStepExecutionHandler(context, jobExplorer, jobRepository);
    }

    @Bean
    public Step workerStep(StepBuilderFactory stepBuilderFactory) {
        return stepBuilderFactory.get(&quot;workerStep&quot;).tasklet(workerTasklet(null)).build();
    }

    @Bean
    @StepScope
    public WorkerTasklet workerTasklet(@Value(&quot;#{stepExecutionContext['key']}&quot;) String key) {
        return new WorkerTasklet(key);
    }

    
}
</code></pre>
<p><strong>Note</strong> that I am passing gridSize and maxWorkers as input argumnets to the master step (From SCDF UI while launching the task).</p>","<p>As mentioned by <a href=""https://stackoverflow.com/users/5019386/mahmoud-ben-hassine"">Mahmoud Ben Hassine</a> in the comments, the workers are <a href=""https://github.com/spring-cloud/spring-cloud-task/blob/7f20c5a06ed7b1edb83b8bd187fcdaccfe8ed20e/spring-cloud-task-batch/src/main/java/org/springframework/cloud/task/batch/partition/DeployerPartitionHandler.java#L309"" rel=""nofollow noreferrer"">launched sequentially</a> :</p>
<pre><code>private void launchWorkers(Set&lt;StepExecution&gt; candidates,
            Set&lt;StepExecution&gt; executed) {
        for (StepExecution execution : candidates) {
            if (this.currentWorkers &lt; this.maxWorkers || this.maxWorkers &lt; 0) {
                launchWorker(execution);
                this.currentWorkers++;

                executed.add(execution);
            }
        }
    }
</code></pre>
<p>As <a href=""https://stackoverflow.com/users/5859097/glenn-renfro"">Glen Renfro</a> mentioned in the comments, an <a href=""https://github.com/spring-cloud/spring-cloud-task/issues/785"" rel=""nofollow noreferrer"">issue</a>  has been created for the same. This answer will be updated if a solution is available for launching workers asynchronously.</p>"
"How to build and deploy kubernetes cluster to Google Cloud using Cloud Build and Skaffold?<p>I am new to micro-services technologies and getting troubled with Google Cloud Build.
I am using Docker, Kubernetes, Ingres Nginx and skaffold and my deployment works fine in local machine.</p>
<p>Now I want to develop locally and build and run remotely using Cloud Platform so, here's what I have done:</p>
<ul>
<li>In Google Cloud, I have set up <em>kubernetes cluster</em></li>
<li>Set local kubectl context to cloud cluster</li>
<li>Set up an <em>Ingress Nginx</em> load balancer</li>
<li>Enabled Cloud Build API (no trigger setup)</li>
</ul>
<p>Here's my <em>create deployment</em> and <em>skaffold</em> yaml files look like:
<a href=""https://i.stack.imgur.com/SxeGB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SxeGB.png"" alt=""enter image description here"" /></a></p>
<p>When I run <strong>skaffold dev</strong>, it logs out: <em>Some taggers failed. Rerun with -vdebug for errors.</em>, then it takes some time and my network bandwidth.</p>
<p>The image does get pushed to <strong>Cloud Container Registry</strong> and I can access the app using load balancer's IP address but the <strong>Cloud Build History</strong> is still empty. Where am I missing?</p>
<p><strong>Note:</strong> Right now I am not pushing my code to any online repository like github.</p>
<p>Sorry If the information I provide is insufficient, I am new to these technologies.</p>","<p>Cloud Build started working:</p>
<p>First, In Cloud Build settings, I enabled kubernetes Engine, Compute Engine and Service Accounts.</p>
<p>Then, I executed these 2 commands:</p>
<ol>
<li><p><code>gcloud auth application-default login</code>:  As google describes it <em>This will acquire new user credentials to use for Application Default Credentials</em></p>
</li>
<li><p>As mentioned in <em>ingress nginx -&gt; deploy -&gt; GCE-GKE</em> documentation, <em>this will Initialize your user as a cluster-admin</em></p>
</li>
</ol>
<pre><code>kubectl create clusterrolebinding cluster-admin-binding \
  --clusterrole cluster-admin \
  --user $(gcloud config get-value account)
</code></pre>"
"Deployment to App Engine using Cloudbuild failing<p>I've built a Node + Vue JS project that is structured with different directories for the frontend and backend code. <code>/api</code> is the backend code with its own package.json and <code>ui</code> is the frontend code with its own package.json. The project is structured like this:</p>
<pre><code>/app
  /api
    package.json
  /ui
    package.json
  /config
    cloudbuild.yaml
</code></pre>
<p>I am attempting to deploy the project to App Engine using Cloudbuild. The <code>cloudbuild.yaml</code> file is structured like this:</p>
<pre><code>steps:
- name: gcr.io/cloud-builders/gcloud:latest
  entrypoint: &quot;ls&quot;
  args: [&quot;-lah&quot;,&quot;/workspace&quot;]
- name: node
  entrypoint: yarn
  args: [&quot;install&quot;]
  dir: &quot;api&quot;
- name: node
  entrypoint: yarn
  args: ['global', 'add', '@vue/cli']
  dir: &quot;ui&quot;
- name: node
  entrypoint: yarn
  args: ['run', 'build']
  dir: &quot;ui&quot;
- name: &quot;gcr.io/cloud-builders/gcloud&quot;
  args: [&quot;app&quot;, &quot;deploy&quot;, &quot;./app.yaml&quot;]
timeout: &quot;1600s&quot;
</code></pre>
<p>Steps 0-2 complete successfully, but the build fails when it comes to building the Vue application for production, specifically the command of <code>yarn run build</code>. This command is listed in the <code>/ui</code> directory's package.json as <code>vue-cli-service build</code>.</p>
<p>The error is <code>/bin/sh: 1: vue-cli-service: not found</code></p>
<p>It seems that Cloudbuild can't find vue-cli as if its not installed OR it doesn't know what to build.</p>
<p>My question is how can I deploy a project with separate directories to App Engine with Cloudbuild?</p>","<p>One of the core principle of Cloud Build, is to start from a next execution context at each step. only the <code>/workspace</code> directory is kept between each step.</p>
<p>In your 3rd step, you install globally your vue cli, therefore, not in your current directory (a subdirectory of workspace), but it the container runtime directory (<code>/etc</code> or somewhere else; in any cases, not under workspace.</p>
<p>If you remove the <code>global</code> parameter, you will install the library locally of your app directory (under <code>/workspace</code>) and therefore the install will persist for the following steps.</p>"
"Failed to start and then listen on the port<p>I have an NodeJS app that I want to deploy on Google Cloud Run.
I have Google Cloud Build configured to build container from dockerfile whenever something has been pushed ona master branch and after build Cloud Run will run new revision.</p>
<p>My problem is that every time I want to deploy my app I got the following error:<br>
<code>Cloud Run error: Container failed to start. Failed to start and then listen on the port defined by the PORT environment variable.</code></p>
<p>Cloud Run have configured <code>containerPort: 8080</code></p>
<p>In my dockerfile I'm exposing port 8080 and in nodejs I have set up simply http server using</p>
<pre class=""lang-js prettyprint-override""><code>const http = require('http');

const server = http.createServer((req, res) =&gt; {
    res.writeHead(200, {'Content-Type': 'text/plain'});
    res.end('Just for testing purposes\n');
});

const port = parseInt(process.env.PORT, 10) || 8080;
server.listen(port, '0.0.0.0', () =&gt; {
    console.log('Hello world listening on port', port);
});
</code></pre>
<p>my Dockerfile</p>
<pre><code>FROM node:12-alpine

# Install app dependencies.
COPY package.json /src/package.json
WORKDIR /src
RUN npm install

# Cloud Run requrement
EXPOSE 8080

COPY index.js /src/index.js

ENTRYPOINT &quot;node index.js&quot;
</code></pre>
<p>Have I missed something? This is my first time working with google cloud so I'm sure there is something I need to configure that I don't know about yet.</p>","<p>The problem was in my Dockerfile.<br>
I had to change <code>ENTRYPOINT &quot;node index.js&quot;</code> to <code>CMD [&quot;node&quot;, &quot;index.js&quot;]</code>.<br>
Bu reason behind it is still unknown to me.</p>"
"Deploy django to GAE standard from cloud build<p>I run following bash commands from my local machine to deploy django project to App engine.</p>
<pre><code>python manage.py migrate
python manage.py collectstatic --noinput
gsutil rsync -R static/ gs://xyz4/static
gcloud config set project project_name_1
gcloud app deploy --quiet
</code></pre>
<p>I would like to set it up on cloud build. I have enabled PUSH triggers on cloud build. Need help in creating <code>cloudbuild.yaml</code> file</p>","<p><em>CloudBuild doesnt support VPC hence cannot be used for migration to private cloud sql - <a href=""https://stackoverflow.com/a/62307285/3842788"">link</a></em></p>
<p>Following are steps I use when deploying code from Github repo to App engine standard. Each step is dependant on previous step running successfully.</p>
<ul>
<li>Create python venv &amp; install all dependencies</li>
<li>Install gcloud proxy &amp; make it executable</li>
<li>Turn on proxy, activate venv, run tests, if test pass then make migrations, collect static files</li>
<li>Upload static files to public bucket</li>
<li>deploy code to GAE standard</li>
</ul>
<p><strong>cloudbuild.yaml:</strong></p>
<pre><code>  - id: setup-venv
    name: python:3.8-slim
    timeout: 100s
    entrypoint: sh
    args:
      - -c
      - '(python -m venv my_venv &amp;&amp; . my_venv/bin/activate &amp;&amp; pip install -r requirements.txt &amp;&amp; ls)'
    waitFor: [ '-' ]

  - id: proxy-install
    name: 'alpine:3.10'
    entrypoint: sh
    args:
      - -c
      - 'wget -O /workspace/cloud_sql_proxy https://storage.googleapis.com/cloudsql-proxy/v1.21.0/cloud_sql_proxy.linux.amd64 &amp;&amp;  chmod +x /workspace/cloud_sql_proxy'
    waitFor: [ 'setup-venv' ]

  - id: run-tests-with-proxy
    name: python:3.8-slim
    entrypoint: sh
    args:
      - -c
      - '(/workspace/cloud_sql_proxy -dir=/workspace -instances=&quot;&lt;instance_name&gt;=tcp:3306&quot; &amp; sleep 2) &amp;&amp; (. my_venv/bin/activate &amp;&amp; python manage.py test --noinput &amp;&amp; python manage.py migrate &amp;&amp; python manage.py collectstatic --noinput )'
    waitFor: [ 'proxy-install' ]
    env:
      - 'CLOUD_BUILD=1'
      - 'PYTHONPATH=/workspace'

  # if tests fail, these sections wont execute coz they waitFor tests section
  - id: upload-static-to-bucket
    name: 'gcr.io/cloud-builders/gsutil'
    entrypoint: 'bash'
    args: [ '-c', 'gsutil rsync -R ./static/ gs://&lt;bucket_name&gt;/static' ]
    waitFor: [ 'run-tests-with-proxy' ]

  - id: deploy
    name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args: [ '-c', 'gcloud app deploy --quiet' ]
    waitFor: [ 'upload-static-to-bucket' ]
</code></pre>
<p><strong>Scope for improvement:</strong></p>
<ul>
<li>how to have args broken into multiple lines instead of everything being on one line</li>
<li>If tests run on local postgres instance on cloudbuild instead of production cloud sql instance, that would be nice. I was able to create a postgres instance, but it did not run in background, hence when running tests my code could not connect to this local instance</li>
</ul>
<p><strong>postgres in cloudbuild.yaml:</strong></p>
<pre><code>- id: setup-postgres
  name: postgres
  timeout: 500s
  waitFor: [ '-' ]
  env:
    - 'POSTGRES_PASSWORD=password123'
    - 'POSTGRES_DB=aseem'
    - 'POSTGRES_USER=aseem'
    - 'PGPORT=5432'
    - 'PGHOST=127:0:0:1'
    
</code></pre>"
"ERROR: (gcloud.builds.submit) HTTPError 403: *** does not have storage.objects.get access to the Google Cloud Storage object<p>I have a Github Action to submit my source-code to Google Cloud Build. The submission does work. The Cloud Build job is being triggered.</p>
<p>However the Github Action exits with an error. This is the error message:</p>
<pre><code>ERROR: (gcloud.builds.submit) HTTPError 403: &lt;?xml version='1.0' encoding='UTF-8'?&gt;&lt;Error&gt;&lt;Code&gt;AccessDenied&lt;/Code&gt;&lt;Message&gt;Access denied.&lt;/Message&gt;&lt;Details&gt;*** does not have storage.objects.get access to the Google Cloud Storage object.&lt;/Details&gt;&lt;/Error&gt;
</code></pre>
<p>I am using a custom service account. These are the roles I've assigned to it. I have no idea why the error is still thrown.</p>
<pre><code>Cloud Build Service Account
Cloud Build Viewer
Environment User and Storage Object Viewer
Cloud Storage for Firebase Viewer
Storage Object Viewer
</code></pre>
<p>I read in another question that this issue has been solved by provided the role <code>Viewer</code> but a role just called <code>Viewer</code> does not exist - at least not in my role listing.</p>
<p>A little side question:</p>
<blockquote>
<p>Is there a way to check what role is needed for a given cloud action? For example seeing this in my logs <code>storage.objects.get</code> I'd like to see what roles do provide access to this.</p>
</blockquote>","<p>The other question mentioned by the OP was referencing the role <a href=""https://cloud.google.com/iam/docs/understanding-roles#basic-definitions"" rel=""nofollow noreferrer""><code>roles/viewer</code></a>, which is viewer in all resources for a project, rather than a more specific viewer role like <a href=""https://cloud.google.com/iam/docs/understanding-roles#cloud-storage-roles"" rel=""nofollow noreferrer""><code>roles/storage.objectViewer</code></a>.</p>
<p>Note that the amount of permissions in each role is very different, and there might be a permission in the <code>roles/viewer</code> role that is not available in <code>roles/storage.objectViewer</code>, but it's needed for the build to work.</p>
<p>I'm guessing at some point the account tries to execute a <a href=""https://cloud.google.com/storage/docs/json_api/v1/buckets/get"" rel=""nofollow noreferrer""><code>buckets.get</code></a>, and since <code>roles/storage.objectViewer</code> role does not have the explicit <code>storage.buckets.get</code> permission, it gives an access error. That permission is granted with the <code>roles/viewer</code> project role <sup>1</sup>.</p>
<p>In any way, the <code>roles/viewer</code> role can be found through the console <a href=""https://console.cloud.google.com/iam-admin/roles/details/roles%3Cviewer"" rel=""nofollow noreferrer"">here</a>, or through the SDK with a command like this one:</p>
<pre><code>gcloud iam service-accounts add-iam-policy-binding &lt;YOUR_SERVICE_ACCOUNT&gt;@&lt;YOUR_PROJECT&gt;.iam.gserviceaccount.com --member='&lt;YOUR_SERVICE_ACCOUNT&gt;@&lt;YOUR_PROJECT&gt;.iam.gserviceaccount.com' --role='roles/viewer'
</code></pre>
<hr />
<p><sup>1</sup> I'm just guessing, but the missing permission could be any other of the hundreds available to <code>roles/viewer</code> and missing from the other roles.</p>"
"How to use python-cloudbuild to run a build trigger<p>How to use python-cloudbuild library to run a build trigger with correctly passing data from SourceRepo?</p>
<p>UPDATE 1:</p>
<p>I have a build trigger set up and I am trying to run that trigger by changing the substitutions and the repo branch</p>
<p>UPDATE 2:</p>
<p>Actual code result:</p>
<p>Traceback (most recent call last): File &quot;/layers/google.python.pip/pip/lib/python3.9/site-packages/google/api_core/grpc_helpers.py&quot;, line 67, in error_remapped_callable return callable_(*args, **kwargs) File &quot;/layers/google.python.pip/pip/lib/python3.9/site-packages/grpc/_channel.py&quot;, line 946, in <strong>call</strong> return _end_unary_response_blocking(state, call, False, None) File &quot;/layers/google.python.pip/pip/lib/python3.9/site-packages/grpc/_channel.py&quot;, line 849, in _end_unary_response_blocking raise _InactiveRpcError(state) grpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:
status = StatusCode.INTERNAL</p>
<pre><code>credentials, project_id = google.auth.default()
client = cloudbuild_v1.services.cloud_build.CloudBuildClient()

trigger_id = '2f1erbc4-asdf-1234-qwery-c4bc74d16d62'

repo_source = cloudbuild_v1.RepoSource()
repo_source.branch_name = 'develop'
repo_source.substitutions = {
    &quot;_ENVIRONMENT&quot;:&quot;dev&quot;,
    &quot;NAMESPACE&quot;:&quot;dev&quot;
}

operation = client.run_build_trigger(
    project_id=project_id,
    trigger_id=trigger_id,
    source=repo_source
)
</code></pre>","<p>I am facing the same issue when using the Cloud Build Client Library for Python (google-cloud-build). However, it does work properly when calling the REST API directly, so the library seems to be at cause here. As an alternative, you can achieve the same using the Google API Python client library (google-api-python-client):</p>
<pre class=""lang-py prettyprint-override""><code>from googleapiclient.discovery import build

project_id = &quot;my-project-id&quot;
trigger_id = &quot;00000000-1111-2222-aaaa-bbbbccccdddd&quot;

with build(&quot;cloudbuild&quot;, &quot;v1&quot;) as cloudbuild:
  run_build_trigger = cloudbuild.projects().triggers().run(
    projectId = project_id,
    triggerId = trigger_id,
    body = {
      &quot;branchName&quot;: &quot;dev&quot;,
      &quot;substitutions&quot;: {
        &quot;_TEST&quot;: &quot;FOO&quot;
      }
    }
  )

  run_build_trigger.execute()
</code></pre>
<p>Make sure that all substitutions are already declared on the existing trigger.</p>"
"Google Cloud Build how to send context with different path to Docker build<p>I have these two docker containers that I need to build, <code>nginx</code> and <code>php-fpm</code></p>
<p>However, both need to use the ROOT directory as context, and Google Cloud Build uses the nginx and php-fpm (the dirs where the Dockerfile is) as context.</p>
<p>How can I change that?</p>
<p><a href=""https://i.stack.imgur.com/dKYUk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dKYUk.png"" alt=""enter image description here"" /></a></p>
<p>This is my cloudbuild.yaml file</p>
<pre><code>steps:
  - name: 'gcr.io/cloud-builders/docker'
    args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/monolito-arg-phpfpm', './infra/docker/php-fpm' ]
    id: 'Build Docker PHP-FPM image'
  - name: 'gcr.io/cloud-builders/docker'
    args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/monolito-arg-nginx', './infra/docker/nginx' ]
    id: 'Build Docker nginx image'
images:
  - 'us-central1-docker.pkg.dev/$PROJECT_ID/monolito-arg/monolito-arg-phpfpm'
  - 'us-central1-docker.pkg.dev/$PROJECT_ID/monolito-arg/monolito-arg-nginx'
</code></pre>","<p>Posting it as an answer to give it more visibility:</p>
<p>As stated in the <a href=""https://stackoverflow.com/questions/68303511/google-cloud-build-how-to-send-context-with-different-path-to-docker-build#:%7E:text=i%20fixed%20it%2C%20i%20needed%20more%20parameters%20in%20the%20build%3A%20args%3A%20%5B%20%27build%27%2C%20%27-t%27%2C%20%27gcr.io%2F%24project_id%2Fmonolito-arg-phpfpm%27%2C%20%27-f%27%2C%20%27.%2Finfra%2Fdocker%2Fphp-fpm%2Fdockerfile%27%2C%20%27.%27%20%5D%20%E2%80%93"">comments</a>:</p>
<p>To fix it you needed more parameters in the build:</p>
<pre><code>args: [ 'build', '-t', 'gcr.io/$PROJECT_ID/monolito-arg-phpfpm', '-f', './infra/docker/php-fpm/Dockerfile', '.' ]
</code></pre>"
"Why Google Cloud Build creates intermediate containers?<p>Could somebody explain to me why Google Cloud Build creates intermediate containers to run commands?</p>
<pre><code>Step #0 - &quot;Build&quot;: Step 2/25 : ARG NODE_ENV
Step #0 - &quot;Build&quot;:  ---&gt; Running in 17281bea0e29
Step #0 - &quot;Build&quot;: Removing intermediate container 17281bea0e29
Step #0 - &quot;Build&quot;:  ---&gt; a43229632036
Step #0 - &quot;Build&quot;: Step 3/25 : ARG DB_NAME
Step #0 - &quot;Build&quot;:  ---&gt; Running in 04a199971761
Step #0 - &quot;Build&quot;: Removing intermediate container 04a199971761
Step #0 - &quot;Build&quot;:  ---&gt; 3f15b2ad5662
Step #0 - &quot;Build&quot;: Step 4/25 : ARG DB_USER
Step #0 - &quot;Build&quot;:  ---&gt; Running in 4fb95096aab1
Step #0 - &quot;Build&quot;: Removing intermediate container 4fb95096aab1
Step #0 - &quot;Build&quot;:  ---&gt; 97c619b21472
Step #0 - &quot;Build&quot;: Step 5/25 : ARG DB_PASSWORD
Step #0 - &quot;Build&quot;:  ---&gt; Running in 2280e3095a20
Step #0 - &quot;Build&quot;: Removing intermediate container 2280e3095a20
Step #0 - &quot;Build&quot;:  ---&gt; a79acad9a411
Step #0 - &quot;Build&quot;: Step 6/25 : ARG CLOUD_SQL_CONNECTION_NAME
Step #0 - &quot;Build&quot;:  ---&gt; Running in 1f63bde588f0
Step #0 - &quot;Build&quot;: Removing intermediate container 1f63bde588f0
Step #0 - &quot;Build&quot;:  ---&gt; 8e22be5c6191
Step #0 - &quot;Build&quot;: Step 7/25 : ENV NODE_ENV ${NODE_ENV}
Step #0 - &quot;Build&quot;:  ---&gt; Running in 6f5495791d72
Step #0 - &quot;Build&quot;: Removing intermediate container 6f5495791d72
Step #0 - &quot;Build&quot;:  ---&gt; 2413826fa5f6
Step #0 - &quot;Build&quot;: Step 8/25 : ENV DB_NAME ${DB_NAME}
Step #0 - &quot;Build&quot;:  ---&gt; Running in e3099226a900
Step #0 - &quot;Build&quot;: Removing intermediate container e3099226a900
Step #0 - &quot;Build&quot;:  ---&gt; 68f11d7cee19
Step #0 - &quot;Build&quot;: Step 9/25 : ENV DB_USER ${DB_USER}
Step #0 - &quot;Build&quot;:  ---&gt; Running in c1c3dc6ca115
Step #0 - &quot;Build&quot;: Removing intermediate container c1c3dc6ca115
Step #0 - &quot;Build&quot;:  ---&gt; 5fd5610b2de5
Step #0 - &quot;Build&quot;: Step 10/25 : ENV DB_PASSWORD ${DB_PASSWORD}
Step #0 - &quot;Build&quot;:  ---&gt; Running in 67c6ed7a56e7
Step #0 - &quot;Build&quot;: Removing intermediate container 67c6ed7a56e7
Step #0 - &quot;Build&quot;:  ---&gt; 7c8ecb080c59
Step #0 - &quot;Build&quot;: Step 11/25 : ENV CLOUD_SQL_CONNECTION_NAME ${CLOUD_SQL_CONNECTION_NAME}
Step #0 - &quot;Build&quot;:  ---&gt; Running in 8dddcddd80c0
Step #0 - &quot;Build&quot;: Removing intermediate container 8dddcddd80c0
Step #0 - &quot;Build&quot;:  ---&gt; 789cc6e25b55
Step #0 - &quot;Build&quot;: Step 12/25 : WORKDIR /usr/src/app
Step #0 - &quot;Build&quot;:  ---&gt; Running in 2c5e9083a8a5
Step #0 - &quot;Build&quot;: Removing intermediate container 2c5e9083a8a5
</code></pre>
<p>I am interested in <code>Removing intermediate container ...</code> after each step. Does it hurt the performance of the build?</p>
<p>cloudbuild.yml</p>
<pre><code>steps:
  - name: gcr.io/cloud-builders/docker
    args:
      - build
      - '--build-arg'
      - NODE_ENV=$_NODE_ENV
      - '--build-arg'
      - DB_NAME=$_DB_NAME
      - '--build-arg'
      - DB_USER=$_DB_USER
      - '--build-arg'
      - DB_PASSWORD=$$DB_PASSWORD
      - '--build-arg'
      - CLOUD_SQL_CONNECTION_NAME=$_CLOUD_SQL_CONNECTION_NAME
      - '--no-cache'
      - '-t'
      - '$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:$COMMIT_SHA'
      - .
      - '-f'
      - Dockerfile
    id: Build
    secretEnv:
      - DB_PASSWORD
  - name: gcr.io/cloud-builders/docker
...
</code></pre>","<p>Cloud Build uses Docker to execute builds. To understand why Cloud Build creates intermediate containers, first you must understand the Docker build process.</p>
<p>For each build step, Cloud Build executes a Docker container as an instance of <code>docker run</code>. Each step is processed in an intermediate container.</p>
<p><em>&quot;Those intermediate containers can succeed or fail. If they succeed, the intermediate container is merged with the image from the last successful build step, and then the intermediate container is deleted.&quot;</em></p>
<p>On the performance perspective, removing immediate containers are part of the build process and it helps reduce the size of your container image.</p>
<p>There are already some existing articles that further explains the Docker build process. Here are some interesting links:</p>
<ul>
<li><a href=""https://stackoverflow.com/a/39706883/7031308"">How are intermediate containers formed?</a></li>
<li><a href=""https://blog.hipolabs.com/understanding-docker-without-losing-your-shit-cf2b30307c63"" rel=""nofollow noreferrer"">https://blog.hipolabs.com/understanding-docker-without-losing-your-shit-cf2b30307c63</a></li>
<li><a href=""https://medium.com/ihme-tech/troubleshooting-the-docker-build-process-454583c80665"" rel=""nofollow noreferrer"">https://medium.com/ihme-tech/troubleshooting-the-docker-build-process-454583c80665</a></li>
</ul>"
"How to configure Serverless VPC Access from Cloud Build ""gradle test""?<p>I'm trying to put some integration tests in the Cloud Build process. So far I managed to connect to a MySQL server, but I can't connect to a Redis server since I can't add <code>--vpc-connector</code> option to <code>gradle test</code> command to configure Serverless VPC Connector.</p>
<p>This is part of <code>cloudbuild.yaml</code>:</p>
<pre><code>steps:
  - name: 'gradle:6.8.3-jdk11'
    args:
      - 'test'
      - '--no-daemon'
      - '-i'
      - '--stacktrace'
    id: Test
    entrypoint: gradle
  - name: gcr.io/cloud-builders/docker
    args:
      - build
      - '--no-cache'
      - '-t'
      - '$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:$COMMIT_SHA'
      - .
      - '-f'
      - Dockerfile
    id: Build
  - name: gcr.io/cloud-builders/docker
    args:
      - push
      - '$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:$COMMIT_SHA'
    id: Push
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:slim'
    args:
      - run
      - services
      - update
      - $_SERVICE_NAME
      - '--platform=managed'
      - '--image=$_GCR_HOSTNAME/$PROJECT_ID/$REPO_NAME/$_SERVICE_NAME:$COMMIT_SHA'
      - &gt;-
        --labels=managed-by=gcp-cloud-build-deploy-cloud-run,commit-sha=$COMMIT_SHA,gcb-build-id=$BUILD_ID,gcb-trigger-id=$_TRIGGER_ID,$_LABELS
      - '--region=$_DEPLOY_REGION'
      - '--quiet'
      - '--vpc-connector=$_SERVERLESS_VPC_CONNECTOR'
    id: Deploy
    entrypoint: gcloud

(... omitted ...)
</code></pre>
<p>Everything works fine If I remove the <code>Test</code> step. I need to add <code>--vpc-connector</code> option to <code>Test</code> step somehow to connect to the Redis server, but there is no such option in the <code>gradle:6.8.3-jdk11</code> image.</p>
<p>How to configure Serverless VPC Connector in the <code>Test</code> step so <code>gradle test</code> command can connect to the Redis server?</p>","<p>You are mixing 2 concepts:</p>
<ul>
<li>Gradle is an application layer</li>
<li>VPC Connector is an infrastructure component to bridge the serverless world managed by Google and the VPC of your current project.</li>
</ul>
<p>So, Gradle absolutely don't care about the infrastructure: It will try to reach a private IP, the REDIS private IP.</p>
<p>Cloud Build doesn't support VPC connector and thus, you can't access private resources in your project through Cloud Build. (A <a href=""https://cloud.google.com/sdk/gcloud/reference/alpha/builds/worker-pools"" rel=""nofollow noreferrer"">private preview</a> is ongoing to have Cloud Build worker directly in your VPC and thus not to have this VPC connectivity issue (because already in the VPC), but I haven't visibility on a public preview of this feature)</p>"
"How to pass authenticated state from the cloud builder to docker?<p>I would like to use Google Cloud Build to build my docker images. These docker images use private packages that are downloaded from Google Artifact Registry.</p>
<p>The builder itself is authenticated and can use the <code>npx google-artifactregistry-auth</code> command. But I cannot call it inside the docker build process.</p>
<p>When I build the image locally I pass my credentials into Dockerfile like so:
<code>--build-arg GOOGLE_CREDS=\&quot;$(cat $GOOGLE_APPLICATION_CREDENTIALS)\&quot;</code></p>
<p>Is there a way to make this work out of the box or do I have to make a separate service account and upload its key as a secret to cloud build? Kind of annoying since both services are on google cloud....</p>
<p>EDIT:
By request I'm adding info on how artifact registry is handled when I build it locally.
My docker command is:</p>
<pre><code>docker build --rm --build-arg GOOGLE_CREDS=\&quot;$(cat $GOOGLE_APPLICATION_CREDENTIALS)\&quot; -f 'Dockerfile' -t image:latest .

</code></pre>
<p>Relevant parts of the docker image are:</p>
<pre><code>ARG GOOGLE_CREDS
ENV GOOGLE_APPLICATION_CREDENTIALS=/credentials.json
RUN echo ${GOOGLE_CREDS} &gt; $GOOGLE_APPLICATION_CREDENTIALS
COPY .npmrc_template /root/.npmrc

RUN npx google-artifactregistry-auth ~/.npmrc 
RUN yarn install --silent
</code></pre>
<p>.npmrc_template contains details about the private repository but no password. It is then filled by <code>google-artifactregistry-auth</code> command</p>","<p>You need to explicitly <a href=""https://cloud.google.com/build/docs/build-config#network"" rel=""nofollow noreferrer"">add the cloudbuild network to your Docker Build</a>, like that</p>
<pre><code>docker build --rm -f 'Dockerfile' -t image:latest --network=cloudbuild .
</code></pre>"
"Running Cloud Build trigger via GCP Console returns 'build.service_account' field cannot be set for triggered builds<p>I am currently using Cloud Build for my Dataflow Flex template to kick off jobs.
Here's my current command:</p>
<pre><code>gcloud beta builds submit --config run.yaml --substitutions _REGION=$REGION \
--substitutions _FMPKEY=$FMPKEY --no-source
</code></pre>
<p>Currently this is running fine from Cloud Shell.
But now I want the build to be kicked off based on  a trigger..
So I created  a Cloud Build that will trigger running this file based on dropping a  message to a topic:</p>
<pre><code>https://github.com/mmistroni/GCP_Experiments/blob/master/dataflow/pipeline/run.yaml
</code></pre>
<p>However, after publishing a message to the selected topic, all my builds fail with the following error:</p>
<pre><code>our build failed to run: generic::invalid_argument:generic::invalid_argument:
 'build.service_account' field cannot be set for triggered builds
</code></pre>
<p>I cannot see any logs or details, so it's not clear to me what is going on..
I am guessing it has something to do with the last line in my run.yaml?</p>
<pre><code>options:
  logging: CLOUD_LOGGING_ONLY

# Use the Compute Engine default service account to launch the job.
serviceAccount: projects/$PROJECT_ID/serviceAccounts/$PROJECT_NUMBER-compute@developer.gserviceaccount.com
</code></pre>
<p>However I see no option for selecting the service account in cloud build. Do I need to set some permissions in IAM?</p>","<p>You are correct with your guess and this is working as intended.</p>
<p>Cloud Build has a default service account to execute builds on your behalf. While GCP allows you to configure user-specific accounts for additional control, it doesn't apply when you're using build triggers. Build triggers only use the default service account to execute builds.</p>
<p>This is documented in GCP <a href=""https://cloud.google.com/build/docs/automating-builds/create-manage-triggers#security_implications_of_build_triggers"" rel=""nofollow noreferrer"">docs</a>:</p>
<blockquote>
<p><em>Build triggers use Cloud Build service account to execute builds. This could provide elevated build-time permissions to users who use triggers to start a build. Keep the following security implications in mind when using build triggers ...</em></p>
</blockquote>
<p>Also as part of <a href=""https://cloud.google.com/build/docs/securing-builds/configure-user-specified-service-accounts#limitations"" rel=""nofollow noreferrer"">limitation</a>:</p>
<blockquote>
<p><em>User-specified service accounts only work with manual builds; they don't work with build triggers.</em></p>
</blockquote>
<p>Therefore, you must pass a config yaml without <code>serviceAccount</code> if you plan on using build triggers.</p>"