{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load API tokens for our 3rd party APIs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import get_circle_api_key, get_gh_api_key, get_google_api_key\n",
    "\n",
    "cci_api_key = get_circle_api_key()\n",
    "gh_api_key = get_gh_api_key()\n",
    "vertex_api_key = get_google_api_key()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up our github branch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import get_repo_name, get_branch\n",
    "\n",
    "course_repo = get_repo_name()\n",
    "course_repo\n",
    "course_branch = get_branch()\n",
    "course_branch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The sample application: AI-powered quiz generator\n",
    "We are going to build a AI powered quiz generator.\n",
    "Create the dataset for the quizz."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "human_template = \"{question}\"\n",
    "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "\n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-RÃ©my-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\"\"\"\n",
    "\n",
    "# Build the prompt template.\n",
    "delimiter = \"####\"\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "The user will provide a category that they want to create a quiz for. Any questions included in the quiz should only refer to the category.\n",
    "\n",
    "Step 1:{delimiter} First identity the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list the topics are below:\n",
    "{quiz_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category.\n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "\n",
    "Use the following format for the quiz:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use langchain to build the prompt template.\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"human\", prompt_template)])\n",
    "# print to observe the content or generated object\n",
    "chat_prompt\n",
    "\n",
    "#Choose the LLM.\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "llm = ChatVertexAI(model_name=\"gemini-pro\",\n",
    "                   convert_system_message_to_human=True)\n",
    "llm\n",
    "# Set up an output parser in LangChain that converts the llm response into a string.\n",
    "# parser\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "out_parser = StrOutputParser()\n",
    "out_parser\n",
    "\n",
    "# Connect the pieces using the pipe operator from Langchain Expression Language.\n",
    "chain = chat_prompt | llm | out_parser\n",
    "chain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build the function 'assistance_chain' to put together all steps above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# taking all components and making reusable as one piece\n",
    "def assistant_chain(\n",
    "    system_message,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatVertexAI(model_name=\"gemini-pro\",\n",
    "                     convert_system_message_to_human=True),\n",
    "    output_parser=StrOutputParser()):\n",
    "  chat_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_message),\n",
    "      (\"human\", human_template),\n",
    "  ])\n",
    "  return chat_prompt | llm | out_parser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluations\n",
    "Create the function 'eval_expected_words' for the first example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval_expected_words(\n",
    "    system_message,\n",
    "    question,\n",
    "    expected_words,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatVertexAI(model_name=\"gemini-pro\",\n",
    "                     convert_system_message_to_human=True),\n",
    "    output_parser=StrOutputParser()):\n",
    "  assistant = assistant_chain(system_message, human_template, llm,\n",
    "                              output_parser)\n",
    "\n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "\n",
    "  print(answer)\n",
    "\n",
    "  assert any(word in answer.lower() for word in\n",
    "             expected_words), f\"Expected the assistant questions to include\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test: Generate a quiz about science."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"Generate a quiz about science.\"\n",
    "expected_words = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
    "# create the eval\n",
    "eval_expected_words(prompt_template, question, expected_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the function 'evaluate_refusal' to define a failing test case where the app should decline to answer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_refusal(\n",
    "    system_message,\n",
    "    question,\n",
    "    decline_response,\n",
    "    human_template=\"{question}\",\n",
    "    llm=ChatVertexAI(model_name=\"gemini-pro\"),\n",
    "    output_parser=StrOutputParser()):\n",
    "  assistant = assistant_chain(human_template,\n",
    "                              system_message,\n",
    "                              llm,\n",
    "                              output_parser)\n",
    "\n",
    "  answer = assistant.invoke({\"question\": question})\n",
    "  print(answer)\n",
    "\n",
    "  assert decline_response.lower() in answer.lower(), \\\n",
    "    f\"Expected the bot to decline with \\\n",
    "    '{decline_response}' got {answer}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a new question (which should be a bad request)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"Generate a quiz about Rome.\"\n",
    "decline_response = \"I'm sorry\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the refusal eval.\n",
    "<p style=\"background-color:pink; padding:15px;\"> <b>Note:</b> The following function call will throw an exception.</p>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_refusal(prompt_template, question, decline_response)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
